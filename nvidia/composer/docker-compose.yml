version: '2.3'

## To Build The Repo: Execute the following command from directory ABOVE notebook repository
## Dev Environment:  docker-compose build && docker-compose up -d
## Prod Environment: docker-compose -f docker-compose.yml -f docker-compose.deployment.yml build && ... 

## Volume shared by frontend and assessment microservices. Frontend must write a file 
## for the assessment microservice to give you credit. Good luck!!
volumes:
  assessment_results:

services:  

  ## Deliver jupyter labs interface for students.
  lab:
    container_name: jupyter-notebook-server  
    init: true
    volumes:  ## Allow /dli/task in container to reference ./notebooks/ in host
      - ./notebooks/:/dli/task/
    ports:  ## Expose the following ports for various features 
    - "7860:7860"
    - "9010:9010"
    - "9011:9011"
    - "9012:9012"
    environment:  ## Change default base_url/mode for ChatNVIDIA etc.
    - NVIDIA_DEFAULT_MODE=open
    - NVIDIA_BASE_URL=http://llm_client:9000/v1

  ## Deliver a server to interface with host docker orchestration
  docker_router:
    container_name: docker_router
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  ## Give access to host docker
    ports:
      - "8070:8070"

  ## Deliver your llm_client microservice
  llm_client:
    container_name: llm_client
    volumes:
      - ./notebooks/llm_client:/llm_client
    ports:
      - "9000:9000"
    env_file:
      - .env  ## pass in your environment variable (i.e. NVIDIA_API_KEY)
      ## This file belongs ABOVE the notebooks directory. Contains some of the following:
      # COMPOSE_PROJECT_NAME=c-fx-15-v1
      # DEV_NGINX_PORT=80
      # DEV_ASSESSMENT_PORT=81
      # NVIDIA_API_KEY=nvapi-...  ## To be filled in

  ## If you had an extra A100, you could deploy your own NIM LLM...
  # nim:
  #   user: root # you don't have to be the root, but need to specify some user
  #   container_name: nim 
  #     image: nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.2
  #   runtime: nvidia
  #   shm_size: 16gb
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=1
  #     # Necessary since we are running as root on potentially-multiple GPUs
  #     - OMPI_ALLOW_RUN_AS_ROOT=1
  #     - OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
  #     - end_id
  #   volumes:
  #     - ./nim-cache/nim:/opt/nim/.cache
  #   env_file:
  #     - .env  ## pass in your environment variable through file (i.e. NGC_API_KEY)

  ##############################################################################
  ## Deliver your gradio frontend for assessment and web ui
  frontend: 
    container_name: frontend
    volumes: 
      - ./notebooks/:/notebooks/
      - assessment_results:/results
    ports:
      - "8090:8090"
    depends_on: 
      - "lab"

  chatbot: 
    container_name: chatbot
    volumes: 
      - ./notebooks/:/notebooks/
      - ./notebooks/imgs/:/web/imgs
      - ./notebooks/slides/:/web/slides
    ports:
      - "8999:8999"
    depends_on: 
      - "lab"
    env_file:
      - .env

  ## Reverse-proxy option. If you can't access port 8090 in browser, try /8090
  frontend_rproxy: 
    container_name: frontend_rproxy
    volumes: 
      - ./notebooks/:/notebooks/
      - assessment_results:/results
    ports:
      - "8091:8091"
    depends_on: 
      - "lab"
    entrypoint: "gunicorn frontend_server_rproxy:app \
        --worker-class uvicorn.workers.UvicornWorker \
        --workers 3 \
        --bind 0.0.0.0:8091 \
        --log-level info"

  chatbot_rproxy: 
    container_name: chatbot_rproxy
    volumes: 
      - ./notebooks/:/notebooks/
      - ./notebooks/imgs/:/web/imgs
      - ./notebooks/slides/:/web/slides
    ports:
      - "8998:8998"
    depends_on: 
      - "lab"
    entrypoint: "gunicorn frontend_server_rproxy:app \
        --worker-class uvicorn.workers.UvicornWorker \
        --workers 3 \
        --bind 0.0.0.0:8998 \
        --log-level info"
    env_file:
      - .env

  #############################################################################
  ## Other Stuff: DLI-Specific

  ## Modifier service. Does nothing in dev. Removes solution component in prod
  mod:
    container_name: modifier
    image: python:3.10-slim
    volumes: 
      - /var/run/docker.sock:/var/run/docker.sock
      - ./notebooks/:/notebooks/
    depends_on: 
      - "frontend"

  ## Assessment microservice. Checks for assignment completion
  assessment:
    image: python:3.10-slim
    volumes:
      - assessment_results:/dli/assessment_results/
      - ./assessment:/dli/assessment
    entrypoint: ["/bin/sh", "-c"]
    command: [". /dli/assessment/entrypoint.assessment.sh"]

  ## Reverse-proxy (serving the front page) service
  nginx:
    image: nginx:1.15.12-alpine
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - lab

networks:
  default:
    name: nvidia-llm
