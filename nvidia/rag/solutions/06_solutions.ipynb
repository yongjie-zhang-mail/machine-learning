{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7816f5-f413-4f7a-baa5-31d8f81b9bde",
   "metadata": {},
   "source": [
    "## **Part 3: [Exercise]** A Synthetic - But More Realistic - Example\n",
    "\n",
    "Observe that the entries that could be perceived as good input/output pairs trigger relatively high similarity on embedding. It is worth mentioning that, depending on the encoder model convergence, the query and document pathway may or may not be significantly different.\n",
    "\n",
    "The real utility of having a \"bi-encoder\" **in general** is that the second encoder can be trained to remain consistent with the first even if the format of the input starts deviating drastically. To help illustrate this, we can flesh out our documents into longer-form variations and try the same experiment again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc709930-775d-47f1-9c28-b9be6480a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")\n",
    "# instruct_llm = ChatNVIDIA(model=\"ai-mixtral-8x7b-instruct\")\n",
    "\n",
    "expound_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate part of a longer story that could reasonably answer all\"\n",
    "    \" of these questions somewhere in its contents: {questions}\\n\"\n",
    "    \" Make sure the passage only answers the following concretely: {q1}.\"\n",
    "    \" Give it some weird formatting, and try not to answer the others.\"\n",
    "    \" Do not include any commentary like 'Here is your response'\"\n",
    ")\n",
    "\n",
    "###############################################################################################\n",
    "## BEGIN TODO\n",
    "\n",
    "expound_chain = (\n",
    "    ## TODO: flesh out documents into a more verbose form by implementing the expound_chain \n",
    "    ##  which takes advantage of the prompt and llm provided above.\n",
    "    {}\n",
    ")\n",
    "\n",
    "expound_chain = (\n",
    "    {'q1' : itemgetter(0), 'questions' : itemgetter(1)} \n",
    "    | expound_prompt \n",
    "    | instruct_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "longer_docs = []\n",
    "for i, q in enumerate(queries):\n",
    "    ## TODO: Invoke the expound_chain pipeline as appropriate\n",
    "    longer_doc = \"\"\n",
    "    longer_doc = expound_chain.invoke([q, queries])\n",
    "    pprint(f\"\\n\\n[Query {i+1}]\")\n",
    "    print(q)\n",
    "    pprint(f\"\\n\\n[Document {i+1}]\")\n",
    "    print(longer_doc)\n",
    "    pprint(\"-\"*64)\n",
    "    longer_docs += [longer_doc]\n",
    "\n",
    "## END TODO\n",
    "###############################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
