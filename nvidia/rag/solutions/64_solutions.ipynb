{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8961a2d3-495d-4730-863e-21afd17d5d52",
   "metadata": {},
   "source": [
    "#### **[Exercise] Embed The Rest of the Responses**\n",
    "\n",
    "Now that you've seen how to do this process, wrap up by embedding the rest of the documents using these new techniques. Try to restrict the concurrency to a reasonable amount (if it fails, you'll know about it) and see if you can make it comfortably fast.\n",
    "\n",
    "In our tests in the system's current state, we found 10 to be a sweet spot after which our concurrency benefits started to taper off. Keep that in mind as you select your values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d05ee90-cef7-44ac-8a09-b9a66f57018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "## BEGIN TODO\n",
    "\n",
    "## Note, we found marginal benefit after value=10 in our tests...\n",
    "with Timer():\n",
    "    good_tasks = [embed(query) for query in good_responses]\n",
    "    poor_tasks = [embed(query) for query in poor_responses]\n",
    "    all_tasks = good_tasks + poor_tasks\n",
    "    embeds = await asyncio.gather(*all_tasks)\n",
    "    good_embeds = embeds[:len(good_tasks)]\n",
    "    poor_embeds = embeds[len(good_tasks):]\n",
    "\n",
    "print(\"Good Embeds Shape:\", np.array(good_embeds).shape)\n",
    "print(\"Poor Embeds Shape:\", np.array(poor_embeds).shape)\n",
    "\n",
    "## END TODO\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58072b5-ae1c-45b4-8daf-4e754e952b41",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Task 5: [Exercise]** Integrating Into Our Chatbot\n",
    "\n",
    "Now that we have a classifier that we can attach to our embedding model, we can use it as part of our event loop with roughly the latency of a single embedding model query.\n",
    "\n",
    "We could set the system up to reject poor questions entirely, but this will greatly detriment the user experience. ***Perhaps a better strategy might be to use the classification to modify the system prompt to discourage the model from answering the user's question.***\n",
    "\n",
    "#### **Task:** Implement the `score_response` method as appropriate to filter the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce499e3-056d-4f80-b5d3-277fd2ebe5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"Help them out\" system message\n",
    "good_sys_msg = (\n",
    "    \"You are an NVIDIA chatbot. Please answer their question while representing NVIDIA.\"\n",
    "    \"  Please help them with their question if it is ethical and relevant.\"\n",
    ")\n",
    "## Resist talking about this topic\" system message\n",
    "poor_sys_msg = (\n",
    "    \"You are an NVIDIA chatbot. Please answer their question while representing NVIDIA.\"\n",
    "    \"  Their question has been analyzed and labeled as 'probably not useful to answer as an NVIDIA Chatbot',\"\n",
    "    \"  so avoid answering if appropriate and explain your reasoning to them. Make your response as short as possible.\"\n",
    ")\n",
    "\n",
    "########################################################################################\n",
    "## BEGIN TODO\n",
    "\n",
    "def score_response(query):\n",
    "    ## TODO: embed the query and pass the embedding into your classifier\n",
    "    ## TODO: return the score for the response\n",
    "    return False\n",
    "\n",
    "def score_response(query):\n",
    "    ## TODO: embed the query and pass the embedding into your classifier\n",
    "    embedding = np.array([embedder.embed_query(query)])\n",
    "    ## TODO: return the score for the response\n",
    "    return model1(embedding)\n",
    "\n",
    "## END TODO\n",
    "########################################################################################\n",
    "\n",
    "chat_chain = (\n",
    "    { 'input'  : (lambda x:x), 'score' : score_response }\n",
    "    | RPrint()\n",
    "    | RunnableAssign(dict(\n",
    "        system = RunnableBranch(\n",
    "            ## Switch statement syntax. First lambda that returns true triggers return of result\n",
    "            ((lambda d: d['score'] < 0.5), RunnableLambda(lambda x: poor_sys_msg)),\n",
    "            ## ... (more branches can also be specified)\n",
    "            ## Default branch. Will run if none of the others do\n",
    "            RunnableLambda(lambda x: good_sys_msg)\n",
    "        )\n",
    "    )) | response_prompt | chat_llm\n",
    ")\n",
    "\n",
    "########################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
