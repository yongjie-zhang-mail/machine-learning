{
    "course": "NVIDIA Deep Learning Institute's Instructor-Led Course titled \"Building RAG Agents with LLMs\"",
    "filenames": [
        "00_jupyterlab.ipynb",
        "01_microservices.ipynb",
        "02_llms.ipynb",
        "03_langchain_intro.ipynb",
        "04_running_state.ipynb",
        "05_documents.ipynb",
        "06_embeddings.ipynb",
        "07_vectorstores.ipynb",
        "08_evaluation.ipynb",
        "09_langserve.ipynb",
        "64_guardrails.ipynb",
        "99_table_of_contents.ipynb"
    ],
    "slides": " <img src='/file=slides/slide01.jpg'>: Introduction slide featuring the NVIDIA logo and the topic 'Building RAG Agents with LLMs'.\n<img src='/file=slides/slide02.jpg'>: 3D rendering of large language models, illustrating components like central server, documents, speech bubbles, and databases.\n<img src='/file=slides/slide03.jpg'>: Concept of Dialog/Retrieval Agents in LLMs with context and control, showing user interaction, agent retrieval, and response generation.\n<img src='/file=slides/slide05.jpg'>: Promotional graphic for LangChain, highlighting components like 'Context', 'Tools', 'Test', 'Extraction', 'Storage & Indexing', and 'Agents'.\n<img src='/file=slides/slide06.jpg'>: Promotional graphic for NVIDIA's AI capabilities, showcasing various AI applications like gaming, graphics, and simulations.\n<img src='/file=slides/slide07.jpg'>: Prerequisites for 'RAG Agents in Production', listing prior LLM/LangChain exposure, intermediate Python experience, and web engineering exposure.\n<img src='/file=slides/slide08.jpg'>: Course objectives for 'RAG Agents in Production', including environment setup, LLM services, LangChain introduction, running state chains, document loading, embeddings, document retrieval, and RAG evaluation.\n<img src='/file=slides/slide09.jpg'>: Jupyter Labs interface overview, showing web browser, device, and stack of blocks labeled 'Files', 'OS', 'Python', and 'C++'.\n<img src='/file=slides/slide10.jpg'>: DLI Jupyter Labs Interface, featuring file explorer, notebook panels, and Python environment.\n<img src='/file=slides/slide11.jpg'>: Diagram of DLI Jupyter Labs Interface, showing connection between user's device, remote host, and additional processes.\n<img src='/file=slides/slide12.jpg'>: Containerization with Docker, illustrating standard host environment vs. Docker container environment.\n<img src='/file=slides/slide13.jpg'>: Microservices Workflow, showing interaction between user's device, remote host, and components within the remote host.\n<img src='/file=slides/slide14.jpg'>: Architecture for scaling containerized applications, showing connection between company database, arbitrary host, and web browser instances.\n<img src='/file=slides/slide15.jpg'>: Gradio webpage, featuring navigation bar, banner, and user interface for machine learning apps.\n<img src='/file=slides/slide16.jpg'>: Simple Gradio ChatInterface, showing GUI and code snippet for chat application.\n<img src='/file=slides/slide17.jpg'>: Gradio in HuggingFace Spaces, featuring form fields, project settings, and image filters.\n<img src='/file=slides/slide18.jpg'>: Custom Gradio Block Interface, showing graphical user interface and code snippet with various programming elements.\n<img src='/file=slides/slide19.jpg'>: Standalone Environment LLM, focusing on Llama-2-7B model and Jupyter Notebook configurations.\n<img src='/file=slides/slide20.jpg'>: Large model hosting platforms, showing remote host with frontend and VRAM-bound Jupyter Notebook connected to NVIDIA GPU Cloud.\n<img src='/file=slides/slide21.jpg'>: Query Router Access, illustrating JSON object, central node, and load balancing components.\n<img src='/file=slides/slide22.jpg'>: Architecture of Large Model Hosting Platforms with OpenAI Gateway, showing flow from End-User Application to Server Management.\n<img src='/file=slides/slide23.jpg'>: Comparison between OpenAI's GPT-4 and NVIDIA's GPU Cloud, highlighting features and accessibility.\n<img src='/file=slides/slide24.jpg'>: Query Router Access, including JSON code snippet, flowchart, and icons representing different technologies.\n<img src='/file=slides/slide25.jpg'>: Full Deployment Stack, illustrating layered architecture for deploying applications with millions of apps and billions of users.\n<img src='/file=slides/slide26.jpg'>: NVIDIA Foundation Model Endpoints, featuring top open foundation models and trending models.\n<img src='/file=slides/slide28.jpg'>: NVIDIA Foundation Model Endpoints, showing code snippet and URL for advanced reasoning and text-to-text tasks.\n<img src='/file=slides/slide29.jpg'>: Process flow from raw requests to LangChain model, illustrating components like Document Loaders, Text Splitters, Vector Stores, Prompts, Models, Output Parsers, and Tools.\n<img src='/file=slides/slide30.jpg'>: Process of converting raw requests into a LangChain model, showing code snippet and query router.\n<img src='/file=slides/slide31.jpg'>: LLM Interfaces and The Whole Stack, featuring diagram with various components and URLs, functions list, and deployment solution.\n<img src='/file=slides/slide32.jpg'>: Structure of LangChain, featuring central circular node and surrounding nodes like Document Loaders, Tools, Output Parsers, Models, Prompts, Example Selectors, Vector Stores, and Text Splitters.\n<img src='/file=slides/slide33.jpg'>: Simple prompt-LLM chain building, showing flow from Input to Output with Prompt and LLM in between.\n<img src='/file=slides/slide34.jpg'>: Chain Building with runnables, illustrating flow from Input to Prompt to LLM and then to Output.\n<img src='/file=slides/slide35.jpg'>: Chain Building with internal and external flows, showing decision points and database interaction.\n<img src='/file=slides/slide37.jpg'>: Computing environment setup with Jupyter Notebook Server, Frontend, LLMs, and Docker Router.\n<img src='/file=slides/slide39.jpg'>: Chain Building Towards Running State, showing flow from Input to Output with intermediate steps involving Prompt, LLM, and Topic.\n<img src='/file=slides/slide40.jpg'>: Chain Building Towards Running State, illustrating flow from Input to Output with intermediate steps involving Prompt, LLM, and Combine Sentence.\n<img src='/file=slides/slide41.jpg'>: Chain Building Towards Running State, showing flow from Input to Output with branch chain and RunnableAssign.\n<img src='/file=slides/slide42.jpg'>: Fibonacci sequence implementation using a while loop, showing initial, running, and final states.\n<img src='/file=slides/slide43.jpg'>: Running State Chain Components Towards LCEL While Loop, featuring Python code snippet and visual representation of state.\n<img src='/file=slides/slide44.jpg'>: Running State Chain Components Towards LCEL While Loop, showing code snippet with running state and state transitions.\n<img src='/file=slides/slide45.jpg'>: Running State Chain Components Towards LCEL While Loop, illustrating next Fibonacci function and state transitions.\n<img src='/file=slides/slide46.jpg'>: Running State Chain Components, featuring code snippets and visual representations of states.\n<img src='/file=slides/slide47.jpg'>: Running State Chain Components, showing code snippet from langchain_core.py with while loop and running state.\n<img src='/file=slides/slide48.jpg'>: Comparison of typical running state loop with running state loop in programming context, showing code snippets for both.\n<img src='/file=slides/slide49.jpg'>: Final Running State Loop with Big Picture, illustrating flow of processes starting with RunnableAssign and leading to Chain.\n<img src='/file=slides/slide50.jpg'>: Airline Chatbot workflow, showing flow from Input to Response with components like Knowledge Base, DB Lookup, and Customer Info.\n<img src='/file=slides/slide52.jpg'>: Modern Chain Paradigms towards powerful running state, illustrating flow from Input to Answer with various components.\n<img src='/file=slides/slide53.jpg'>: Modern Chain Paradigms towards powerful running state, showing flowchart with three main branches and examples of text inputs and outputs.\n<img src='/file=slides/slide54.jpg'>: Modern Chain Paradigms towards powerful running state, illustrating flowchart with four paths leading to Environment and Tool Choice Schema.\n<img src='/file=slides/slide55.jpg'>: Final Objective of Knowledge Base + Running State Chain, featuring JSON objects and code snippet for updating knowledge base.\n<img src='/file=slides/slide56.jpg'>: Document Reasoning, illustrating flow from Prompt Context to LLM and response generation.\n<img src='/file=slides/slide57.jpg'>: Document Reasoning within company database context, showing flow from Company Database to Prompt Context and response generation.\n<img src='/file=slides/slide58.jpg'>: Document Reasoning, illustrating flow from Prompt Context to Question and response generation.\n<img src='/file=slides/slide59.jpg'>: Chunking, featuring document with green border and table with blue and green cells.\n<img src='/file=slides/slide60.jpg'>: Document Stuffing, illustrating process of using LLMs for open-source and fine-tuned chat models.\n<img src='/file=slides/slide61.jpg'>: Map Reduce Chain, featuring central node labeled 'Prompt' connected to 'LLM' and 'Smaller Chunk' nodes.\n<img src='/file=slides/slide62.jpg'>: Refinement Chain, illustrating flow from Prompt to LLM and output generation.\n<img src='/file=slides/slide63.jpg'>: Knowledge Graph Traversal, showing flow from central concept to various nodes and branches.\n<img src='/file=slides/slide64.jpg'>: Knowledge Graph Traversal, illustrating flow from question to Prompt Find Info and LLM.\n<img src='/file=slides/slide65.jpg'>: Optional Tangent: LangGraph, featuring code snippet and flowchart with central node labeled 'Call_Tool'.\n<img src='/file=slides/slide66.jpg'>: Modern Chain Paradigms Towards Powerful Running State, illustrating flow from Input to Environment and Vector Database.\n<img src='/file=slides/slide67.jpg'>: Retrieval-Augmented Generation (RAG), illustrating flow from question to Retriever and Generator.\n<img src='/file=slides/slide68.jpg'>: Transformer Architecture, focusing on Primary Backbone of LLMs with Encoder and Decoder sections.\n<img src='/file=slides/slide69.jpg'>: Comparison between Autoregressive and Embedding transformer architectures, showing sequence of letters and text output.\n<img src='/file=slides/slide70.jpg'>: NVIDIA's Retrieval QA Embedding, featuring title, description, visual representation, and JSON code snippet.\n<img src='/file=slides/slide71.jpg'>: Embedding Spaces with Learned Semantically Dense Values, featuring visual representation and scatter plot.\n<img src='/file=slides/slide72.jpg'>: Comparison between Bi-Encoder and Cross-Encoder language embedding schemes, showing encoders and functions.\n<img src='/file=slides/slide73.jpg'>: Comparison between Symmetric and Asymmetric/Generalized language embedding schemes, showing encoders and functions.\n<img src='/file=slides/slide74.jpg'>: Language Embedding Schemes, illustrating flow from function to score and input document.\n<img src='/file=slides/slide75.jpg'>: Embedding and Comparing, featuring list of phrases with data metrics and visual representations of a database.\n<img src='/file=slides/slide76.jpg'>: Semantic Guardrails, illustrating flow from Prompt Answer Option to Classifier Branch and relevant topics.\n<img src='/file=slides/slide77.jpg'>: Embedding Classification, featuring 3D grid with colored dots and classification head.\n<img src='/file=slides/slide78.jpg'>: Integrating a Vector Store, illustrating process flow from loading source data to querying and retrieving results.\n<img src='/file=slides/slide79.jpg'>: Integrating a Vector Store, featuring Python code snippet for setting up a vector store using FAISS and NVIDIAEmbeddings.\n<img src='/file=slides/slide80.jpg'>: Retrieving and Reordering Information, illustrating process flow from user query to VDB Retriever and Reranker.\n<img src='/file=slides/slide81.jpg'>: RAG Fusion, illustrating process flow from prompt to LLM and VDB Retriever.\n<img src='/file=slides/slide82.jpg'>: Integrating a local vector store with a local host system, showing connection between local host components and local vector store.\n<img src='/file=slides/slide83.jpg'>: GPU-Accelerating Vector Stores, featuring bar chart and table comparing vector search algorithms.\n<img src='/file=slides/slide84.jpg'>: Compute Scale Progression, illustrating progression from Jupyter Notebook Server to Milvus Cluster.\n<img src='/file=slides/slide85.jpg'>: Simple Conversation Setup for Rule-Based Agent (RAG), showing sequence of exchanges between user and RAG.\n<img src='/file=slides/slide86.jpg'>: Simple RAG Agents workflow, illustrating flow from question to Classifier Branch and LLM.\n<img src='/file=slides/slide87.jpg'>: Proper RAG Agent workflow, illustrating cyclical process with toolset and LLMs.\n<img src='/file=slides/slide88.jpg'>: LLM-As-A-Judge Pipeline Evaluation, illustrating RAG Pipeline and Evaluation Pipeline with central LLM.\n<img src='/file=slides/slide89.jpg'>: General Evaluation Chain, illustrating components of evaluation chain and key metrics.\n<img src='/file=slides/slide90.jpg'>: RAG Evaluation process with Quantifying System Goodness using LLM-as-a-Judge, illustrating generation and retrieval sections.\n<img src='/file=slides/slide92.jpg'>: Pipeline Evaluation using Evaluator Agent, illustrating sequence of prompts and responses leading to chain evaluation.\n<img src='/file=slides/slide93.jpg'>: Evaluation Chain, illustrating flow from VDB and Doc1 to Prompt Ask/Answer and LLM.\n<img src='/file=slides/slide94.jpg'>: Evaluation of RAG in frontend process, showing remote host setup with Jupyter Notebook and NVIDIA GPU Cloud.\n<img src='/file=slides/slide95.jpg'>: Congratulatory message from NVIDIA, featuring NVIDIA logo and tech-oriented design.",
    "summary": " ```\n00_jupyterlab.ipynb\n - Introduction to JupyterLab: Overview of JupyterLab interface, including menu bar, file browser, and main work area. Explanation of executing code in cells and handling input requests.\n - Special Syntax: Discussion on input and getpass functions, implicit string concatenation, and multi-line grouping in Python.\n - Main Ideas and Relevance To Course: Introduction to JupyterLab environment, basic Python syntax, and handling user inputs. Essential for navigating and executing code in subsequent notebooks.\n - Important Code: input, getpass, SecretStr, implicit string concatenation, multi-line grouping.\n - Connections to previous notebooks: None, as this is the introductory notebook.\n - Relevant Images: <img src='/file=img/jl_launcher.png'>\n```\n ```\n01_microservices.ipynb\n - Welcome To Your Cloud Environment: Introduction to the Jupyter Labs environment, its components, and its role in the course.\n - Part 1: Hosting Containers: Explanation of the cloud environment setup, including CPU, GPU, and exposed ports. Introduction to microservices and their role in the environment.\n - Part 2: The Jupyter Labs Microservice: Detailed explanation of the Jupyter Labs microservice, its configuration, and how it is launched.\n - Part 3: Interacting With Microservices As The Host: Instructions on interacting with microservices from both outside and inside the Jupyter Labs microservice.\n - Part 4: Checking Our Frontend: Introduction to the frontend microservice and how to interact with it.\n - Part 5: Wrap-Up: Summary of the notebook and next steps for the course.\n - Main Ideas and Relevance To Course: Introduction to the course environment, microservices, Docker, containerization, Jupyter Labs, and interaction with microservices.\n - Important Code: Docker commands, Dockerfile, docker-compose.yaml, requests library, curl commands.\n - Connections to previous notebooks: None, as this is the first notebook introducing the course environment.\n - Relevant Images: <img src='/file=img/simple-env.png'>, <img src='/file=img/docker-ms.png'>, <img src='/file=img/environment.png'>\n```\n ```\n02_llms.ipynb\n - Notebook 2: LLM Services and AI Foundation Models: Introduction to LLM services, pros and cons of local vs cloud deployment, AI Foundation Model Endpoint schemes, and accessing LLM generations.\n - Part 1: Getting Large Models Into Your Environment: Discussion on microservices, deployment scenarios for large models, and the current environment's constraints.\n - Part 2: Hosted Large Model Services: Evaluation of hosting options, including black-box hosted models and self-hosted models, and introduction to NVIDIA NGC Service.\n - Part 3: Getting Started With Hosted Inference: Explanation of deploying models for scaled inference, NVIDIA NIM Microservices, and API gateway servers.\n - Part 4: [Exercise] Trying Out The Foundation Model Endpoints: Hands-on exercise on interacting with LLM endpoints using Python requests, OpenAI client, and ChatNVIDIA client.\n - Part 5: Wrap-Up: Summary of the notebook's goals and next steps.\n - Main Ideas and Relevance To Course: Understanding deployment strategies for LLMs, interacting with hosted models, and using APIs for LLM access.\n - Important Code: Python requests library, OpenAI client, ChatNVIDIA client, API URLs, headers, payloads, and streaming responses.\n - Connections to previous notebooks: Builds on the course environment introduced in Notebook 1, focusing on microservices and LLM deployment.\n - Relevant Images: <img src='/file=img/ai-playground-api.png'>, <img src='/file=img/mixtral_api.png'>, <img src='/file=img/openai_chat.png'>\n```\n ```\n03_langchain_intro.ipynb\n - Notebook 3: LangChain Expression Language: Introduction to LangChain, a popular LLM orchestration library. Covers core systems like chains and runnables, and the LangChain Expression Language (LCEL). Demonstrates creating a simple chat model.\n - Learning Objectives: Leveraging chains and runnables for LLM systems, using LLMs for external conversation and internal reasoning, setting up a Gradio interface.\n - Main Ideas and Relevance To Course: LangChain orchestration, LCEL, Gradio interface, LLM systems, internal and external reasoning.\n - Important Code: RunnableLambda, RunnablePassthrough, ChatNVIDIA, ChatPromptTemplate, StrOutputParser, Gradio ChatInterface.\n - Connections to previous notebooks: Builds on the course environment and special syntax introduced in previous notebooks.\n - Relevant Images: <img src='/file=img/langchain-diagram.png'>\n```\n ```\n04_running_state.ipynb\n - Learning Objectives: Learning how to leverage runnables to orchestrate interesting LLM systems. Understanding how running state chains can be used for dialog management and iterative decision-making.\n - Questions To Think About: Would there ever be a use for a single-module variant of the running state chain that is not constantly querying the environment for input? What kinds of issues do you expect to encounter in JSON prediction? What kinds of approaches can you think of completely swapping prompts as part of the running state chain?\n - Notebook Source: Part of a larger NVIDIA Deep Learning Institute course titled Building RAG Agents with LLMs.\n - Environment Setup: Necessary for Colab, not necessary for course environment. Importing and setting up necessary libraries and environment variables.\n - Part 1: Keeping Variables Flowing: Implementing interesting logic in standalone chains by creating, mutating, and consuming states. Recall of zero-shot classification example from the last notebook.\n - Part 2: Running State Chain: Introduction to running state chains, their properties, and how they enforce the flow of information through a chain.\n - Part 3: Implementing a Knowledge Base with Running State Chain: Extending running state chains to manage more complex tasks like knowledge bases using json-enabled slot filling.\n - Part 4: [Exercise] Airline Customer Service Bot: Implementing a simple but effective dialog manager chatbot using running state chains and knowledge bases.\n - Part 5: Wrap-Up: Summary of the notebook and next steps.\n - Main Ideas and Relevance To Course: Introduction to advanced LangChain material revolving around the use of knowledge bases and running state chains. Key concepts include zero-shot classification, knowledge bases, and dialog management.\n - Important Code: Use of RunnableLambda, RunnableAssign, ChatPromptTemplate, StrOutputParser, PydanticOutputParser, BaseModel, Field, RExtract, and various LLM models.\n - Connections to previous notebooks: Builds on concepts introduced in previous notebooks, such as LangChain Expression Language (LCEL), chains, runnables, and zero-shot classification.\n - Relevant Images: <img src='/file=img/running_state_chain.png'>\n```\n ```\n05_documents.ipynb\n - Learning Objectives: Get familiar with document loaders and the kinds of utilities they might provide you. Learn how to parse large documents with limited context room by chunking the document and building up a knowledge base progressively. Understand how the progressive recontextualization, coersion, and consolidation of document chunks can be extremely useful, and also where it will encounter natural limitations.\n - Questions To Think About: Looking at the chunks that come out of your ArxivParser, you'll notice that some of the chunks make little sense on their own or have been completely corrupted by the conversion to text. Is it doing a pass over the chunks to clean them up? Considering the document summarization workflow (or any similar workflow that processes through a large list of document chunks), how often should this happen, and when is it justifiable?\n - Notebook Source: This notebook is part of a larger NVIDIA Deep Learning Institute course titled Building RAG Agents with LLMs. If sharing this material, please give credit and link back to the original course.\n - Environment Setup: Necessary for Colab, not necessary for course environment. Importing and installing necessary libraries such as langchain, langchain-nvidia-ai-endpoints, gradio, arxiv, and pymupdf.\n - Part 1: Chatting with Documents: Discusses the use of LLMs to chat with documents, including modifiable knowledge bases, sorting through and pulling references, and interacting with documents. Introduces the concept of document stuffing.\n - Part 2: Loading Documents: Explains how to load documents using LangChain's document loaders, such as UnstructuredFileLoader and ArxivLoader. Demonstrates loading a research paper from Arxiv.\n - Part 3: Transforming The Documents: Introduces the concept of chunking documents using LangChain's RecursiveCharacterTextSplitter. Demonstrates splitting a document into smaller chunks.\n - Part 4: [Exercise] Refining Summaries: Implements a Runnable that uses a while loop and the running state chain formulation to summarize a set of document chunks. Introduces the DocumentSummaryBase model and the RExtract function.\n - Part 5: Synthetic Data Processing: Discusses the broader context and potential challenges of document summarization using LLMs. Introduces the concept of domain-specific knowledge graphs and LangChain Knowledge Graphs.\n - Part 6: Wrap-Up: Summarizes the notebook and introduces the next notebook on semantic retrieval with embedding models.\n - Main Ideas and Relevance To Course: Working with large documents, document loaders, chunking, document summarization, running state chains, knowledge bases, LangChain, ArxivLoader, RecursiveCharacterTextSplitter, DocumentSummaryBase, RExtract, domain-specific knowledge graphs, LangChain Knowledge Graphs.\n - Important Code: LangChain, langchain-nvidia-ai-endpoints, gradio, arxiv, pymupdf, RecursiveCharacterTextSplitter, DocumentSummaryBase, RExtract, RunnableLambda, RunnableAssign, ChatPromptTemplate, StrOutputParser, PydanticOutputParser, ChatNVIDIA.\n - Connections to previous notebooks: Builds on the concepts of running state chains and knowledge bases from Notebook 4. Introduces new techniques for working with large documents and document summarization.\n - Relevant Images: <img src='/file=img/doc_stuff.png'>, <img src='/file=img/doc_refine.png'>\n```\n ```\n06_embeddings.ipynb\n - Part 1: Refreshing On Embedding Models: Introduction to embedding models, latent embeddings, word embeddings, sentence/document embeddings, decoder models, and encoder models.\n - Part 2: Using An NVIDIAEmbeddings Model: Practical application of NVIDIAEmbeddings model, query and document embedding, similarity checks, and visualization of cross-similarity matrix.\n - Part 3: A Synthetic - But More Realistic - Example: Expanding queries into longer-form documents, comparing embeddings of longer documents.\n - Part 4: Embeddings For Semantic Guardrails: Introduction to semantic guardrailing using embeddings.\n - Part 5: Wrap-Up: Summary and next steps.\n - Main Ideas and Relevance To Course: Embedding models, semantic reasoning, NVIDIAEmbeddings, query and document embedding, similarity checks, semantic guardrailing.\n - Important Code: NVIDIAEmbeddings, embed_query, embed_documents, cosine_similarity, plot_cross_similarity_matrix.\n - Connections to previous notebooks: Builds on LangChain concepts from Notebook 3, document processing from Notebook 5, and prepares for semantic guardrailing in Notebook 6.4.\n - Relevant Images: <img src='/file=img/encoder-decoder.png'>\n```\n ```\n07_vectorstores.ipynb\n - Learning Objectives: Understand how semantic-similarity-backed systems can facilitate easy-to-use retrieval formulations. Learn how to incorporate retrieval modules into your chat model systems for a retrieval-augmented generation (RAG) pipeline, which can be applied to tasks like document retrieval and conversation memory buffers.\n - Questions To Think About: Consider what modifications would be necessary to make these components work in an LCEL chain. Consider when it would be best to move your vector store solution into a scalable service and when a GPU will become necessary for optimization.\n - Notebook Source: This notebook is part of a larger NVIDIA Deep Learning Institute course titled Building RAG Agents with LLMs. If sharing this material, please give credit and link back to the original course.\n - Environment Setup: Necessary for Colab, not necessary for course environment. Necessary pip installs include langchain, langchain-nvidia-ai-endpoints, gradio, rich, arxiv, pymupdf, faiss-cpu.\n - Part 1: Summary of RAG Workflows: Explores several paradigms and derive reference code to help you approach some of the most common retrieval-augmented workflows. Specifically, the following sections will be covered (with the differences highlighted): Vector Store Workflow for Conversational Exchanges, Modified Workflow for an Arbitrary Document, Extended Workflow for a Directory of Arbitrary Documents.\n - Part 2: RAG for Conversation History: Delves into the capabilities of document embedding models and used them to embed, store, and compare semantic vector representations of text. Though we could motivate how to efficiently extend this into vector store land manually, the true beauty of working with a standard API is its strong incorporation with other frameworks that can already do the heavy lifting for us!\n - Part 3 [Exercise]: RAG For Document Chunk Retrieval: Given our prior exploration of document loading, the idea that data chunks can be embedded and searched through probably isn\u2019t surprising. With that said, it is definitely worth going over since applying RAG with documents is a double-edged sword; it may seem to work well out of the box but requires some extra care when optimizing it for truly reliable performance. It also provides an excellent opportunity to review some fundamental LCEL skills, so let\u2019s see what we can do!\n - Part 4: Saving Your Index For Evaluation: After you\u2019ve implemented your RAG chain, please save your accumulated vector store as shown in the official documentation. You\u2019ll have a chance to use it again for your final assessment!\n - Part 5: Wrap-Up: Congratulations! Assuming your RAG chain is all good, you\u2019re now ready to move on to the RAG Evaluation [Assessment] section!\n - Main Ideas and Relevance To Course: This notebook focuses on retrieval-augmented generation (RAG) with vector stores, covering how to use semantic-similarity-backed systems for easy-to-use retrieval formulations. It explains how to incorporate retrieval modules into chat model systems for tasks like document retrieval and conversation memory buffers. The notebook also discusses the importance of optimizing vector store solutions for scalability and GPU usage.\n - Important Code: NVIDIAEmbeddings, ChatNVIDIA, FAISS, RecursiveCharacterTextSplitter, ArxivLoader, LongContextReorder, ChatPromptTemplate, StrOutputParser, RunnableLambda, RunnableAssign, default_FAISS, aggregate_vstores, save_memory_and_get_output, chat_gen.\n - Connections to previous notebooks: This notebook builds on the concepts introduced in previous notebooks, such as embedding models and semantic reasoning (Notebook 6), document loading and chunking (Notebook 5), and the use of LangChain for building and orchestrating LLM systems (Notebook 3). It also introduces new concepts like vector stores and retrieval-augmented generation, which will be further explored in the RAG Evaluation notebook (Notebook 8).\n - Relevant Images: <img src='/file=img/data_connection_langchain.jpeg'>, <img src='/file=img/vector_stores.jpeg'>\n```\n ```\n08_evaluation.ipynb\n - Part 1: Pre-Release Evaluation: Discusses the importance of thorough testing for chatbot performance, including typical use inspection, edge case inspection, and progressive rollout.\n - Part 2: LLM-as-a-Judge Formulation: Introduces the concept of using LLMs as evaluators for natural language task performance, including frameworks like RAGAs and LangChain Evaluators.\n - Part 3: [Assessment Prep] Pairwise Evaluator: Outlines the steps to implement a custom pairwise evaluator for RAG chains, including sampling document chunks, generating synthetic QA pairs, and comparing responses.\n - Part 4: Advanced Formulations: Discusses various evaluation metrics such as style evaluation, ground-truth evaluation, retrieval/augmentation evaluation, and trajectory evaluation.\n - Part 5: [Assessment] Evaluating For Credit: Provides instructions for the final assessment, including setting up the environment and implementing the necessary endpoints.\n - Main Ideas and Relevance To Course: Focuses on evaluating RAG pipelines using LLM-as-a-Judge metrics, integrating techniques from prior notebooks to numerically approximate pipeline performance.\n - Important Code: Variables like `docstore_index`, `NVIDIAEmbeddings`, `ChatNVIDIA`, `FAISS`, `LangChain`, `RAGAs`, `LangChain Evaluators`, `pairwise evaluator`, `synthetic QA pairs`.\n - Connections to previous notebooks: Builds on concepts from Notebook 7 (Retrieval-Augmented Generation with Vector Stores) and Notebook 6 (Embedding Models and Semantic Reasoning), integrating document retrieval and embedding models.\n - Relevant Images: <img src='/file=img/DLI_Header_White.png'>\n```\n ```\n09_langserve.ipynb\n - LangServe Server Setup: Introduction to LangServe, setting up a simple API server using LangChain's Runnable interfaces with FastAPI. Demonstrates integration of LangChain models like `ChatNVIDIA` to create accessible API routes.\n - Part 1: Delivering the /basic_chat endpoint: Instructions for launching a `/basic_chat` endpoint as a standalone Python file for basic decision-making without internal reasoning.\n - Part 2: Using The Server: Explanation on accessing the `basic_chat` endpoint from another file using `RemoteRunnable`.\n - Part 3: Final Assessment: Guidance on completing the final assessment by implementing `/generator` and `/retriever` endpoints.\n - Main Ideas and Relevance To Course: Setting up a server using LangServe and FastAPI, integrating LangChain models, creating API routes, final assessment preparation.\n - Important Code: FastAPI, LangChain Runnable interfaces, `ChatNVIDIA`, `NVIDIAEmbeddings`, `RemoteRunnable`, `RunnableLambda`, `uvicorn`.\n - Connections to previous notebooks: Builds on LangChain concepts from Notebook 3, uses models and chains from previous notebooks, prepares for final assessment.\n - Relevant Images: <img src='/file=img/DLI_Header_White.png'>\n```\n ```\n64_guardrails.ipynb\n - Environment Setup: Necessary for Colab, not necessary for course environment. Importing and setting up essential libraries and models such as LangChain, NVIDIAEmbeddings, and ChatNVIDIA.\n - Part 4: [Advanced Exercise] Embeddings For Semantic Guardrails: Explores the use of embedding models for semantic guardrailing. Discusses the advantages and disadvantages of autoregression-guided filtering versus embedding-based filtering.\n - Task 1: Generate Synthetic Data: Generates representative good and poor examples for downstream guardrail fitting using LangChain and ChatNVIDIA.\n - Task 2: Generate More Embeddings (and faster): Uses asynchronous techniques to embed documents faster. Introduces concepts like coroutines and asyncio.gather.\n - Task 3: Confirming Semantic Density: Uses PCA and t-SNE for dimensionality reduction to visualize semantic clusters.\n - Task 4: Training Our Classifier: Trains a deep classifier using Keras and a simpler logistic regression classifier using scikit-learn to predict whether an embedding is good or bad.\n - Task 5: [Exercise] Integrating Into Our Chatbot: Integrates the classifier into the chatbot to modify the system prompt based on the classification score.\n - Task 6: [Exercise] Testing Out Your Chatbot: Provides exercises to test the guardrailed chatbot with various questions.\n - Main Ideas and Relevance To Course: Semantic guardrailing, embedding models, asynchronous programming, dimensionality reduction, classifier training, chatbot integration.\n - Important Code: LangChain, NVIDIAEmbeddings, ChatNVIDIA, asyncio, PCA, t-SNE, Keras, scikit-learn, RunnableBranch, RunnableAssign, ChatPromptTemplate.\n - Connections to previous notebooks: Builds on concepts from Notebook 6: Embedding Models and Semantic Reasoning. Uses LangChain and embedding models introduced in previous notebooks.\n - Relevant Images: <img src='/file=img/DLI_Header_White.png'>\n```\n ```\n99_table_of_contents.ipynb\n - Table Of Contents: Introduction to the course, overview of microservices, caches, and instructions for using the course chatbot, exercise frontend, and Docker-Router to read logs.\n - Main Ideas and Relevance To Course: Provides a structured overview of the course components, including microservices (chatbot, composer, docker-router, frontend, llm_client) and caches (imgs, slides, solutions). Essential for navigating the course materials and understanding the environment setup.\n - Important Code: JavaScript code snippets for linking to Gradio Chatbot and Gradio Frontend, Python code for reading logs from Docker-Router.\n - Connections to previous notebooks: Serves as a central hub for navigating through other notebooks and understanding the overall course structure.\n - Relevant Images: <img src='/file=img/DLI_Header_White.png'>\n```"
}