{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34",
   "metadata": {
    "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 5:** 处理大型文档</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "在上一个 notebook 中，我们了解了运行状态链和知识库！最后，我们拥有了执行简单对话管理和自定义知识跟踪所需的所有工具。在此 notebook 中，我们将把类似的思路应用在大型文档上，看看这会使我们的 LLM 遇到些什么问题。\n",
    "\n",
    "<br>\n",
    "\n",
    "### **学习目标：**\n",
    "\n",
    "* 熟悉文档加载器及其实用功能。\n",
    "* 了解如何通过对文档进行分块来逐渐建立知识库，从而在有限的上下文空间中解析大型文档。\n",
    "* 理解如何利用文档块进行渐进式重构（progressive recontextualization）、转换（coersion）和整合（consolidation），以及它们的局限。\n",
    "\n",
    "<br>\n",
    "\n",
    "### **思考问题：**\n",
    "\n",
    "* 查看 ArxivParser 中的数据块，您会发现有些块没什么意义，或者在文本转换的过程中被损坏了。它是否会对这些数据块进行清理？\n",
    "* 考虑一下文档摘要工作流（或其它需要处理大量文档块的工作流），应该多久执行一次，什么时候执行合理？\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook 版权声明：**\n",
    "\n",
    "* 本 notebook 是 [**NVIDIA 深度学习培训中心**](https://www.nvidia.cn/training/)的课程[**《构建大语言模型 RAG 智能体》**](https://www.nvidia.cn/training/instructor-led-workshops/building-rag-agents-with-llms/)中的一部分，未经 NVIDIA 授权不得分发。\n",
    "\n",
    "<br> \n",
    "\n",
    "### **环境设置：**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52",
   "metadata": {
    "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -qq langchain langchain-nvidia-ai-endpoints gradio\n",
    "# %pip install -qq arxiv pymupdf\n",
    "\n",
    "# import os\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-...\"\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c33c07-19b8-4c81-8d99-30fa2b3b2017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8583a1-c10a-41da-8256-49520f868670",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful utility method for printing intermediate states\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "\n",
    "def RPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def PPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        pprint(preface, x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 1 部分：** 与文档聊天\n",
    "\n",
    "此 notebook 将开始讨论用 LLM 与文档进行聊天。当前，在大型公共数据或特定数据上训练聊天模型的成本令人望而却步，而让 LLM 理解一系列 PDF 甚至 YouTube 视频的想法打开了更多可能性！\n",
    "\n",
    "* **您的 LLM 可以拥有一个基于人类可读文档构建的可修改知识库，**这意味着您可以直接控制它能访问什么样的数据，并指示它与其交互。\n",
    "* **您的 LLM 可以直接从文档集中整理和提取引用信息。**借助充分的提示工程和指令遵循先验，您可以强制模型仅根据您提供的材料执行动作。\n",
    "* **您的 LLM 甚至可以与您的文档进行交互，必要的时候执行自动修改。**这为自动内容优化和合成开辟了道路，稍后将深入探讨。\n",
    "\n",
    "您可以尽情发挥想象力，想象更多的应用场景。下面就来看看如何实现这些！\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **朴素的方法：将文档塞给模型**\n",
    "\n",
    "假设您有一些文本文档（PDF、博客等），并想要对文档相关内容进行提问。其中一个可以尝试的方法就是把文档的某种表示一股脑塞进聊天模型！从文档的角度来看，这称为[**文档填充**（document stuffing）](https://python.langchain.com/docs/modules/chains/document/stuff)。\n",
    "\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_stuff.png\" width=800px/>\n",
    ">\n",
    "> From [**Stuff | LangChain**🦜️🔗](https://python.langchain.com/docs/modules/chains/document/stuff)\n",
    "\n",
    "<br>\n",
    "\n",
    "如果您的模型足够强大且文档足够短，这个方法可能会有不错的效果，但不应该期待它会在整个文档中都表现的很好。由于训练限制，许多现代 LLM 在处理长上下文时都会有很大的问题。虽然无论您使用当今的哪种大模型，退化并不会带来灾难性后果，但它可能很快就无法正常地遵循指令了。\n",
    "  \n",
    "<br>\n",
    "\n",
    "**文档推理需要解决的关键问题是：**\n",
    "\n",
    "* 如何将文档分割成可推理的部分？\n",
    "* 随着文档大小和数量的增加，我们如何高效地查找和考虑这些文档块？\n",
    "\n",
    "本课程将探索解决这些问题的几种方法，同时继续培养 LLM 编排技能。**本 notebook 将有助于扩展我们之前的运行链技能，实现渐进式的推理，而下一个 notebook 将介绍大规模检索的新技术。**我们将继续利用先进的开源工具，来构建标准、可集成的解决方案。\n",
    "\n",
    "文档加载框架有很多选择，整个课程中将涉及两个主要的框架：\n",
    "* [**LangChain**](https://python.langchain.com/docs/get_started/introduction) 提供了一个简单的框架，可以通过常规分块策略将 LLM 连接到您自己的数据源，并与嵌入（embedding）框架/服务一同工作。此框架最初是为支持 LLM 而开发，其优势在于链的抽象和协调智能体。\n",
    "* [**LlamaIndex**](https://gpt-index.readthedocs.io/en/stable/) 是一个数据框架，供 LLM 应用提取、构建和访问私有或领域特定数据。后来也扩展成类似 LangChain 的通用 LLM 框架，但目前它最擅长的仍然是为 LLM 处理文档，毕竟它最初就是为此设计的。\n",
    "\n",
    "十分推荐您详细了解一下 LlamaIndex 和 LangChain 各自的优势，选择最适合您的。由于 LlamaIndex 可与 LangChain *一起*使用，因此两个框架的功能[可以结合使用](https://docs.llamaindex.ai/en/stable/community/integrations/using_with_langchain.html)。为简单起见，我们将在本课程中一直用 LangChain，对 LlamaIndex 感兴趣的学员可以通过 [**NVIDIA/GenerativeAIExamples 代码库**](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RetrievalAugmentedGeneration/notebooks)进一步探索。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310462b-f215-4d00-9d59-e613921bed0a",
   "metadata": {
    "id": "3310462b-f215-4d00-9d59-e613921bed0a"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 2 部分：** 加载文档\n",
    "\n",
    "LangChain 提供了各种[文档加载器](https://python.langchain.com/docs/integrations/document_loaders)以从不同的来源和位置（本地存储、私有 s3 存储桶、公共网站、消息 API 等）读取各种文档（HTML、PDF、代码）。这些加载程序会查询您的数据源并返回包含内容和元数据的 `Document` 对象，通常是纯文本或人类可读的格式。有许多可用的文档加载程序，[这里](https://python.langchain.com/docs/integrations/document_loaders)列出了 LangChain 的几个第一方选项。\n",
    "\n",
    "**在本示例中，我们可以使用以下 LangChain 的加载器之一加载研究论文：**\n",
    "* [`UnstructuredFileLoader`](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file)：适用于任意文件的文档加载器，不会对文档结构做太多假设，通常够用了。\n",
    "* [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv)：一个更专业的文件加载程序，可以直接与 Arxiv 接口通信。[仅举一个例子](https://python.langchain.com/docs/integrations/document_loaders)，这将对数据做出更多假设，以生成更好的解析并自动填充元数据（当您有多种文档/格式时很有用）。\n",
    "\n",
    "我们的代码示例会默认使用 `ArxivLoader` 来加载 [MRKL](https://arxiv.org/abs/2205.00445) 或 [ReAct](https://arxiv.org/abs/2210.03629) 文章，您很有可能在继续研究聊天模型的某个阶段读到这些论文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4382b61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3944,
     "status": "ok",
     "timestamp": 1703112979370,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "b4382b61",
    "outputId": "d6e95b9b-97be-4984-a9fd-58a528091146"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "## Loading in the file\n",
    "\n",
    "## Unstructured File Loader: Good for arbitrary \"probably good enough\" loader\n",
    "# documents = UnstructuredFileLoader(\"llama2_paper.pdf\").load()\n",
    "\n",
    "## More specialized loader, won't work for everything, but simple API and usually better results\n",
    "documents = ArxivLoader(query=\"2404.16130\").load()  ## GraphRAG\n",
    "# documents = ArxivLoader(query=\"2404.03622\").load()  ## Visualization-of-Thought\n",
    "# documents = ArxivLoader(query=\"2404.19756\").load()  ## KAN: Kolmogorov-Arnold Networks\n",
    "# documents = ArxivLoader(query=\"2404.07143\").load()  ## Infini-Attention\n",
    "# documents = ArxivLoader(query=\"2210.03629\").load()  ## ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hw0SL--6cirp",
   "metadata": {
    "id": "hw0SL--6cirp"
   },
   "source": [
    "<br>\n",
    "\n",
    "从导入中我们可以看到，此连接器（connector）允许我们访问两个不同的组件：\n",
    "* `page_content` 实际上就是人类可读格式的文档正文。\n",
    "* `metadata` 是连接器通过其数据源提供的文档相关信息。\n",
    "\n",
    "下面我们就来看看文档正文中都包括什么，您可能会注意到文档的长度有点不受控制了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1703113455184,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
    "outputId": "98b9ef68-c36b-478f-9bbb-1e45b2c49d60"
   },
   "outputs": [],
   "source": [
    "## Printing out a sample of the content\n",
    "print(\"Number of Documents Retrieved:\", len(documents))\n",
    "print(f\"Sample of Document 1 Content (Total Length: {len(documents[0].page_content)}):\")\n",
    "print(documents[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1JjUK2ZSd0HL",
   "metadata": {
    "id": "1JjUK2ZSd0HL"
   },
   "source": [
    "<br>  \n",
    "\n",
    "相比之下，元数据的大小更精简，足够作为您的聊天模型的上下文组件了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Py2lbRXlcX81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1703112982386,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "Py2lbRXlcX81",
    "outputId": "07197dd4-1609-4ecf-ae54-cf6ef3d25458"
   },
   "outputs": [],
   "source": [
    "pprint(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046ea74-0b81-400e-8364-449f421d2add",
   "metadata": {
    "id": "7046ea74-0b81-400e-8364-449f421d2add"
   },
   "source": [
    "<br>  \n",
    "\n",
    "尽管按原样保留元数据格式并完全忽略正文听起来好像很有吸引力，但如果不深入完整文本，就无法获取一些关键特征：\n",
    "* **不一定保证有元数据。**对于 `arxiv`，论文摘要、标题、作者和日期是提交文章所必须包含的信息，因此能查到它们并不稀奇。但对于任意的一个 PDF 或网页，情况就不一定了。\n",
    "* **智能体将无法更深入地了解文档内容。**摘要方便我们了解信息，可以按原样保留，但它并不提供与正文进行任意交互的能力。\n",
    "* **智能体仍然无法同时推理太多文档。**也许在 MRKL/ReAct 示例中，您可以将这两个摘要合并到一个上下文中提问。那当您需要同时与 5 个文档进行交互时怎么办呢？整个目录呢？您很快就会发现，哪怕只是总结和列出这些文档，都会使上下文窗口信息超载！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0449e4",
   "metadata": {
    "id": "4e0449e4"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 3 部分** 转换文档\n",
    "\n",
    "文档加载完成后，要是我们想把它作为上下文传给 LLM，通常要先做一步转换。一种转换方法是**分块**（chunking），它把大段的内容分解成小段。这种技巧很有用，因为它能[优化从向量数据库返回内容的相关性](https://www.pinecone.io/learn/chunking-strategies/)。\n",
    "\n",
    "LangChain 提供了[各种文档转换器](https://python.langchain.com/docs/integrations/document_transformers/)，我们将使用 [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)。它将帮我们遵循一些自然的停止点来分割文档（越多越好）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1703112527056,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
    "outputId": "a4e666e5-5a5c-413b-f5a4-acca742d80d8"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "## Some nice custom preprocessing\n",
    "# documents[0].page_content = documents[0].page_content.replace(\". .\", \"\")\n",
    "docs_split = text_splitter.split_documents(documents)\n",
    "\n",
    "# def include_doc(doc):\n",
    "#     ## Some chunks will be overburdened with useless numerical data, so we'll filter it out\n",
    "#     string = doc.page_content\n",
    "#     if len([l for l in string if l.isalpha()]) < (len(string)//2):\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# docs_split = [doc for doc in docs_split if include_doc(doc)]\n",
    "print(len(docs_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1703112530925,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
    "outputId": "1cf24605-65bb-40a2-e7aa-e2d9a8fb6382"
   },
   "outputs": [],
   "source": [
    "for i in (0, 1, 2, 15, -1):\n",
    "    pprint(f\"[Document {i}]\")\n",
    "    print(docs_split[i].page_content)\n",
    "    pprint(\"=\"*64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2f969-72cd-4d0e-a150-e3efafc1cdfc",
   "metadata": {
    "id": "57e2f969-72cd-4d0e-a150-e3efafc1cdfc"
   },
   "source": [
    "<br>  \n",
    "\n",
    "我们现在用的的分块方法很朴素，但至少能看出来我们可以轻松地让应用跑起来。我们为得到较小的分块做了一些努力，以便模型能有效地将其作为上下文，但我们要怎么把这些块用于推理呢？\n",
    "\n",
    "**在针对一组任意文档扩展和优化此方法时，可以借鉴以下做法：**\n",
    "\n",
    "* 识别逻辑中断的技术，以及合成文本的技术（手动、自动、或者 LLM 辅助）。\n",
    "* 构建信息丰富且独特的数据块，避免冗余，从而最大限度提高数据库效用。\n",
    "* 自定义分块以适应文档的特性，确保分块与上下文相关且一致。\n",
    "* 在每个数据块中都放入关键概念、关键词或元数据片段，以提高数据库的可搜索性和相关性。\n",
    "* 持续评估分块效果，并随时准备调整策略，以在分块大小和内容丰富度间取得最佳平衡。\n",
    "* 考虑使用层级结构（隐式生成或明确指定）来改进检索。\n",
    "\t+ 如果您有兴趣，可以查看[索引指南中的 LlamaIndex 树结构](https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide.html#tree-index)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-0QApYgNbyJD",
   "metadata": {
    "id": "-0QApYgNbyJD"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 4 部分：[练习]** 重构摘要（Refining Summaries）\n",
    "\n",
    "为了自动推理大量文档，一个可能的想法是用 LLM 创建一个压缩的摘要或知识库。与我们在上一个 notebook 中通过填充槽位来维护对话历史记录类似，保持整个文档的历史记录会有什么问题么？\n",
    "\n",
    "本节我们将重点介绍一个令人兴奋的 LLM 应用：**大规模自动重构（refine）、转换（coerce）和整合（consolidate）数据**。具体来说，我们将实现一个简单但实用的运行时，用 while 循环和运行状态链机制来总结一系列文档块。这个过程通常被称为[**“文档重构”**](https://python.langchain.com/docs/modules/chains/document/refine)，很大程度上类似于我们之前的对话填槽练习。唯一的区别是我们现在处理的是大型文档，而不是进行中的聊天记录。\n",
    "\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_refine.png\" width=1000px/>\n",
    ">\n",
    "> From [**Refine | LangChain**🦜️🔗](https://python.langchain.com/docs/modules/chains/document/refine)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **DocumentSummaryBase 模型**\n",
    "\n",
    "就像上一个 notebook 的 `KnowledgeBase` 类，我们可以创建一个 `DocumentSummaryBase` 结构来封装文档。下面的例子就将用 `running_summary` 字段来查询模型以获得最终摘要，同时用 `main_ideas` 和 `loose_ends` 字段来防止摘要演进过快。我们需要通过提示工程来保证这一点，因此用到了 `summary_prompt`。您可以根据需要对其进行修改，以适用于您的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gE8y2JvLvZ5T",
   "metadata": {
    "id": "gE8y2JvLvZ5T"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class DocumentSummaryBase(BaseModel):\n",
    "    running_summary: str = Field(\"\", description=\"Running description of the document. Do not override; only update!\")\n",
    "    main_ideas: List[str] = Field([], description=\"Most important information from the document (max 3)\")\n",
    "    loose_ends: List[str] = Field([], description=\"Open questions that would be good to incorporate into summary, but that are yet unknown (max 3)\")\n",
    "\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are generating a running summary of the document. Make it readable by a technical user.\"\n",
    "    \" After this, the old knowledge base will be replaced by the new one. Make sure a reader can still understand everything.\"\n",
    "    \" Keep it short, but as dense and useful as possible! The information should flow from chunk to (loose ends or main ideas) to running_summary.\"\n",
    "    \" The updated knowledge base keep all of the information from running_summary here: {info_base}.\"\n",
    "    \"\\n\\n{format_instructions}. Follow the format precisely, including quotations and commas\"\n",
    "    \"\\n\\nWithout losing any of the info, update the knowledge base with the following: {input}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7LkjfpOAvlEd",
   "metadata": {
    "id": "7LkjfpOAvlEd"
   },
   "source": [
    "<br>  \n",
    "\n",
    "现在正好可以用上前一个 notebook 的 `RExtract` 函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "khRhVghHxBaz",
   "metadata": {
    "id": "khRhVghHxBaz"
   },
   "outputs": [],
   "source": [
    "def RExtract(pydantic_class, llm, prompt):\n",
    "    '''\n",
    "    Runnable Extraction module\n",
    "    Returns a knowledge dictionary populated by slot-filling extraction\n",
    "    '''\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
    "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
    "    def preparse(string):\n",
    "        if '{' not in string: string = '{' + string\n",
    "        if '}' not in string: string = string + '}'\n",
    "        string = (string\n",
    "            .replace(\"\\\\_\", \"_\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\"\\]\", \"]\")\n",
    "            .replace(\"\\[\", \"[\")\n",
    "        )\n",
    "        # print(string)  ## Good for diagnostics\n",
    "        return string\n",
    "    return instruct_merge | prompt | llm | preparse | parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFtME_s4PRoW",
   "metadata": {
    "id": "oFtME_s4PRoW"
   },
   "source": [
    "<br>\n",
    "\n",
    "记住，以下代码会在 for 循环中调用正在运行的状态链来迭代文档！唯一需要实现的是 `parse_chain`，它需要能够将状态正确地传递到 `RExtract`。之后，系统应能正常工作，动态地维护文档摘要（针对使用的模型，可能需要调整提示词）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6sODIfHUgz6m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79192,
     "status": "ok",
     "timestamp": 1703112894722,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6sODIfHUgz6m",
    "outputId": "7b5aee70-078b-458e-d2a7-e8601b789fb1"
   },
   "outputs": [],
   "source": [
    "latest_summary = \"\"\n",
    "\n",
    "## TODO: Use the techniques from the previous notebook to complete the exercise\n",
    "def RSummarizer(knowledge, llm, prompt, verbose=False):\n",
    "    '''\n",
    "    Exercise: Create a chain that summarizes\n",
    "    '''\n",
    "    ###########################################################################################\n",
    "    ## START TODO:\n",
    "\n",
    "    def summarize_docs(docs):        \n",
    "        ## TODO: Initialize the parse_chain appropriately; should include an RExtract instance.\n",
    "        ## HINT: You can get a class using the <object>.__class__ attribute...\n",
    "        parse_chain = RunnableAssign({'info_base' : (lambda x: None)})\n",
    "        ## TODO: Initialize a valid starting state. Should be similar to notebook 4\n",
    "        state = {}\n",
    "\n",
    "        global latest_summary  ## If your loop crashes, you can check out the latest_summary\n",
    "        \n",
    "        for i, doc in enumerate(docs):\n",
    "            ## TODO: Update the state as appropriate using your parse_chain component\n",
    "\n",
    "            assert 'info_base' in state \n",
    "            if verbose:\n",
    "                print(f\"Considered {i+1} documents\")\n",
    "                pprint(state['info_base'])\n",
    "                latest_summary = state['info_base']\n",
    "                clear_output(wait=True)\n",
    "\n",
    "        return state['info_base']\n",
    "        \n",
    "    ## END TODO\n",
    "    ###########################################################################################\n",
    "    \n",
    "    return RunnableLambda(summarize_docs)\n",
    "\n",
    "# instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\").bind(max_tokens=4096)\n",
    "instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\").bind(max_tokens=4096)\n",
    "instruct_llm = instruct_model | StrOutputParser()\n",
    "\n",
    "## Take the first 10 document chunks and accumulate a DocumentSummaryBase\n",
    "summarizer = RSummarizer(DocumentSummaryBase(), instruct_llm, summary_prompt, verbose=True)\n",
    "summary = summarizer.invoke(docs_split[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb5710-23f7-4782-84eb-1fc8f73500b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(latest_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tKtoLf6DPv4Z",
   "metadata": {
    "id": "tKtoLf6DPv4Z"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 5 部分：** 处理合成数据\n",
    "\n",
    "在结束对使用 LLM 做文档摘要的探索之前，值得花点时间探讨一下更广的场景和潜在挑战。我们已经展示了一种能提取出简洁、有意义摘要的方法，现在考虑一下为什么这是个至关重要的方法，以及其中涉及的复杂性。\n",
    "\n",
    "#### **泛化重构**\n",
    "\n",
    "需要注意，这种“渐进式总结”技术只是一种入门级链，对初始数据和所需的输出格式几乎不做假设。相同的技术可以泛化到用来生成包含已知元数据、存在大量假设和下游目标的合成内容上。\n",
    "\n",
    "**考虑以下潜在应用：**\n",
    "\n",
    "1. **聚合数据**（Aggregating Data）：构建一种结构，将原始数据从文档块转换为一致、有用的摘要。\n",
    "2. **分类和子主题分析**：创建一个系统，将洞察从数据块提炼到预定的类别中，并跟踪其中新出现的子主题。\n",
    "3. **整合为密集信息块**：优化这些结构，将洞察提炼为紧凑的段，通过直接引用进行更深入的分析。\n",
    "\n",
    "这些应用都预示需要创建一个聊天模型能访问的**特定领域知识图谱**。已经有一些应用可以自动化地生成图谱了，比如 [**LangChain 知识图谱**](https://python.langchain.com/docs/modules/memory/types/kg)。虽然您可能需要开发层次化的（hierachical）结构和工具来构建和遍历这种结构，但如果能为您的场景构建出一个好用的知识图谱，那一点都不亏！对那些有兴趣构建高级知识图谱（依赖更庞大系统和向量相似性）的学员，我们发现 [**LangChain x Neo4j 文章**](https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/) 很有帮助。\n",
    "\n",
    "### **大规模数据处理的挑战**\n",
    "\n",
    "虽然我们的方法让人很有想象空间，但它仍然在处理大量数据时面临挑战：\n",
    "\n",
    "* **预处理的局限**：尽管做摘要是相对简单的，但开发在各种场景都普遍有效的层次结构是很有挑战的。\n",
    "* **粒度和引导（navigation）成本**：在层次结构中实现精细的粒度可能需要大量资源，需要复杂的整合或很多分支来保持每次交互的可管理上下文大小。\n",
    "* **对精确指令执行的依赖**：用我们当前的工具引导层次结构的检索，很依赖在强大的指令微调模型上做大量提示工程。推理延迟和参数预测中的错误风险可能会很大，因此用 LLM 实现会成为一个挑战。\n",
    "\n",
    "在您继续学习课程的过程中，想想这些挑战是靠什么技术解决的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdFSMXOVRzEa",
   "metadata": {
    "id": "cdFSMXOVRzEa"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 6 部分：** 总结\n",
    "\n",
    "此 notebook 的目标是介绍聊天模型如何处理大型文档。下一个 notebook，我们将研究一个有不同优劣势的补充工具：**使用嵌入模型进行语义检索**。\n",
    "\n",
    "### <font color=\"#76b900\">**非常好！**</font>\n",
    "\n",
    "### **接下来：**\n",
    "1. **[可选]** 回顾 notebook 顶部的“思考问题”。\n",
    "2. **[可选]** 此 notebook 包含一些基本的文档处理链，但不涉及 [Map Reduce](https://python.langchain.com/docs/modules/chains/document/map_reduce) 和 [Map Rerank](https://python.langchain.com/docs/modules/chains/document/map_rerank) 链，它们也非常有用，而且是基于大致相同的直觉构建的。了解一下有助于您加深理解！"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
