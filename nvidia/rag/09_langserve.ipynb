{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa527d2",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12504020",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 9:** LangServe 和评估</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "## LangServe 服务器设置\n",
    "\n",
    "这个 notebook 是为那些对使用 LangChain 和 [**LangServe**](https://python.langchain.com/docs/langserve) 开发交互式 Web 应用感兴趣的人提供的一个游乐场。目的是提供一个最小代码示例，展示 LangChain 在 Web 应用场景中的潜力。\n",
    "\n",
    "本节提供了一个使用 LangChain 的运行时（Runnable）接口与 FastAPI 设置简单 API 服务器的流程示例。这个示例演示了如何集成 LangChain 模型，例如 `ChatNVIDIA`，来创建和分发可访问的 API 路由。通过这个，您将能够为前端服务的 [**`frontend_server.py`**](frontend/frontend_server.py) 会话提供功能，该会话期望：\n",
    "- 一个名为 `:9012/basic_chat` 的简单入口，用于基本聊天机器人，示例如下。\n",
    "- 一对名为 `:9012/retriever` 和 `:9012/generator` 的入口，用于 RAG 聊天机器人。\n",
    "- 全部三个入口用于进行**评估**。*稍后会详细说明！*\n",
    "\n",
    "**重要提示：**\n",
    "- 确保点击方框（ $\\square$ ）按钮两次以关闭运行中的 FastAPI 单元。第一次点击可能会失败或在异步进程上触发 try-catch。\n",
    "- 如果仍然无法正常工作，请用 **Kernel -> Restart Kernel** 对这个 notebook 进行强制重启。\n",
    "- 当 FastAPI 服务器在您的单元中运行时，预计该进程会阻塞这个 notebook。其它 notebook 不应该受到影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5605b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **第 1 部分：** 提供 /basic_chat 入口\n",
    "\n",
    "提供了作为独立 Python 文件启动 `/basic_chat` 入口的说明。这将被前端用于做出基本决策，而无需内部推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e62131",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile server_app.py\n",
    "# https://python.langchain.com/docs/langserve#server\n",
    "from fastapi import FastAPI\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langserve import add_routes\n",
    "\n",
    "## May be useful later\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch, RunnablePassthrough\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "## TODO: Make sure to pick your LLM and do your prompt engineering as necessary for the final assessment\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\")\n",
    "\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "## PRE-ASSESSMENT: Run as-is and see the basic chain in action\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    instruct_llm,\n",
    "    path=\"/basic_chat\",\n",
    ")\n",
    "\n",
    "## ASSESSMENT TODO: Implement these components as appropriate\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    RunnableLambda(lambda x: \"Not Implemented\"),\n",
    "    path=\"/generator\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    RunnableLambda(lambda x: []),\n",
    "    path=\"/retriever\",\n",
    ")\n",
    "\n",
    "## Might be encountered if this were for a standalone python file...\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=9012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc272f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Works, but will block the notebook.\n",
    "!python server_app.py  \n",
    "\n",
    "## Will technically work, but not recommended in a notebook. \n",
    "## You may be surprised at the interesting side effects...\n",
    "# import os\n",
    "# os.system(\"python server_app.py &\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b84fbc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **第 2 部分：** 使用服务器：\n",
    "\n",
    "虽然在 Google Colab 中无法轻松使用（或者说没有很多特殊技巧的话），但上面的脚本会激活一个与 notebook 进程绑定的运行服务器。服务器运行时，请不要尝试使用这个 notebook（除了关闭/重启服务）。\n",
    "\n",
    "不过在另一个文件中，您应该能够使用以下接口访问 `basic_chat` 入口：\n",
    "\n",
    "```python\n",
    "from langserve import RemoteRunnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = RemoteRunnable(\"http://0.0.0.0:9012/basic_chat/\") | StrOutputParser()\n",
    "for token in llm.stream(\"Hello World! How is it going?\"):\n",
    "    print(token, end='')\n",
    "```\n",
    "\n",
    "**请在另一个文件中试试看，看看它是否能工作！**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee27d4d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **第 3 部分：最终评估**\n",
    "\n",
    "**这个 notebook 将用于完成最终评估！** 在您完成课程后，我们建议克隆这个 notebook，打开一个新标签页并实现 Evaluate 功能，通过实现上面的 `/generator` 和 `/retriever` 入口！要快速链接到前端，可以运行下面的单元："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71313fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:#76b900;\" target=\"_blank\" href='+url+'><h2>< Link To Gradio Frontend ></h2></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfebe34",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "#### **评估提示：** \n",
    "请注意，以下功能已经在前端微服务中实现。 \n",
    "```python\n",
    "## Necessary Endpoints\n",
    "chains_dict = {\n",
    "    'basic' : RemoteRunnable(\"http://lab:9012/basic_chat/\"),\n",
    "    'retriever' : RemoteRunnable(\"http://lab:9012/retriever/\"),  ## For the final assessment\n",
    "    'generator' : RemoteRunnable(\"http://lab:9012/generator/\"),  ## For the final assessment\n",
    "}\n",
    "\n",
    "basic_chain = chains_dict['basic']\n",
    "\n",
    "## Retrieval-Augmented Generation Chain\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign(\n",
    "        {'context' : itemgetter('input') \n",
    "        | chains_dict['retriever'] \n",
    "        | LongContextReorder().transform_documents\n",
    "        | docs2str\n",
    "    })\n",
    ")\n",
    "\n",
    "output_chain = RunnableAssign({\"output\" : chains_dict['generator'] }) | output_puller\n",
    "rag_chain = retrieval_chain | output_chain\n",
    "```\n",
    "**为了符合这个入口的引入策略，确保不要重复工作流的功能，只部署缺失的特性！**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
