{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dace1a29-864a-4a69-bd00-c0952a73d4ea",
   "metadata": {
    "id": "dace1a29-864a-4a69-bd00-c0952a73d4ea"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TONXsdAOC5Dn",
   "metadata": {
    "id": "TONXsdAOC5Dn"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 4:** 运行状态链（Running State Chain）</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "在上一个 notebook 中，我们介绍了运行时相关的 LangChain Expression Language（LCEL）。到目前为止，您应该已经对内部和外部推理都比较熟悉了，也了解了如何开发推理工作流！在此 notebook 中，我们将进入到更高级的范式，使我们能编排更复杂的对话管理策略、执行长文档推理。\n",
    "\n",
    "<br>\n",
    "\n",
    "### **学习目标：**\n",
    "\n",
    "* 学习利用运行时（Runnable）编排有趣的 LLM 系统。\n",
    "* 理解运行状态链在对话管理和迭代决策中的作用。\n",
    "\n",
    "<br>\n",
    "\n",
    "### **思考问题：**\n",
    "* 只有一个模块且不持续向环境索要输入的运行状态链变体会有什么用处么？\n",
    "* 您可能会注意到，JSON 预测的效果是很不错的。当然，根据问题和 JSON 格式的复杂性，它的效果可能并不总是那么好。据此您预计会遇到哪些问题？\n",
    "* 您能想到哪些可行的方法，把状态链的提示词完全替换掉？\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook 版权声明：**\n",
    "\n",
    "* 本 notebook 是 [**NVIDIA 深度学习培训中心**](https://www.nvidia.cn/training/)的课程[**《构建大语言模型 RAG 智能体》**](https://www.nvidia.cn/training/instructor-led-workshops/building-rag-agents-with-llms/)中的一部分，未经 NVIDIA 授权不得分发。\n",
    "\n",
    "<br> \n",
    "\n",
    "\n",
    "### **环境设置：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5AbHxz61Hq9x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44079,
     "status": "ok",
     "timestamp": 1703083363284,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "5AbHxz61Hq9x",
    "outputId": "b50f4635-f16e-44f9-da54-fdf108c83445"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio\n",
    "\n",
    "# import os\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-...\"\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8c16f-78af-4ea3-b1f8-e2caff0fe733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce88bae-906b-49fd-9209-141c649d5320",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful utility method for printing intermediate states\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "\n",
    "def RPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def PPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        pprint(preface, x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2e4ef-6e3a-4796-b51e-c9da0c156ded",
   "metadata": {
    "id": "0bf2e4ef-6e3a-4796-b51e-c9da0c156ded"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 1 部分：** 让变量传递下去\n",
    "\n",
    "在之前的示例中，我们能通过**创建**、**转变**（mutating）和**消耗**（consuming）状态，在独立的链中实现有趣的逻辑。这些状态以字典的形式进行传递，其中的键值对有实际含义，它们将用于为后续步骤提供操作所需的信息！\n",
    "\n",
    "**回顾上一个 notebook 中的零样本分类示例：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa215f6-050c-4062-a455-feffd77daaeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5829,
     "status": "ok",
     "timestamp": 1703083414406,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "7aa215f6-050c-4062-a455-feffd77daaeb",
    "outputId": "df76d362-e2b9-437c-910f-aa1127cd3f44"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## ^^ This notebook is timed, which will print out how long it all took\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List, Union\n",
    "from operator import itemgetter\n",
    "\n",
    "## Zero-shot classification prompt and chain w/ explicit few-shot prompting\n",
    "sys_msg = (\n",
    "    \"Choose the most likely topic classification given the sentence as context.\"\n",
    "    \" Only one word, no explanation.\\n[Options : {options}]\"\n",
    ")\n",
    "\n",
    "zsc_prompt = ChatPromptTemplate.from_template(\n",
    "    f\"{sys_msg}\\n\\n\"\n",
    "    \"[[The sea is awesome]][/INST]boat</s><s>[INST]\"\n",
    "    \"[[{input}]]\"\n",
    ")\n",
    "\n",
    "## Define your simple instruct_model\n",
    "instruct_chat = ChatNVIDIA(model=\"mistralai/mistral-7b-instruct-v0.2\")\n",
    "instruct_llm = instruct_chat | StrOutputParser()\n",
    "one_word_llm = instruct_chat.bind(stop=[\" \", \"\\n\"]) | StrOutputParser()\n",
    "\n",
    "\n",
    "## Function that just prints out the first word of the output. With early stopping bind\n",
    "zsc_chain = zsc_prompt | one_word_llm\n",
    "\n",
    "## Function that just prints out the first word of the output. With early stopping bind\n",
    "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
    "    return zsc_chain.invoke({\"input\" : input, \"options\" : options}).split()[0]\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4HYI7SIxLs5W",
   "metadata": {
    "id": "4HYI7SIxLs5W"
   },
   "source": [
    "这个链做出了几个使其易于使用的设计决策，最关键的如下：\n",
    "\n",
    "**我们希望它像函数一样，因此我们只需要它能生成并返回输出。**\n",
    "\n",
    "这使得该链能很自然地成为更大的链系统中的一个模块。比如，以下链将接受字符串，提取最有可能的主题，然后根据主题生成新句子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LuldQwa_PQNS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "executionInfo": {
     "elapsed": 3827,
     "status": "ok",
     "timestamp": 1703083418214,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "LuldQwa_PQNS",
    "outputId": "cdca3240-8037-4b31-e336-02a271017b6b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## ^^ This notebook is timed, which will print out how long it all took\n",
    "gen_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Make a new sentence about the the following topic: {topic}. Be creative!\"\n",
    ")\n",
    "\n",
    "gen_chain = gen_prompt | instruct_llm\n",
    "\n",
    "input_msg = \"I get seasick, so I think I'll pass on the trip\"\n",
    "options = [\"car\", \"boat\", \"airplane\", \"bike\"]\n",
    "\n",
    "chain = (\n",
    "    ## -> {\"input\", \"options\"}\n",
    "    {'topic' : zsc_chain}\n",
    "    | RPrint()\n",
    "    ## -> {**, \"topic\"}\n",
    "    | gen_chain\n",
    "    ## -> string\n",
    ")\n",
    "\n",
    "chain.invoke({\"input\" : input_msg, \"options\" : options})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1zzDhPsfXRhn",
   "metadata": {
    "id": "1zzDhPsfXRhn"
   },
   "source": [
    "<br>\n",
    "\n",
    "但如果您想让信息能够继续向下游传递，就会出问题，因为我们在生成响应时丢失了主题和输入变量。如果我们想同时根据输出和输入执行某些操作，就需要确保这两个变量都能传递下去。\n",
    "\n",
    "幸运的是，我们可以用运行时映射（从字典构建或者直接用 RunnableMap），将链的输出也分配给一个键（key），随着其它键一起传递下去。或者也可以用 `RunnableAssign`，它默认情况下会将需要消耗状态的链的输出与输入字典合并起来。\n",
    "\n",
    "借助这种技巧，我们就能通过链系统传递任何内容了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MfXm7oT5XVOd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1703083424179,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "MfXm7oT5XVOd",
    "outputId": "895b237d-dd15-4c7c-8d0e-b25be664e4a3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## ^^ This notebook is timed, which will print out how long it all took\n",
    "\n",
    "from langchain.schema.runnable import RunnableBranch, RunnablePassthrough\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from functools import partial\n",
    "\n",
    "big_chain = (\n",
    "    PPrint()\n",
    "    ## Manual mapping. Can be useful sometimes and inside branch chains\n",
    "    | {'input' : lambda d: d.get('input'), 'topic' : zsc_chain}\n",
    "    | PPrint()\n",
    "    ## RunnableAssign passing. Better for running state chains by default\n",
    "    | RunnableAssign({'generation' : gen_chain})\n",
    "    | PPrint()\n",
    "    ## Using the input and generation together\n",
    "    | RunnableAssign({'combination' : (\n",
    "        ChatPromptTemplate.from_template(\n",
    "            \"Consider the following passages:\"\n",
    "            \"\\nP1: {input}\"\n",
    "            \"\\nP2: {generation}\"\n",
    "            \"\\n\\nCombine the ideas from both sentences into one simple one.\"\n",
    "        )\n",
    "        | instruct_llm\n",
    "    )})\n",
    ")\n",
    "\n",
    "output = big_chain.invoke({\n",
    "    \"input\" : \"I get seasick, so I think I'll pass on the trip\",\n",
    "    \"options\" : [\"car\", \"boat\", \"airplane\", \"bike\", \"unknown\"]\n",
    "})\n",
    "pprint(\"Final Output: \", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tHONiEtGK2qv",
   "metadata": {
    "id": "tHONiEtGK2qv"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 2 部分：** 运行状态链\n",
    "\n",
    "上面只是一个示例，它展示出了将许多 LLM 调用连接在一起进行内部推理时的缺陷。但让信息在链中传递的能力，对那些需要积累状态信息、或多次执行的复杂链至关重要。\n",
    "\n",
    "具体来说，一个非常简单但有效的链是**运行状态链**，它有以下属性：\n",
    "* “运行状态”（running state）是一个包含了系统关注的所有变量的字典。\n",
    "* “分支”（branch）是一条可以拉取运行状态并生成响应的链。\n",
    "* “分支”只能在 **RunnableAssign** 内运行，分支的输入应来自**运行状态**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nGOBMbF-phA6",
   "metadata": {
    "id": "nGOBMbF-phA6"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/running_state_chain.png\" width=1000px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70M-ZOpMpinO",
   "metadata": {
    "id": "70M-ZOpMpinO"
   },
   "source": [
    "您可以抽象地将运行状态链看作 Python 类的变体，其中包含状态变量（或者说属性）和函数（或者说方法）。\n",
    "* 这种链像一个囊括了所有功能的抽象类。\n",
    "* 运行状态类似于属性（应使其始终可访问）。\n",
    "* 分支类似于类方法（可以选择要用的属性）。\n",
    "* `.invoke` 或类似的方法就像是按顺序在分支中运行 `__call__` 方法。\n",
    "\n",
    "**通过在链中强制实行此范式：**\n",
    "* 您可以让状态变量在链中传播，允许内部逻辑访问任何需要的内容，并累积状态值以供之后使用。\n",
    "* 您还可以将链的输出再次作为您的输入，从而创建出一个“while-loop”式的根据运行状态不断构建的链。\n",
    "\n",
    "此 notebook 的剩余部分将包括两个练习，它们能很好的体现运行状态链抽象：**知识库**和**数据库查询聊天机器人**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "--3BDo7-wFBF",
   "metadata": {
    "id": "--3BDo7-wFBF"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 3 部分：** 使用运行状态链实现知识库\n",
    "\n",
    "在了解了运行状态链的基本结构和原理后，我们就可以探索怎么扩展此方法以管理更复杂的任务了，尤其是在创建通过交互而演进的动态系统时。本节将重点介绍如何通过 **JSON 格式的槽位填充**（JSON-Enabled Slot Filling）实现**知识库**：\n",
    "\n",
    "* **知识库：**存储 LLM 需要关注的信息的存储库。\n",
    "* **JSON 格式的槽位填充：**要求指令微调过的模型输出带有槽位的 JSON 格式（可以包括字典），再用 LLM 为这些槽位填充有用的相关信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imHxfz52wSgY",
   "metadata": {
    "id": "imHxfz52wSgY"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### **定义知识库**\n",
    "\n",
    "为了构建一个高响应的智能系统，我们需要一种不仅能处理输入，还能通过对话流保留和更新基本信息的方法。这正是我们需要结合 LangChain 和 Pydantic 的地方。作为热门的 Python 验证库，[**Pydantic**](https://docs.pydantic.dev/latest/) 在构建和验证数据模型方面发挥了重要作用。Pydantic 的功能之一是提供结构化的“模型（model）”类，这些类可以通过简洁的语法和自定义来验证数据、类等对象。此框架在整个 LangChain 中都有广泛使用，是数据转换的必要组件。\n",
    "\n",
    "“模型”擅长的就是定义一个包含预期参数的类，并能通过一些特别的方法验证它们！我们不会在课程中介绍过多关于验证脚本的情况，感兴趣的学员可以查看 [**Pydantic 验证器指南**](https://docs.pydantic.dev/1.10/usage/validators/)（很快就会变得深入起来）。我们只需要知道怎么用就可以了，可以像下面所示，构建一个 `BaseModel` 类，再定义一些 `Field` 变量来创建结构化知识库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WWaYkSP9wQaF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1703083424179,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "WWaYkSP9wQaF",
    "outputId": "76d4c995-4423-478b-9c66-01b48a27c866"
   },
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Dict, Union, Optional\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    ## Fields of the BaseModel, which will be validated/assigned when the knowledge base is constructed\n",
    "    topic: str = Field('general', description=\"Current conversation topic\")\n",
    "    user_preferences: Dict[str, Union[str, int]] = Field({}, description=\"User preferences and choices\")\n",
    "    session_notes: str = Field(\"\", description=\"Notes on the ongoing session\")\n",
    "    unresolved_queries: list = Field([], description=\"Unresolved user queries\")\n",
    "    action_items: list = Field([], description=\"Actionable items identified during the conversation\")\n",
    "\n",
    "print(repr(KnowledgeBase(topic = \"Travel\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_g8qYKZBwYsO",
   "metadata": {
    "id": "_g8qYKZBwYsO"
   },
   "source": [
    "<br>  \n",
    "\n",
    "这种方式的优势在于，我们可以集成 LangChain 提供的其它 LLM 功能。其中一个就是 `PydanticOutputParser`，它可以根据 Pydantic 对象自动生成指令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H6uK93zwwdcn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1703083424311,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "H6uK93zwwdcn",
    "outputId": "fb4f1a54-0a3c-46e6-fe19-4f1f9282bc61"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "instruct_string = PydanticOutputParser(pydantic_object=KnowledgeBase).get_format_instructions()\n",
    "pprint(instruct_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFyxUZEMwp9s",
   "metadata": {
    "id": "oFyxUZEMwp9s"
   },
   "source": [
    "<br>\n",
    "\n",
    "此功能会生成知识库能接收的有效输入指令，帮助为 LLM 提供具体的单样本学习样例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JIGIXNfmwqAz",
   "metadata": {
    "id": "JIGIXNfmwqAz"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### **可运行的提取模块**\n",
    "\n",
    "现在我们已经知道可以用 Pydantic 对象来生成良好的 LLM 指令，那就可以创建一个封装了 Pydantic 类的运行时，构建从提示词、生成结果，到更新知识库的流水线了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zya1sucNwwIO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3469,
     "status": "ok",
     "timestamp": 1703086115894,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "Zya1sucNwwIO",
    "outputId": "fb5a0b21-dcfc-4e6f-f621-a5466047fc3f"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Definition of RExtract\n",
    "def RExtract(pydantic_class, llm, prompt):\n",
    "    '''\n",
    "    Runnable Extraction module\n",
    "    Returns a knowledge dictionary populated by slot-filling extraction\n",
    "    '''\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
    "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
    "    def preparse(string):\n",
    "        if '{' not in string: string = '{' + string\n",
    "        if '}' not in string: string = string + '}'\n",
    "        string = (string\n",
    "            .replace(\"\\\\_\", \"_\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\"\\]\", \"]\")\n",
    "            .replace(\"\\[\", \"[\")\n",
    "        )\n",
    "        # print(string)  ## Good for diagnostics\n",
    "        return string\n",
    "    return instruct_merge | prompt | llm | preparse | parser\n",
    "\n",
    "################################################################################\n",
    "## Practical Use of RExtract\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Update the knowledge base: {format_instructions}. Only use information from the input.\"\n",
    "    \"\\n\\nNEW MESSAGE: {input}\"\n",
    ")\n",
    "\n",
    "extractor = RExtract(KnowledgeBase, instruct_llm, parser_prompt)\n",
    "\n",
    "knowledge = extractor.invoke({'input' : \"I love flowers so much! The orchids are amazing! Can you buy me some?\"})\n",
    "pprint(knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Rjncqne2-zm",
   "metadata": {
    "id": "6Rjncqne2-zm"
   },
   "source": [
    "<br>\n",
    "\n",
    "提醒一点，由于 LLM 预测的模糊性，这个过程可能会失败，尤其是未经指令优化的模型！这个时候，有一个擅长遵循指令的 LLM 、加入额外的检查和纠错机制是非常重要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qePT8nsdDskr",
   "metadata": {
    "id": "qePT8nsdDskr"
   },
   "source": [
    "<br>  \n",
    "\n",
    "#### **动态更新知识库**\n",
    "\n",
    "最后，我们可以创建一个能在整个对话过程中持续更新知识库的系统。这是通过将知识库的当前状态及用户输入持续反馈给系统来实现的。\n",
    "\n",
    "以下是一个示例，展示了该方案填充细节的能力，也能看出让模型做填充相较于完成一般响应任务的局限性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8yKtrLeBDu35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6433,
     "status": "ok",
     "timestamp": 1703083433913,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "8yKtrLeBDu35",
    "outputId": "282c71b5-8056-4115-d4c1-70254a1e9caf"
   },
   "outputs": [],
   "source": [
    "class KnowledgeBase(BaseModel):\n",
    "    firstname: str = Field('unknown', description=\"Chatting user's first name, unknown if unknown\")\n",
    "    lastname: str = Field('unknown', description=\"Chatting user's last name, unknown if unknown\")\n",
    "    location: str = Field('unknown', description=\"Where the user is located\")\n",
    "    hints: str = Field('unknown', description=\"Hints to help answer other questions\")\n",
    "    response: str = Field('unknown', description=\"An ideal response to the user based on their new message\")\n",
    "\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are chatting with a user. The user just responded. Please update the knowledge base based on the response.\"\n",
    "    \" This information will be acted on to respond to the user in the next interaction.\"\n",
    "    \" Do not hallucinate any details, and make sure the knowledge base is not redundant.\"\n",
    "    \" Update the entries frequently to adapt to the conversation flow.\"\n",
    "    \"\\n{format_instructions}\"\n",
    "    \"\\n\\nOLD KNOWLEDGE BASE: {know_base}\"\n",
    "    \"\\n\\nNEW MESSAGE: {input}\"\n",
    "    \"\\n\\nNEW KNOWLEDGE BASE:\"\n",
    ")\n",
    "\n",
    "## Switch to a more powerful base model\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\") | StrOutputParser()\n",
    "\n",
    "extractor = RExtract(KnowledgeBase, instruct_llm, parser_prompt)\n",
    "info_update = RunnableAssign({'know_base' : extractor})\n",
    "\n",
    "## Initialize the knowledge base and see what you get\n",
    "state = {'know_base' : KnowledgeBase()}\n",
    "state['input'] = \"My name is Carmen Sandiego! Guess where I am! Hint: It's somewhere in the United States.\"\n",
    "state = info_update.invoke(state)\n",
    "pprint(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0415ceac-bd40-4bc6-a3a1-c66f863c258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state['input'] = \"I'm in a place considered the birthplace of Jazz.\"\n",
    "state = info_update.invoke(state)\n",
    "pprint(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e0a259-3695-47ea-8e35-9288e2f28f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state['input'] = \"Yeah, I'm in New Orleans... How did you know?\"\n",
    "pprint(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Abnm3hvCtQaF",
   "metadata": {
    "id": "Abnm3hvCtQaF"
   },
   "source": [
    "<br>\n",
    "\n",
    "这个例子就演示了如何有效地利用运行状态链来处理不断变化上下文和需求的对话，展示了它如何成为开发复杂交互式系统的强大工具。\n",
    "\n",
    "此 notebook 的下个部分将通过探索两个应用程序来扩展这些概念：**文档知识库**和**数据库查询聊天机器人**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa26368-6580-4a69-8abf-3fc3a6460070",
   "metadata": {
    "id": "7fa26368-6580-4a69-8abf-3fc3a6460070"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 4 部分: [练习]** Airline 客户服务机器人\n",
    "\n",
    "在本练习中，我们来扩展所学的工具，实现一个简单有效的对话管理器聊天机器人。我们将创建一个帮客户了解航班情况的航空公司客服机器人！\n",
    "\n",
    "先创建一个简单的数据库，用字典获取一些客户信息！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4965800-e71c-4d56-a16e-c8bc4ea77d55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1703094239296,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "e4965800-e71c-4d56-a16e-c8bc4ea77d55",
    "outputId": "c68c924b-cef0-4d6f-a461-8d7d4a22a904"
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "## Function that can be queried for information. Implementation details not important\n",
    "def get_flight_info(d: dict) -> str:\n",
    "    \"\"\"\n",
    "    Example of a retrieval function which takes a dictionary as key. Resembles SQL DB Query\n",
    "    \"\"\"\n",
    "    req_keys = ['first_name', 'last_name', 'confirmation']\n",
    "    assert all((key in d) for key in req_keys), f\"Expected dictionary with keys {req_keys}, got {d}\"\n",
    "\n",
    "    ## Static dataset. get_key and get_val can be used to work with it, and db is your variable\n",
    "    keys = req_keys + [\"departure\", \"destination\", \"departure_time\", \"arrival_time\", \"flight_day\"]\n",
    "    values = [\n",
    "        [\"Jane\", \"Doe\", 12345, \"San Jose\", \"New Orleans\", \"12:30 PM\", \"9:30 PM\", \"tomorrow\"],\n",
    "        [\"John\", \"Smith\", 54321, \"New York\", \"Los Angeles\", \"8:00 AM\", \"11:00 AM\", \"Sunday\"],\n",
    "        [\"Alice\", \"Johnson\", 98765, \"Chicago\", \"Miami\", \"7:00 PM\", \"11:00 PM\", \"next week\"],\n",
    "        [\"Bob\", \"Brown\", 56789, \"Dallas\", \"Seattle\", \"1:00 PM\", \"4:00 PM\", \"yesterday\"],\n",
    "    ]\n",
    "    get_key = lambda d: \"|\".join([d['first_name'], d['last_name'], str(d['confirmation'])])\n",
    "    get_val = lambda l: {k:v for k,v in zip(keys, l)}\n",
    "    db = {get_key(get_val(entry)) : get_val(entry) for entry in values}\n",
    "\n",
    "    # Search for the matching entry\n",
    "    data = db.get(get_key(d))\n",
    "    if not data:\n",
    "        return (\n",
    "            f\"Based on {req_keys} = {get_key(d)}) from your knowledge base, no info on the user flight was found.\"\n",
    "            \" This process happens every time new info is learned. If it's important, ask them to confirm this info.\"\n",
    "        )\n",
    "    return (\n",
    "        f\"{data['first_name']} {data['last_name']}'s flight from {data['departure']} to {data['destination']}\"\n",
    "        f\" departs at {data['departure_time']} {data['flight_day']} and lands at {data['arrival_time']}.\"\n",
    "    )\n",
    "\n",
    "#######################################################################################\n",
    "## Usage example. Actually important\n",
    "\n",
    "print(get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0897722-c517-4b43-834c-0604810d5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_flight_info({\"first_name\" : \"Alice\", \"last_name\" : \"Johnson\", \"confirmation\" : 98765}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847eb9a2-535d-4a9b-9b75-e698a349b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_flight_info({\"first_name\" : \"Bob\", \"last_name\" : \"Brown\", \"confirmation\" : 27494}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ac572-4f4c-49b7-abc5-d716c49a4523",
   "metadata": {
    "id": "7d0ac572-4f4c-49b7-abc5-d716c49a4523"
   },
   "source": [
    "<br>  \n",
    "\n",
    "这是一个很好用的接口，可以达成两个目的：\n",
    "* 从外部环境（数据库）获取用户的最新信息。\n",
    "* 防止未经授权的敏感信息泄露。\n",
    "\n",
    "如果我们的网络可以访问此类接口，那么就能替用户检索信息！例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799f182-a9b2-4078-a5c7-c53f9a8ef9a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "executionInfo": {
     "elapsed": 2258,
     "status": "ok",
     "timestamp": 1703083436156,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6799f182-a9b2-4078-a5c7-c53f9a8ef9a2",
    "outputId": "7591a21d-8eac-4dd6-f001-d953e12af547"
   },
   "outputs": [],
   "source": [
    "external_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a SkyFlow chatbot, and you are helping a customer with their issue.\"\n",
    "    \" Please help them with their question, remembering that your job is to represent SkyFlow airlines.\"\n",
    "    \" Assume SkyFlow uses industry-average practices regarding arrival times, operations, etc.\"\n",
    "    \" (This is a trade secret. Do not disclose).\"  ## soft reinforcement\n",
    "    \" Please keep your discussion short and sweet if possible. Avoid saying hello unless necessary.\"\n",
    "    \" The following is some context that may be useful in answering the question.\"\n",
    "    \"\\n\\nContext: {context}\"\n",
    "    \"\\n\\nUser: {input}\"\n",
    ")\n",
    "\n",
    "basic_chain = external_prompt | instruct_llm\n",
    "\n",
    "basic_chain.invoke({\n",
    "    'input' : 'Can you please tell me when I need to get to the airport?',\n",
    "    'context' : get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7288b-d931-4d3f-ba78-4bc0c5883fa0",
   "metadata": {
    "id": "18d7288b-d931-4d3f-ba78-4bc0c5883fa0"
   },
   "source": [
    "<br>\n",
    "\n",
    "这很有趣，但我们如何让这个系统在实际环境中运行？实际上，我们可以用上面的 KnowledgeBase 模式来这样提供此类信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078210a-41ab-453a-8934-ac234fb0ed6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1703086127938,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "d078210a-41ab-453a-8934-ac234fb0ed6f",
    "outputId": "5a38a98a-8875-4d7b-ff89-04c7bbc14a69"
   },
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Dict, Union\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    first_name: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
    "    last_name: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
    "    confirmation: int = Field(-1, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
    "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
    "    open_problems: list = Field([], description=\"Topics that have not been resolved yet\")\n",
    "    current_goals: list = Field([], description=\"Current goal for the agent to address\")\n",
    "\n",
    "def get_key_fn(base: BaseModel) -> dict:\n",
    "    '''Given a dictionary with a knowledge base, return a key for get_flight_info'''\n",
    "    return {  ## More automatic options possible, but this is more explicit\n",
    "        'first_name' : base.first_name,\n",
    "        'last_name' : base.last_name,\n",
    "        'confirmation' : base.confirmation,\n",
    "    }\n",
    "\n",
    "know_base = KnowledgeBase(first_name = \"Jane\", last_name = \"Doe\", confirmation = 12345)\n",
    "\n",
    "# get_flight_info(get_key_fn(know_base))\n",
    "\n",
    "get_key = RunnableLambda(get_key_fn)\n",
    "(get_key | get_flight_info).invoke(know_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c0c43-c031-4cb9-be48-a1eb4d85d289",
   "metadata": {
    "id": "c20c0c43-c031-4cb9-be48-a1eb4d85d289"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **目标：**\n",
    "\n",
    "您希望用户能够在交互对话中灵活地调用以下函数：\n",
    "\n",
    "```python\n",
    "get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}) ->\n",
    "    \"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\"\n",
    "```\n",
    "\n",
    "`RExtract` 用来使以下知识库语法能被接受：\n",
    "```python\n",
    "known_info = KnowledgeBase()\n",
    "extractor = RExtract(KnowledgeBase, InstructLLM(), parser_prompt)\n",
    "results = extractor.invoke({'info_base' : known_info, 'input' : 'My message'})\n",
    "known_info = results['info_base']\n",
    "```\n",
    "\n",
    "**设计可以实现以下功能的聊天机器人：**\n",
    "* 机器人应先进行闲聊，或许可以帮助用户处理一些不需要询问隐私信息的非敏感查询。\n",
    "* 当用户开始询问数据库中的内容时，请让用户提供必要的信息。\n",
    "* 检索成功后，智能体将能够围绕数据库中的信息继续展开对话。\n",
    "\n",
    "**这可以通过各种技术来完成，包括：**\n",
    "* **提示工程和上下文解析（Context Parsing）**，整体的聊天提示词保持大致相同，但可以通过操纵上下文来改变智能体行为。比如，可以通过注入自然语言指令来调整失败的数据库检索过程：*`\"Information could not be retrieved with keys {...}. Please ask the user for clarification or help them with known information.\"`*\n",
    "* **“提示词传递”**（Prompt Passing），将当前的提示词作为状态变量传递，并能用链中的信息覆盖。\n",
    "* **分支链**（Branching chains），例如用 [`RunnableBranch`](https://python.langchain.com/docs/expression_language/how_to/routing) 或更多定制方案来实现特定条件的路由机制。\n",
    "\t+ 这是 [`RunnableBranch`](https://python.langchain.com/docs/expression_language/how_to/routing) 类似 `switch` 的语法： \n",
    "\t```python\n",
    "\tfrom langchain.schema.runnable import RunnableBranch\n",
    "\tRunnableBranch(\n",
    "\t    ((lambda x: 1 in x), RPrint(\"Has 1 (didn't check 2): \")),\n",
    "\t    ((lambda x: 2 in x), RPrint(\"Has 2 (not 1 though): \")),\n",
    "\t    RPrint(\"Has neither 1 not 2: \")\n",
    "\t).invoke([2, 1, 3]);  ## -> Has 1 (didn't check 2): [2, 1, 3]\n",
    "\t```\n",
    "\n",
    "提供一些提示词和 gradio 循环可能会有所帮助，但智能体目前仍然只会产生幻觉！请通过实现内部的链来尝试检索相关信息。在尝试之前，可以观察一下模型的默认行为，看看它是怎么产生幻觉或忘记东西的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065dff4-ff23-4fcb-85fb-34f508f0b620",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655
    },
    "id": "c065dff4-ff23-4fcb-85fb-34f508f0b620",
    "outputId": "67f46fc6-0469-443d-cd88-8a2758d2c29d"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,       ## Wrap an implicit \"dictionary\" runnable\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, ChatMessage, AIMessage\n",
    "from typing import Iterable\n",
    "import gradio as gr\n",
    "\n",
    "external_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a chatbot for SkyFlow Airlines, and you are helping a customer with their issue.\"\n",
    "        \" Please chat with them! Stay concise and clear!\"\n",
    "        \" Your running knowledge base is: {know_base}.\"\n",
    "        \" This is for you only; Do not mention it!\"\n",
    "        \" \\nUsing that, we retrieved the following: {context}\\n\"\n",
    "        \" If they provide info and the retrieval fails, ask to confirm their first/last name and confirmation.\"\n",
    "        \" Do not ask them any other personal info.\"\n",
    "        \" If it's not important to know about their flight, do not ask.\"\n",
    "        \" The checking happens automatically; you cannot check manually.\"\n",
    "    )),\n",
    "    (\"assistant\", \"{output}\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "##########################################################################\n",
    "## Knowledge Base Things\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    first_name: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
    "    last_name: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
    "    confirmation: Optional[int] = Field(None, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
    "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
    "    open_problems: str = Field(\"\", description=\"Topics that have not been resolved yet\")\n",
    "    current_goals: str = Field(\"\", description=\"Current goal for the agent to address\")\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a chat assistant representing the airline SkyFlow, and are trying to track info about the conversation.\"\n",
    "    \" You have just recieved a message from the user. Please fill in the schema based on the chat.\"\n",
    "    \"\\n\\n{format_instructions}\"\n",
    "    \"\\n\\nOLD KNOWLEDGE BASE: {know_base}\"\n",
    "    \"\\n\\nASSISTANT RESPONSE: {output}\"\n",
    "    \"\\n\\nUSER MESSAGE: {input}\"\n",
    "    \"\\n\\nNEW KNOWLEDGE BASE: \"\n",
    ")\n",
    "\n",
    "## Your goal is to invoke the following through natural conversation\n",
    "# get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}) ->\n",
    "#     \"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\"\n",
    "\n",
    "chat_llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\") | StrOutputParser()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\") | StrOutputParser()\n",
    "\n",
    "external_chain = external_prompt | chat_llm\n",
    "\n",
    "#####################################################################################\n",
    "## START TODO: Define the extractor and internal chain to satisfy the objective\n",
    "\n",
    "## TODO: Make a chain that will populate your knowledge base based on provided context\n",
    "knowbase_getter = lambda x: KnowledgeBase()\n",
    "\n",
    "## TODO: Make a chain to pull d[\"know_base\"] and outputs a retrieval from db\n",
    "database_getter = lambda x: \"Not implemented\"\n",
    "\n",
    "## These components integrate to make your internal chain\n",
    "internal_chain = (\n",
    "    RunnableAssign({'know_base' : knowbase_getter})\n",
    "    | RunnableAssign({'context' : database_getter})\n",
    ")\n",
    "\n",
    "## END TODO\n",
    "#####################################################################################\n",
    "\n",
    "state = {'know_base' : KnowledgeBase()}\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "\n",
    "    ## Pulling in, updating, and printing the state\n",
    "    global state\n",
    "    state['input'] = message\n",
    "    state['history'] = history\n",
    "    state['output'] = \"\" if not history else history[-1][1]\n",
    "\n",
    "    ## Generating the new state from the internal chain\n",
    "    state = internal_chain.invoke(state)\n",
    "    print(\"State after chain run:\")\n",
    "    pprint({k:v for k,v in state.items() if k != \"history\"})\n",
    "    \n",
    "    ## Streaming the results\n",
    "    buffer = \"\"\n",
    "    for token in external_chain.stream(state):\n",
    "        buffer += token\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "def queue_fake_streaming_gradio(chat_stream, history = [], max_questions=8):\n",
    "\n",
    "    ## Mimic of the gradio initialization routine, where a set of starter messages can be printed off\n",
    "    for human_msg, agent_msg in history:\n",
    "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
    "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
    "\n",
    "    ## Mimic of the gradio loop with an initial message from the agent.\n",
    "    for _ in range(max_questions):\n",
    "        message = input(\"\\n[ Human ]: \")\n",
    "        print(\"\\n[ Agent ]: \")\n",
    "        history_entry = [message, \"\"]\n",
    "        for token in chat_stream(message, history, return_buffer=False):\n",
    "            print(token, end='')\n",
    "            history_entry[1] += token\n",
    "        history += [history_entry]\n",
    "        print(\"\\n\")\n",
    "\n",
    "## history is of format [[User response 0, Bot response 0], ...]\n",
    "chat_history = [[None, \"Hello! I'm your SkyFlow agent! How can I help you?\"]]\n",
    "\n",
    "## Simulating the queueing of a streaming gradio interface, using python input\n",
    "queue_fake_streaming_gradio(\n",
    "    chat_stream = chat_gen,\n",
    "    history = chat_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb983c1-7319-41bb-8dd8-98fa72c51f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = {'know_base' : KnowledgeBase()}\n",
    "\n",
    "# chatbot = gr.Chatbot(value=[[None, \"Hello! I'm your SkyFlow agent! How can I help you?\"]])\n",
    "# demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue().launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfc194-ce36-4983-82a9-b195eb1faaef",
   "metadata": {
    "id": "3cbfc194-ce36-4983-82a9-b195eb1faaef"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "**注意:**\n",
    "* 如果 Gradio 界面在出现异常后挂起，您可能需要单击“STOP”并尝试重启。这是已知的 Jupyter Notebook 环境问题，不会在专门的 Gradio 运行文件中遇到。\n",
    "* **此处复制了您的聊天指令，以便快速访问：**\n",
    "```\n",
    "## Your goal is to invoke the following through natural conversation\n",
    "get_flight_info({\n",
    "    \"first_name\" : \"Jane\",\n",
    "    \"last_name\" : \"Doe\",\n",
    "    \"confirmation\" : 12345,\n",
    "}) -> \"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\"\n",
    "```\n",
    "* **要确认您的系统是否正常工作，可以尝试以下对话：**\n",
    "```\n",
    "> How's it going?\n",
    "> Can you tell me a bit about skyflow?\n",
    "> Can you tell me about my flight?\n",
    "> My name is Jane Doe and my flight confirmation is 12345\n",
    "> Can you tell me when I should get to my flight?\n",
    "```\n",
    "\n",
    "* **您可以在“Solutions”目录中找到练习的答案。**这是第一个提供了答案的练习，后续的 notebook 中的练习答案也可以从该目录找到。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7PZooExdzgLh",
   "metadata": {
    "id": "7PZooExdzgLh"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 5 部分：** 总结\n",
    "\n",
    "此 notebook 的目标是介绍一些更前沿的 LangChain 知识：知识库和运行状态链！这里的练习有一定的深度，恭喜您完成了任务！\n",
    "\n",
    "### <font color=\"#76b900\">**非常好！**</font>\n",
    "\n",
    "### **接下来：**\n",
    "**[可选]** 回顾 notebook 顶部的“思考问题”。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
