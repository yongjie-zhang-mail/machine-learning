{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dace1a29-864a-4a69-bd00-c0952a73d4ea",
   "metadata": {
    "id": "dace1a29-864a-4a69-bd00-c0952a73d4ea"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 3:** LangChain</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "在之前的 notebook 中，我们介绍了一些用于 LLM 应用的服务，包括外部 LLM 平台和本地托管的前端服务。这两个组件都用到了 LangChain，但目前我们还没细致的讨论它。如果您有使用 LangChain 和 LLM 的经验就再好不过了，但没有也没关系，本 notebook 会带您了解这些，以便顺利完成本课程！\n",
    "\n",
    "本 notebook 旨在带您了解 LangChain（一个领先的大语言模型编排库）的应用，还包括之前提到的 AI Foundation Endpoints。无论您是经验丰富的开发者还是 LLM 新手，本课程都将提升您构建复杂 LLM 应用的技能。\n",
    "\n",
    "<br>\n",
    "\n",
    "### **学习目标：**\n",
    "\n",
    "* 学习如何利用链（chain）和运行时（runnable）编排有趣的 LLM 系统。\n",
    "* 熟悉使用 LLM 进行外部对话和内部推理。\n",
    "* 能够在 notebook 中启动和运行简单的 Gradio 界面。\n",
    "\n",
    "<br>\n",
    "\n",
    "### **思考问题：**\n",
    "\n",
    "* 需要哪些工具来保持信息在工作流中的传输（**下一个 notebook 的前提知识**）。\n",
    "* 当您看到 `gradio` 界面时，想想您之前是否在哪里看到过这种风格的界面。有可能是 [HuggingFace Spaces](https://huggingface.co/spaces)。\n",
    "* 在本节最后，您将可以将链以路由的方式传递并通过端口访问。如果您希望其它微服务能接收链的输出，应该做出哪些要求？\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook 版权声明：**\n",
    "\n",
    "* 本 notebook 是 [**NVIDIA 深度学习培训中心**](https://www.nvidia.cn/training/)的课程[**《构建大语言模型 RAG 智能体》**](https://www.nvidia.cn/training/instructor-led-workshops/building-rag-agents-with-llms/)中的一部分，未经 NVIDIA 授权不得分发。\n",
    "\n",
    "<br> \n",
    "\n",
    "\n",
    "### **环境设置：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i-ZOOmvbybyE",
   "metadata": {
    "id": "i-ZOOmvbybyE"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio\n",
    "\n",
    "# import os\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-...\"\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d427c-68e7-45ca-90f7-8799aa9d7eff",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **考虑模型**\n",
    "\n",
    "回到 [**NGC Catalog**](https://catalog.ngc.nvidia.com/ai-foundation-models)，我们能找到一些可以从环境直接调用的有趣的模型。放这些模型在这是因为它们都在实际的生产流程中被用到过，您可以看看哪些适合您的应用场景。\n",
    "\n",
    "**课程提供的代码包括一些已经列出的模型，但如果您发现有更好的选择或者模型不再可用，您完全可以替换为其它模型。*这适用于整个课程，请记住这一点！***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jhAKeBiDurIz",
   "metadata": {
    "id": "jhAKeBiDurIz"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 1 部分：** LangChain 是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evFkwVD6ux-B",
   "metadata": {
    "id": "evFkwVD6ux-B"
   },
   "source": [
    "LangChain 是一个流行的 LLM 编排库，可帮助组织一个有单个或多个 LLM 组件的系统。这个库当下非常受欢迎，并且会根据该领域的发展迅速做出变化，这意味着开发者会对 LangChain 的某些部分有丰富的经验，同时又对其它部分几乎不了解（一方面是因为有太多不同的功能，另一方面，该领域在不断迭代更新，有些功能是最近才实现的）。\n",
    "\n",
    "此 notebook 将使用 **LangChain Expression Language (LCEL)**，从基本的链规范了解到更高级的对话管理实践，希望您能享受这趟旅程，即使是经验丰富的 LangChain 开发者也能有所收获！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
   "metadata": {
    "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/langchain-diagram.png\" width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff906c9-d776-4117-b4f6-19ff535b8f57",
   "metadata": {
    "id": "3ff906c9-d776-4117-b4f6-19ff535b8f57"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 2 部分：** 链和运行时\n",
    "\n",
    "在探索一个新库的时候，首先要关注库的核心系统是什么，以及它是怎么使用的。\n",
    "\n",
    "在 LangChain 中，主要的构建块*曾经是*经典的**链（Chain）**：是一个执行特定操作的小型功能模块，可以跟其它链连接以构建系统。因此它可以被抽象为一个“构建块系统”，其中每个构建块都很容易创建，它们有一致的方法（`invoke`，`generate`，`stream`，等），并且可以连接成一整个系统协同工作。一些传统的链包括 `LLMChain`，`ConversationChain`，`TransformationChain`，`SequentialChain` 等等。\n",
    "\n",
    "最近，出现了一种更易于使用且极其紧凑的规范，即 **LangChain Expression Language (LCEL)**。这种新范式依赖于另一种基础组建，即**运行时（Runnable）**，它就是一个封装函数的对象。允许将字典隐式转换为运行时，并可以通过 **pipe |** 操作符来创建一个从左到右传递数据 的运行时（比如 `fn1 | fn2` 就是一个运行时），通过这样一种简单的方式就可以创建复杂的逻辑！\n",
    "\n",
    "下面是几个很有代表性的运行时，基于 `RunnableLambda` 类创建的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e676de75-cc9b-4fa2-8ce7-d3bd6f9949b7",
   "metadata": {
    "id": "e676de75-cc9b-4fa2-8ce7-d3bd6f9949b7"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from functools import partial\n",
    "\n",
    "################################################################################\n",
    "## Very simple \"take input and return it\"\n",
    "identity = RunnableLambda(lambda x: x)  ## Or RunnablePassthrough works\n",
    "\n",
    "################################################################################\n",
    "## Given an arbitrary function, you can make a runnable with it\n",
    "def print_and_return(x, preface=\"\"):\n",
    "    print(f\"{preface}{x}\")\n",
    "    return x\n",
    "\n",
    "rprint0 = RunnableLambda(print_and_return)\n",
    "\n",
    "################################################################################\n",
    "## You can also pre-fill some of values using functools.partial\n",
    "rprint1 = RunnableLambda(partial(print_and_return, preface=\"1: \"))\n",
    "\n",
    "################################################################################\n",
    "## And you can use the same idea to make your own custom Runnable generator\n",
    "def RPrint(preface=\"\"):\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "################################################################################\n",
    "## Chaining two runnables\n",
    "chain1 = identity | rprint0\n",
    "chain1.invoke(\"Hello World!\")\n",
    "print()\n",
    "\n",
    "################################################################################\n",
    "## Chaining that one in as well\n",
    "output = (\n",
    "    chain1           ## Prints \"Hello World\" & passes \"Welcome Home!\" onward\n",
    "    | rprint1        ## Prints \"1: Hello World\" & passes \"Welcome Home!\" onward\n",
    "    | RPrint(\"2: \")  ## Prints \"2: Hello World\" & passes \"Welcome Home!\" onward\n",
    ").invoke(\"Welcome Home!\")\n",
    "\n",
    "## Final Output Is Preserved As \"Welcome Home!\"\n",
    "print(\"\\nOutput:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb670bea-4593-49a2-9b58-6ff2becf1dbe",
   "metadata": {
    "id": "cb670bea-4593-49a2-9b58-6ff2becf1dbe"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 3 部分：** 使用聊天模型的字典工作流\n",
    "\n",
    "您可以借助运行时做很多事，但先来规范一些最佳实践是很重要的。出于几个重要原因，最简单的做法是将*字典*作为默认的变量容器。\n",
    "\n",
    "**传递字典有助于我们按名称跟踪变量。**\n",
    "\n",
    "由于字典允许我们传播命名变量（由键索引出的值），因此很适合用它们来锁定链组件的输出。\n",
    "\n",
    "**LangChain 提示词需要以字典的形式提供值。**\n",
    "\n",
    "在 LCEL 中让 LLM 链接收字典并生成字符串是非常自然的，反过来也同样轻松。是有意这样设计的，部分原因就是我们刚刚提到的那些。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xMHAThLp_AgQ",
   "metadata": {
    "id": "xMHAThLp_AgQ"
   },
   "source": [
    "<br>  \n",
    "\n",
    "### **示例 1：** 一个简单的 LLM 链\n",
    "\n",
    "经典 LangChain 最基本的组件之一就是接收一个**提示词**和一个 **LLM** 的 `LLMChain`：\n",
    "\n",
    "* 提示词通常是从像 `PromptTemplate.from_template(\"string with {key1} and {key2}\")` 这样的用于创建字符串的模板构造出来的。可以传入 `{\"key1\" : 1, \"key2\" : 2}` 这种字典，这样就能得到字符串 `\"string with 1 and 2\"`。\n",
    "\t+ 对于 `ChatNVIDIA` 聊天模型，需要使用 `ChatPromptTemplate.from_messages` 方法。\n",
    "* LLM 接收字符串作为输入并返回一个生成的字符串。\n",
    "\t+ `ChatNVIDIA` 是用消息处理的，但也一个道理！最后用 **StrOutputParser** 就可以从消息中提取响应内容了。\n",
    "\n",
    "下面就是上述简单聊天链的一个轻量级示例。它只做一件事，接收一个用来构造系统消息（用于指定总体目标）的字典，以及一条用户输入，返回响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uYBqZ_Za985q",
   "metadata": {
    "id": "uYBqZ_Za985q"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## Simple Chat Pipeline\n",
    "chat_llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Only respond in rhymes\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "rhyme_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "print(rhyme_chain.invoke({\"input\" : \"Tell me about birds!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a3935d",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "    Birds are quite a delightful find,\n",
    "    With feathers and wings, they soar and entwine.\n",
    "    In trees, they alight, with tails so bright,\n",
    "    And sing their songs, with morning light.\n",
    "    \n",
    "    Some have beaks that curve, some have beaks that straight,\n",
    "    Their chirps and chatter, fill the air and create\n",
    "    A symphony sweet, of melodic sound,\n",
    "    As birds take flight, their magic's all around.\n",
    "    \n",
    "    From robins to sparrows, to hawks on high,\n",
    "    Each species unique, yet all touch the sky.\n",
    "    With colors bright, and forms so grand,\n",
    "    Birds are a wonder, in this world so bland."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4torS7DgBk2T",
   "metadata": {
    "id": "4torS7DgBk2T"
   },
   "source": [
    "除了按原样用代码的方式调用之外，我们还可以尝试使用 [**Gradio 界面**](https://www.gradio.app/guides/creating-a-chatbot-fast)。Gradio 是一款热门的工具，可以方便的创建自定义生成式 AI 界面！下面就展示了如何用这个示例链创建简单的 Gradio 聊天界面："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CAQ_DjX7-2oO",
   "metadata": {
    "id": "CAQ_DjX7-2oO"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "#######################################################\n",
    "## Non-streaming Interface like that shown above\n",
    "\n",
    "# def rhyme_chat(message, history):\n",
    "#     return rhyme_chain.invoke({\"input\" : message})\n",
    "\n",
    "# gr.ChatInterface(rhyme_chat).launch()\n",
    "\n",
    "#######################################################\n",
    "## Streaming Interface\n",
    "\n",
    "def rhyme_chat_stream(message, history):\n",
    "    ## This is a generator function, where each call will yield the next entry\n",
    "    buffer = \"\"\n",
    "    for token in rhyme_chain.stream({\"input\" : message}):\n",
    "        buffer += token\n",
    "        yield buffer\n",
    "\n",
    "## Uncomment when you're ready to try this. IF USING COLAB: Share=False is faster\n",
    "gr.ChatInterface(rhyme_chat_stream).queue().launch(server_name=\"0.0.0.0\", share=True, debug=True) \n",
    "\n",
    "## IMPORTANT!! When you're done, please click the Square button (twice to be safe) to stop the session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tBGUORsV96_E",
   "metadata": {
    "id": "tBGUORsV96_E"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **示例 2：内部响应**\n",
    "\n",
    "有时，您还希望在响应实际发送给用户之前，在背后进行一些快速推理。执行此任务时，您需要一个内置的强指令遵循先验模型。\n",
    "\n",
    "下面是一个对句子进行分类的“零样本分类”（zero-shot classification）流程的示例。\n",
    "\n",
    "**零样本分类链的各步骤如下：**\n",
    "* 接收含有 `input` 和 `options` 两个必要键的字典。\n",
    "* 传给零样本提示词，得到 LLM 的输入。\n",
    "* 将该字符串传给模型来获取结果。\n",
    "\n",
    "**任务：**选几个您认为适合该任务的模型，看看效果怎么样！具体来说：\n",
    "* **试试在多个示例中表现稳定的模型。**如果格式都很容易解析且有很高的可预测性，那么这个模型应该就对了。\n",
    "* **尝试寻找最快的模型！**这很重要，因为内部推理通常在外部响应返回之前一直在背后进行着。所以这是一个阻塞过程，会拖慢“面向用户”的结果生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1868d-4ece-4ee6-b6c3-92ec7164bda5",
   "metadata": {
    "id": "39b1868d-4ece-4ee6-b6c3-92ec7164bda5"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## TODO: Try out some more models and see if there are better options\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mistral-7b-instruct-v0.2\")\n",
    "\n",
    "sys_msg = (\n",
    "    \"Choose the most likely topic classification given the sentence as context.\"\n",
    "    \" Only one word, no explanation.\\n[Options : {options}]\"\n",
    ")\n",
    "\n",
    "## One-shot classification prompt with heavy format assumptions.\n",
    "zsc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_msg),\n",
    "    (\"user\", \"[[The sea is awesome]]\"),\n",
    "    (\"assistant\", \"boat\"),\n",
    "    (\"user\", \"[[{input}]]\"),\n",
    "])\n",
    "\n",
    "## Roughly equivalent as above for <s>[INST]instruction[/INST]response</s> format\n",
    "zsc_prompt = ChatPromptTemplate.from_template(\n",
    "    f\"{sys_msg}\\n\\n\"\n",
    "    \"[[The sea is awesome]][/INST]boat</s><s>[INST]\"\n",
    "    \"[[{input}]]\"\n",
    ")\n",
    "\n",
    "zsc_chain = zsc_prompt | instruct_llm | StrOutputParser()\n",
    "\n",
    "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
    "    return zsc_chain.invoke({\"input\" : input, \"options\" : options}).split()[0]\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jXdjidkkKG9W",
   "metadata": {
    "id": "jXdjidkkKG9W"
   },
   "source": [
    "<br>  \n",
    "\n",
    "### **示例 3：多组件链**\n",
    "\n",
    "前面我们展示了如何通过将字典传递给 `prompt -> LLM` 链来转成一个字符串，这样的简单结构很适合用容器来构建。但是将字符串转回字典是否也同样简单？\n",
    "\n",
    "**是的！**最简单的方法就是用 LCEL 的 *“implicit runnable”* 语法，它允许您将以字典形式组织的多个函数（包括链）作为运行时，执行时会运行每个函数并将值映射到输出字典的键。\n",
    "\n",
    "下面的练习就用到了这个功能，同时还提供了一些实用的额外工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yi8-lKSIKhXe",
   "metadata": {
    "id": "Yi8-lKSIKhXe"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Example of dictionary enforcement methods\n",
    "def make_dictionary(v, key):\n",
    "    if isinstance(v, dict):\n",
    "        return v\n",
    "    return {key : v}\n",
    "\n",
    "def RInput(key='input'):\n",
    "    '''Coercing method to mold a value (i.e. string) to in-like dict'''\n",
    "    return RunnableLambda(partial(make_dictionary, key=key))\n",
    "\n",
    "def ROutput(key='output'):\n",
    "    '''Coercing method to mold a value (i.e. string) to out-like dict'''\n",
    "    return RunnableLambda(partial(make_dictionary, key=key))\n",
    "\n",
    "def RPrint(preface=\"\"):\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "################################################################################\n",
    "## Common LCEL utility for pulling values from dictionaries\n",
    "from operator import itemgetter\n",
    "\n",
    "up_and_down = (\n",
    "    RPrint(\"A: \")\n",
    "    ## Custom ensure-dictionary process\n",
    "    | RInput()\n",
    "    | RPrint(\"B: \")\n",
    "    ## Pull-values-from-dictionary utility\n",
    "    | itemgetter(\"input\")\n",
    "    | RPrint(\"C: \")\n",
    "    ## Anything-in Dictionary-out implicit map\n",
    "    | {\n",
    "        'word1' : (lambda x : x.split()[0]),\n",
    "        'word2' : (lambda x : x.split()[1]),\n",
    "        'words' : (lambda x: x),  ## <- == to RunnablePassthrough()\n",
    "    }\n",
    "    | RPrint(\"D: \")\n",
    "    | itemgetter(\"word1\")\n",
    "    | RPrint(\"E: \")\n",
    "    ## Anything-in anything-out lambda application\n",
    "    | RunnableLambda(lambda x: x.upper())\n",
    "    | RPrint(\"F: \")\n",
    "    ## Custom ensure-dictionary process\n",
    "    | ROutput()\n",
    ")\n",
    "\n",
    "up_and_down.invoke({\"input\" : \"Hello World\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kk0t7kUcMhFT",
   "metadata": {
    "id": "kk0t7kUcMhFT"
   },
   "outputs": [],
   "source": [
    "## NOTE how the dictionary enforcement methods make it easy to make the following syntax equivalent\n",
    "up_and_down.invoke(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LVIagnD0byq1",
   "metadata": {
    "id": "LVIagnD0byq1"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 4 部分：[练习]** Rhyme Re-themer 聊天机器人\n",
    "\n",
    "下面是一个诗歌生成示例，展示了如何以单个智能体的形式组织两个不同的任务。这个系统跟前面简单的 Gradio 例子类似，但会在背后扩展一些样板（boiler-plate）响应和逻辑。\n",
    "\n",
    "它的主要功能包括：\n",
    "* 在第一个响应中，它会根据您的输入生成一首诗。\n",
    "* 在后续的回复中，它会在保留原始诗的格式和结构的同时，修改诗的主题。\n",
    "\n",
    "**问题：**目前，系统应该可以正常完成第一个功能，但第二个功能还没实现。\n",
    "\n",
    "**目标：**实现 `rhyme_chat2_stream`，让智能体能正确完成这两个功能。\n",
    "\n",
    "为了让 gradio 组件更易于使用，我们提供了一个更简洁的 `queue_fake_streaming_gradio` 方法，用标准 Python `input` 方法模拟 gradio 聊天事件循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21077ce-8ad1-4d55-933a-1bfe54d07e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatNVIDIA.get_available_models(filter=\"mistralai/\", list_none=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B-bzsMyrKQ5m",
   "metadata": {
    "id": "B-bzsMyrKQ5m"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from copy import deepcopy\n",
    "\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")  ## Feel free to change the models\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_messages([(\"user\", (\n",
    "    \"INSTRUCTION: Only respond in rhymes\"\n",
    "    \"\\n\\nPROMPT: {input}\"\n",
    "))])\n",
    "\n",
    "prompt2 =  ChatPromptTemplate.from_messages([(\"user\", (\n",
    "    \"INSTRUCTION: Only responding in rhyme, change the topic of the input poem to be about {topic}!\"\n",
    "    \" Make it happy! Try to keep the same sentence structure, but make sure it's easy to recite!\"\n",
    "    \" Try not to rhyme a word with itself.\"\n",
    "    \"\\n\\nOriginal Poem: {input}\"\n",
    "    \"\\n\\nNew Topic: {topic}\"\n",
    "))])\n",
    "\n",
    "## These are the main chains, constructed here as modules of functionality.\n",
    "chain1 = prompt1 | instruct_llm | StrOutputParser()  ## only expects input\n",
    "chain2 = prompt2 | instruct_llm | StrOutputParser()  ## expects both input and topic\n",
    "\n",
    "################################################################################\n",
    "## SUMMARY OF TASK: chain1 currently gets invoked for the first input.\n",
    "##  Please invoke chain2 for subsequent invocations.\n",
    "\n",
    "def rhyme_chat2_stream(message, history, return_buffer=True):\n",
    "    '''This is a generator function, where each call will yield the next entry'''\n",
    "\n",
    "    first_poem = None\n",
    "    for entry in history:\n",
    "        if entry[0] and entry[1]:\n",
    "            ## If a generation occurred as a direct result of a user input,\n",
    "            ##  keep that response (the first poem generated) and break out\n",
    "            first_poem = entry[1]\n",
    "            break\n",
    "\n",
    "    if first_poem is None:\n",
    "        ## First Case: There is no initial poem generated. Better make one up!\n",
    "\n",
    "        buffer = \"Oh! I can make a wonderful poem about that! Let me think!\\n\\n\"\n",
    "        yield buffer\n",
    "\n",
    "        ## iterate over stream generator for first generation\n",
    "        inst_out = \"\"\n",
    "        chat_gen = chain1.stream({\"input\" : message})\n",
    "        for token in chat_gen:\n",
    "            inst_out += token\n",
    "            buffer += token\n",
    "            yield buffer if return_buffer else token\n",
    "\n",
    "        passage = \"\\n\\nNow let me rewrite it with a different focus! What should the new focus be?\"\n",
    "        buffer += passage\n",
    "        yield buffer if return_buffer else passage\n",
    "\n",
    "    else:\n",
    "        ## Subsequent Cases: There is a poem to start with. Generate a similar one with a new topic!\n",
    "\n",
    "        buffer = f\"Sure! Here you go!\\n\\n\"\n",
    "        yield buffer\n",
    "\n",
    "        return  ## <- TODO: Early termination for generators. Comment this out\n",
    "\n",
    "        ########################################################################\n",
    "        ## TODO: Invoke the second chain to generate the new rhymes.\n",
    "\n",
    "        ## iterate over stream generator for second generation\n",
    "\n",
    "        ## END TODO\n",
    "        ########################################################################\n",
    "\n",
    "        passage = \"\\n\\nThis is fun! Give me another topic!\"\n",
    "        buffer += passage\n",
    "        yield buffer if return_buffer else passage\n",
    "\n",
    "################################################################################\n",
    "## Below: This is a small-scale simulation of the gradio routine.\n",
    "\n",
    "def queue_fake_streaming_gradio(chat_stream, history = [], max_questions=3):\n",
    "\n",
    "    ## Mimic of the gradio initialization routine, where a set of starter messages can be printed off\n",
    "    for human_msg, agent_msg in history:\n",
    "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
    "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
    "\n",
    "    ## Mimic of the gradio loop with an initial message from the agent.\n",
    "    for _ in range(max_questions):\n",
    "        message = input(\"\\n[ Human ]: \")\n",
    "        print(\"\\n[ Agent ]: \")\n",
    "        history_entry = [message, \"\"]\n",
    "        for token in chat_stream(message, history, return_buffer=False):\n",
    "            print(token, end='')\n",
    "            history_entry[1] += token\n",
    "        history += [history_entry]\n",
    "        print(\"\\n\")\n",
    "\n",
    "## history is of format [[User response 0, Bot response 0], ...]\n",
    "history = [[None, \"Let me help you make a poem! What would you like for me to write?\"]]\n",
    "\n",
    "## Simulating the queueing of a streaming gradio interface, using python input\n",
    "queue_fake_streaming_gradio(\n",
    "    chat_stream = rhyme_chat2_stream,\n",
    "    history = history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k9-1X9EVQ42t",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "k9-1X9EVQ42t"
   },
   "outputs": [],
   "source": [
    "## Simple way to initialize history for the ChatInterface\n",
    "chatbot = gr.Chatbot(value = [[None, \"Let me help you make a poem! What would you like for me to write?\"]])\n",
    "\n",
    "## IF USING COLAB: Share=False is faster\n",
    "gr.ChatInterface(rhyme_chat2_stream, chatbot=chatbot).queue().launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VJ6mEQvgkH7w",
   "metadata": {
    "id": "VJ6mEQvgkH7w"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 5 部分：[练习]** 更深入的使用 LangChain 集成\n",
    "\n",
    "本练习让您有机会探究一些 [**LangServer**](https://www.langchain.com/langserve) 的示例代码。具体来说是 [`frontend`](frontend) 目录以及 [`09_langserve.ipynb`](09_langserve.ipynb) notebook。\n",
    "\n",
    "本练习需要使用课程环境。此服务仅在您提交最终项目时才需要！\n",
    "\n",
    "* 访问 [`09_langserve.ipynb`](09_langserve.ipynb) 并运行脚本以启动具有多个可用路由的服务。\n",
    "* 完成后，请验证以下 [**LangServer `RemoteRunnable`**](https://python.langchain.com/docs/langserve) 能正常工作。[`RemoteRunnable`](https://python.langchain.com/docs/langserve) 的目的是为了能轻松地将 LangChain 链托管为 API 入口，以下操作只是测试它是否可以正常工作。\n",
    "\t+ 如果第一次不成功，可能是操作顺序出了问题。您可以尝试重启 langserve notebook。LangServe 仍处于早期（v0.0.35），所以可能就是会遇到一些问题。\n",
    "* 假设您的本地实例正常，在浏览器复制一个当前的选项卡，将地址中“/lab”开始的内容换成 `:8090`（即 `http://<...>.courses.nvidia.com:8090`）。这里包括了部署好的可供交互的 [`frontend`](frontend) 文件夹。**或者，您也可以运行下面的单元来生成链接：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6b7c8-832c-4d4f-a8f1-422100ca0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe52531b-a874-4745-bca4-e8f4ea326cd2",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**注意：**在这种环境中部署 LangServe API 是一种不太常规的操作，仅为给学员展示一些有趣的代码。实践中可以使用优化过的更稳定的单功能容器，以下网址包含更多相关内容：\n",
    "* [**NVIDIA “支持检索增强生成的 AI 聊天机器人” 技术简介**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief)\n",
    "* [**NVIDIA/GenerativeAIExamples GitHub Repo**](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RetrievalAugmentedGeneration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1rEA-cZWwNSx",
   "metadata": {
    "id": "1rEA-cZWwNSx"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 6 部分：** 总结\n",
    "\n",
    "此 notebook 的目标是向您介绍 LangChain Expression Language 的模式，并让您接触一下用来提供 LLM 功能的 `gradio` 和 `LangServe` 接口！之后的 notebook 会继续本 notebook 中对 LLM 智能体开发新范式的探索。\n",
    "\n",
    "### <font color=\"#76b900\">**非常好！**</font>\n",
    "\n",
    "### 后续步骤：\n",
    "1. **[可选]** 花几分钟时间从 `frontend` 目录中看看部署的具体方式和底层功能。\n",
    "2. **[可选]** 回顾 notebook 顶部的“思考问题”。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
