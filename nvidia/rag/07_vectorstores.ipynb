{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qk4Uw_iSr3Mc",
   "metadata": {
    "id": "Qk4Uw_iSr3Mc"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 7:** ä½¿ç”¨å‘é‡å­˜å‚¨å®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆ</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "æˆ‘ä»¬åœ¨å‰é¢çš„ notebook ä¸­äº†è§£å¹¶å°è¯•äº†åµŒå…¥æ¨¡å‹ã€‚è®¨è®ºäº†å®ƒåœ¨é•¿æ–‡æ¡£æ¯”è¾ƒä¸­çš„åº”ç”¨ï¼Œå¹¶ä»¥å®ƒä¸ºä¸»å¹²å®ç°äº†åŸºäºè¯­ä¹‰çš„æ¯”è¾ƒã€‚æœ¬ notebook å°†æŠŠè¿™ä¸ªæ€è·¯ç”¨åˆ°æ£€ç´¢æ¨¡å‹ä¸Šï¼Œæ¢ç´¢å¦‚ä½•é *å‘é‡å­˜å‚¨*æ¥æ„å»ºè‡ªåŠ¨ä¿å­˜å’Œæ£€ç´¢ä¿¡æ¯çš„èŠå¤©æœºå™¨äººç³»ç»Ÿã€‚\n",
    "\n",
    "<br>\n",
    "\n",
    "### **å­¦ä¹ ç›®æ ‡ï¼š**\n",
    "\n",
    "* ç†è§£è¯­ä¹‰ç›¸ä¼¼åº¦ç³»ç»Ÿæ˜¯æ€ä¹ˆæ–¹ä¾¿åœ°å®ç°æ£€ç´¢çš„ã€‚\n",
    "* å­¦ä¼šå°†æ£€ç´¢æ¨¡å—æ•´åˆåˆ°èŠå¤©æ¨¡å‹ç³»ç»Ÿä¸­ï¼Œä»¥åˆ›å»ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·¥ä½œæµï¼Œç”¨äºå®Œæˆæ–‡æ¡£æ£€ç´¢æˆ–å¯¹è¯å†…å­˜ç¼“å†²ç­‰ä»»åŠ¡ã€‚\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **æ€è€ƒé—®é¢˜ï¼š**\n",
    "\n",
    "* æœ¬ notebook ä¸ä¼šå°è¯•åŠ å…¥å±‚æ¬¡åŒ–æ¨ç†ï¼ˆhierachical reasoningï¼‰æˆ–éæœ´ç´ ï¼ˆnon-naiveï¼‰çš„ RAGï¼Œå¦‚è§„åˆ’æ™ºèƒ½ä½“ï¼ˆpalnning agentsï¼‰ã€‚æƒ³æƒ³éœ€è¦å¦‚ä½•è°ƒæ•´æ‰èƒ½è®©è¿™äº›ç»„ä»¶åœ¨ LCEL é“¾ä¸­è¿è¡Œã€‚\n",
    "* æ€è€ƒå°†å‘é‡å­˜å‚¨æ–¹æ¡ˆç”¨åœ¨è§„æ¨¡åŒ–éƒ¨ç½²çš„æœ€å¥½æ—¶æœºæ˜¯ä»€ä¹ˆï¼Œä»¥åŠä»€ä¹ˆæ—¶å€™éœ€è¦ç”¨ GPU è¿›è¡Œä¼˜åŒ–ã€‚\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **Notebook ç‰ˆæƒå£°æ˜ï¼š**\n",
    "\n",
    "* æœ¬ notebook æ˜¯ [**NVIDIA æ·±åº¦å­¦ä¹ åŸ¹è®­ä¸­å¿ƒ**](https://www.nvidia.cn/training/)çš„è¯¾ç¨‹[**ã€Šæ„å»ºå¤§è¯­è¨€æ¨¡å‹ RAG æ™ºèƒ½ä½“ã€‹**](https://www.nvidia.cn/training/instructor-led-workshops/building-rag-agents-with-llms/)ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œæœªç» NVIDIA æˆæƒä¸å¾—åˆ†å‘ã€‚\n",
    "\n",
    "<br> \n",
    "\n",
    "### **ç¯å¢ƒè®¾ç½®ï¼š**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5XmeiiOWtuxC",
   "metadata": {
    "id": "5XmeiiOWtuxC"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "## ^^ Comment out if you want to see the pip install process\n",
    "\n",
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37fe234-2bdb-4107-8483-efda9aa5e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
   "metadata": {
    "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## ç¬¬ 1 éƒ¨åˆ†ï¼šRAG å·¥ä½œæµæ¦‚è¿°\n",
    "\n",
    "æ­¤ notebook å°†æ¢ç´¢å¤šä¸ªèŒƒå¼å¹¶ç»™å‡ºå‚è€ƒä»£ç ï¼Œä»¥å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨æœ€å¸¸è§ä¸€äº›çš„æ£€ç´¢å¢å¼ºå·¥ä½œæµã€‚å…·ä½“æ¥è¯´å°†æ¶µç›–ä»¥ä¸‹éƒ¨åˆ†ï¼ˆæ¯ä¸ªéƒ¨åˆ†å„æœ‰ä¾§é‡ï¼‰ï¼š\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***é€‚ç”¨äºäº¤äº’å¼å¯¹è¯çš„å‘é‡å­˜å‚¨å·¥ä½œæµï¼š***\n",
    "* ä¸ºæ–°å¯¹è¯ç”Ÿæˆè¯­ä¹‰åµŒå…¥ã€‚\n",
    "* å°†æ¶ˆæ¯æ­£æ–‡æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä»¥ä¾›æ£€ç´¢ã€‚\n",
    "* åœ¨å‘é‡å­˜å‚¨ä¸­æŸ¥è¯¢ç›¸å…³æ¶ˆæ¯å¡«å……åˆ° LLM ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***å¤„ç†ä»»æ„æ–‡æ¡£çš„å·¥ä½œæµï¼š***\n",
    "* **å°†æ–‡æ¡£åˆ†å¿«å¹¶å¤„ç†æˆæœ‰ç”¨ä¿¡æ¯ã€‚**\n",
    "* ä¸ºæ¯ä¸ª**æ–°æ–‡æ¡£å—**ç”Ÿæˆè¯­ä¹‰åµŒå…¥ã€‚\n",
    "* å°†**å—æ­£æ–‡ï¼ˆchunk bodiesï¼‰**å­˜åˆ°å‘é‡å­˜å‚¨ä¸­ä»¥ä¾›æ£€ç´¢ã€‚\n",
    "* åœ¨å‘é‡å­˜å‚¨ä¸­æŸ¥è¯¢ç›¸å…³çš„**å—**ï¼Œç”¨æ¥å¡«å…… LLM ä¸Šä¸‹æ–‡ã€‚\n",
    "\t+ ***å¯é€‰ï¼š*ä¿®æ”¹/åˆæˆç»“æœä»¥è·å¾—æ›´å¥½çš„ LLM ç»“æœã€‚**\n",
    "\n",
    "<br>\n",
    "\n",
    "> **é€‚ç”¨äºä»»æ„æ–‡æ¡£ç›®å½•çš„æ‰©å±•å·¥ä½œæµï¼š**\n",
    "* å°†**æ¯ä¸ªæ–‡æ¡£**åˆ†ä¸ºå¤šä¸ªå—å¹¶å¤„ç†æˆæœ‰ç”¨çš„ä¿¡æ¯ã€‚\n",
    "* ä¸ºæ¯ä¸ªæ–°æ–‡æ¡£å—ç”Ÿæˆè¯­ä¹‰åµŒå…¥ã€‚\n",
    "* å°†å—æ­£æ–‡å­˜åˆ°**å¯æ‰©å±•çš„å‘é‡æ•°æ®åº“ä¸­ä»¥å®ç°å¿«é€Ÿæ£€ç´¢**ã€‚\n",
    "\t+ ***å¯é€‰ï¼š*åˆ©ç”¨æ›´å¤§ç³»ç»Ÿçš„å±‚æ¬¡åŒ–ç»“æ„æˆ–å…ƒæ•°æ®ç»“æ„ã€‚**\n",
    "* åœ¨**å‘é‡æ•°æ®åº“**ä¸­æŸ¥è¯¢ç›¸å…³çš„å—æ¥å¡«å…… LLM ä¸Šä¸‹æ–‡ã€‚\n",
    "\t+ *å¯é€‰ï¼š*ä¿®æ”¹/åˆæˆç»“æœä»¥è·å¾—æ›´å¥½çš„ LLM ç»“æœã€‚\n",
    "\n",
    "<br>  \n",
    "\n",
    "ä¸ RAG ç›¸å…³çš„ä¸€äº›é‡è¦æœ¯è¯­éƒ½å¯ä»¥åœ¨ [**LlamaIndex Concepts é¡µé¢**](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html) æŸ¥åˆ°ï¼Œè¿™æ˜¯å­¦ä¹  LlamaIndex åŠ è½½å’Œæ£€ç´¢ç­–ç•¥çš„å¾ˆå¥½çš„èµ„æºã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨åœ¨å­¦ä¹ æ­¤ notebook çš„è¿‡ç¨‹ä¸­å‚è€ƒå®ƒï¼Œå¹¶é¼“åŠ±æ‚¨åœ¨è¯¾åè¯•è¯• LlamaIndex äº²æ‰‹ä½“ä¼šå®ƒçš„ä¼˜ç¼ºç‚¹ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437",
   "metadata": {
    "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/data_connection_langchain.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Retrieval | LangChain**ğŸ¦œï¸ğŸ”—](https://python.langchain.com/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XaZ20XoeSTD-",
   "metadata": {
    "id": "XaZ20XoeSTD-"
   },
   "source": [
    "----\n",
    "\n",
    "<br>  \n",
    "\n",
    "## **ç¬¬ 2 éƒ¨åˆ†ï¼š** ç”¨äºå¯¹è¯å†å²çš„ RAG\n",
    "\n",
    "åœ¨ä¹‹å‰çš„æ¢ç´¢ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†æ–‡æ¡£åµŒå…¥æ¨¡å‹çš„åŠŸèƒ½ï¼Œå¹¶ç”¨å®ƒæ¥åµŒå…¥ã€å­˜å‚¨å’Œæ¯”è¾ƒæ–‡æœ¬çš„è¯­ä¹‰å‘é‡è¡¨ç¤ºã€‚å°½ç®¡æˆ‘ä»¬å¯ä»¥åŠ¨æ‰‹å°†å…¶æ‰©å±•åˆ°å‘é‡å­˜å‚¨é¢†åŸŸï¼Œä½†å¦‚æœç”¨æ ‡å‡† API é…åˆæ¡†æ¶çš„è¯ï¼Œå°±èƒ½å‘ç°å®ƒå·²ç»æ›¿æˆ‘ä»¬å®Œæˆäº†å¾ˆå¤šç¹é‡çš„å·¥ä½œï¼\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LRx0XUf_Sdxw",
   "metadata": {
    "id": "LRx0XUf_Sdxw"
   },
   "source": [
    "### **ç¬¬ 1 æ­¥ï¼š** åˆ›å»ºä¸€æ®µå¯¹è¯\n",
    "\n",
    "æƒ³è±¡ä¸€æ®µ Llama-13B èŠå¤©æ™ºèƒ½ä½“å’Œä¸€åªåä¸º Beras çš„ç†Šä¹‹é—´çš„å¯¹è¯ã€‚è¿™æ®µå¯¹è¯åŒ…å«äº†å¤§é‡ç»†èŠ‚å’Œæ½œåœ¨çš„åˆ†æ”¯ï¼Œä¸ºæˆ‘ä»¬çš„ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„æ•°æ®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IUfCuMkoShWI",
   "metadata": {
    "id": "IUfCuMkoShWI"
   },
   "outputs": [],
   "source": [
    "conversation = [  ## This conversation was generated partially by an AI system, and modified to exhibit desirable properties\n",
    "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
    "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
    "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
    "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
    "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
    "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
    "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
    "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tDL2tAo2Skh2",
   "metadata": {
    "id": "tDL2tAo2Skh2"
   },
   "source": [
    "ä»ç„¶å¯ä»¥ç”¨ä¸Šä¸€ä¸ª notebook çš„æ‰‹åŠ¨åµŒå…¥ç­–ç•¥ï¼Œä½†æˆ‘ä»¬å®Œå…¨å¯ä»¥è®©å‘é‡æ•°æ®åº“æ›¿æˆ‘ä»¬åšï¼\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5hIp943mSqGZ",
   "metadata": {
    "id": "5hIp943mSqGZ"
   },
   "source": [
    "### **ç¬¬ 2 æ­¥ï¼š** æ„å»ºå‘é‡å­˜å‚¨æ£€ç´¢å™¨\n",
    "\n",
    "ä¸ºäº†æµç¨‹åŒ–å¯¹è¯ä¸­çš„ç›¸ä¼¼æ€§æŸ¥è¯¢ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‘é‡å­˜å‚¨æ¥å¸®åŠ©æˆ‘ä»¬è¿½è¸ªæ–‡æœ¬ï¼**å‘é‡å­˜å‚¨**ï¼ˆVector Storesï¼‰æˆ–è€…å«å‘é‡å­˜å‚¨ç³»ç»Ÿï¼Œå¯¹åµŒå…¥/æ¯”è¾ƒç­–ç•¥çš„å¤§éƒ¨åˆ†åº•å±‚ç»†èŠ‚åšäº†æŠ½è±¡ï¼Œä¸ºåŠ è½½å’Œæ¯”è¾ƒå‘é‡æä¾›äº†ä¸€ä¸ªç®€æ´çš„æ¥å£ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pnaOBgexS-kp",
   "metadata": {
    "id": "pnaOBgexS-kp"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/vector_stores.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Vector Stores | LangChain**ğŸ¦œï¸ğŸ”—](https://python.langchain.com/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DwZUh6kgS5Ki",
   "metadata": {
    "id": "DwZUh6kgS5Ki"
   },
   "source": [
    "<br>\n",
    "\n",
    "é™¤äº†å€ŸåŠ© API ç®€åŒ–æµç¨‹å¤–ï¼Œå‘é‡å­˜å‚¨è¿˜åœ¨èƒŒåå®ç°äº†è¿æ¥å™¨ï¼ˆconnectorï¼‰ã€é›†æˆï¼ˆintegrationï¼‰å’Œä¼˜åŒ–ã€‚æˆ‘ä»¬å°†ä» [**FAISS å‘é‡å­˜å‚¨**](https://python.langchain.com/docs/integrations/vectorstores/faiss)å¼€å§‹ï¼Œå®ƒé›†æˆäº†å…¼å®¹ LangChain çš„åµŒå…¥æ¨¡å‹ [**FAISS (Facebook AI Similarity Search)**](https://github.com/facebookresearch/faiss)ï¼Œä»è€Œå…è®¸åœ¨æœ¬åœ°å®ç°å¿«é€Ÿå¯æ‰©å±•çš„æµç¨‹ï¼\n",
    "\n",
    "\n",
    "**å…·ä½“æ¥è¯´ï¼š**\n",
    "\n",
    "1. æˆ‘ä»¬å¯ä»¥é€šè¿‡ `from_texts` æ„é€ å™¨å°†å¯¹è¯è¾“å…¥åˆ° [**FAISS å‘é‡å­˜å‚¨**](https://python.langchain.com/docs/integrations/vectorstores/faiss)ã€‚è¿™æ ·æˆ‘ä»¬çš„å¯¹è¯æ•°æ®å’ŒåµŒå…¥æ¨¡å‹å°±ä¼šç”¨æ¥åˆ›å»ºç´¢å¼•ã€‚\n",
    "2. ç„¶åï¼Œè¿™ä¸ªå‘é‡å­˜å‚¨å°±å¯ä»¥ä½œä¸ºæ£€ç´¢å™¨ï¼Œæ”¯æŒç”¨ LangChain è¿è¡Œæ—¶ API æ¥æ£€ç´¢æ–‡æ¡£ã€‚\n",
    "\n",
    "ä»¥ä¸‹å†…å®¹å±•ç¤ºäº†å¦‚ä½•æ„å»º FAISS å‘é‡å­˜å‚¨å¹¶ä½¿ç”¨ LangChain `vectorstore` API å°†å…¶ä½œä¸ºæ£€ç´¢å™¨ä½¿ç”¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1kE2-ejoTKKU",
   "metadata": {
    "id": "1kE2-ejoTKKU"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Streamlined from_texts FAISS vectorstore construction from text list\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "retriever = convstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muN66v5PW5dW",
   "metadata": {
    "id": "muN66v5PW5dW"
   },
   "source": [
    "ç°åœ¨ï¼Œæ£€ç´¢å™¨å¯ä»¥åƒä»»ä½•å…¶ä»–å¯è¿è¡Œçš„ LangChain ä¸€æ ·ç”¨äºæŸ¥è¯¢å‘é‡å­˜å‚¨ä¸­çš„æŸäº›ç›¸å…³æ–‡æ¡£ï¼š\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kNZJTnlEWVYh",
   "metadata": {
    "id": "kNZJTnlEWVYh"
   },
   "outputs": [],
   "source": [
    "pprint(retriever.invoke(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SE1eDZTEWScC",
   "metadata": {
    "id": "SE1eDZTEWScC"
   },
   "outputs": [],
   "source": [
    "pprint(retriever.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtNCEXLYTVf4",
   "metadata": {
    "id": "mtNCEXLYTVf4"
   },
   "source": [
    "å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ£€ç´¢å·¥å…·ä»æˆ‘ä»¬çš„æŸ¥è¯¢ä¸­æ‰¾åˆ°äº†ä¸€äº›è¯­ä¹‰ç›¸å…³çš„æ–‡æ¡£ã€‚æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ï¼Œä¸æ˜¯æ‰€æœ‰æ–‡æ¡£éƒ½æœ‰ç”¨æˆ–æ¸…æ™°ã€‚æ¯”å¦‚ï¼Œå¦‚æœä¸æ˜¯å‡ºäºä¸Šä¸‹æ–‡ï¼Œæ£€ç´¢è¯¢é—®*â€œæ‚¨çš„å§“åâ€*æ—¶æŠŠ*â€œBerasâ€*æ£€ç´¢å‡ºæ¥å¯èƒ½ä¸æ˜¯ä¸ªå¥½äº‹ã€‚æå‰è€ƒè™‘åˆ°æ½œåœ¨çš„é—®é¢˜å¹¶è®© LLM ç»„ä»¶ç›¸äº’ååŒæ›´æœ‰å¯èƒ½è®© RAG è¾¾åˆ°å¥½çš„æ•ˆæœã€‚\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZEDEzpqmTYMv",
   "metadata": {
    "id": "ZEDEzpqmTYMv"
   },
   "source": [
    "### **ç¬¬ 3 æ­¥ï¼š** å°†å¯¹è¯æ£€ç´¢åŠŸèƒ½æ•´åˆåˆ°æˆ‘ä»¬çš„é“¾ä¸­\n",
    "\n",
    "ç°åœ¨ï¼Œæˆ‘ä»¬å·²æŠŠæ£€ç´¢å™¨ç»„ä»¶ä½œä¸ºä¸€ä¸ªé“¾äº†ï¼Œå¯ä»¥åƒä»¥å‰ä¸€æ ·å°†å…¶æ•´åˆåˆ°ç°æœ‰çš„èŠå¤©ç³»ç»Ÿä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥æ„å»ºä¸€ä¸ª***ä¿æŒåœ¨çº¿ï¼ˆalways-onï¼‰çš„ RAG*** äº†ï¼Œå…¶ä¸­ï¼š\n",
    "* **é»˜è®¤æƒ…å†µä¸‹ï¼Œæ£€ç´¢å™¨å§‹ç»ˆåœ¨æ£€ç´¢ä¸Šä¸‹æ–‡ã€‚**\n",
    "* **ç”Ÿæˆå™¨æ ¹æ®æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ‰§è¡Œæ“ä½œã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64abe478-9bcb-4802-a26e-dc5a1756e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface: print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uue5UY3_TcvF",
   "metadata": {
    "id": "uue5UY3_TcvF"
   },
   "outputs": [],
   "source": [
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {question}\"\n",
    "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | context_prompt\n",
    "    # | RPrint()\n",
    "    | instruct_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(chain.invoke(\"Where does Beras live?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FSIqTMuuTjIh",
   "metadata": {
    "id": "FSIqTMuuTjIh"
   },
   "source": [
    "å¤šè¯•å‡ ä¸ªè°ƒç”¨ï¼Œçœ‹çœ‹æ–°é…ç½®çš„æ•ˆæœã€‚æ— è®ºæ‚¨é€‰æ‹©çš„æ˜¯å“ªä¸ªæ¨¡å‹ï¼Œéƒ½å¯ä»¥å…ˆä»ä¸‹é¢çš„å‡ ä¸ªé—®é¢˜å¼€å§‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4jDJwrYpTmpd",
   "metadata": {
    "id": "4jDJwrYpTmpd"
   },
   "outputs": [],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-artagLfTpBy",
   "metadata": {
    "id": "-artagLfTpBy"
   },
   "outputs": [],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains? Are they close to California?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GDgjdfdpTrV5",
   "metadata": {
    "id": "GDgjdfdpTrV5"
   },
   "outputs": [],
   "source": [
    "pprint(chain.invoke(\"How far away is Beras from the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wp9-8CbT0L9",
   "metadata": {
    "id": "8wp9-8CbT0L9"
   },
   "source": [
    "<br>  \n",
    "\n",
    "æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°æŠŠè¿™ä¸ªä¿æŒåœ¨çº¿ï¼ˆalways-onï¼‰çš„æ£€ç´¢èŠ‚ç‚¹æ”¾åˆ°å¾ªç¯é‡Œæ•ˆæœå¾ˆä¸é”™ï¼Œå› ä¸ºç›®å‰è¾“å…¥ LLM çš„ä¸Šä¸‹æ–‡ä»ç„¶ç›¸å¯¹è¾ƒå°ã€‚æœ‰å¿…è¦åå¤å°è¯•åµŒå…¥å¤§å°ã€ä¸Šä¸‹æ–‡é™åˆ¶ç­‰é…ç½®ï¼Œæ¥æ›´å¥½åœ°é¢„æµ‹æ¨¡å‹è¡¨ç°ï¼Œå¹¶è¡¡é‡ä¸ºæé«˜æ€§èƒ½å€¼å¾—åšå‡ºä½•ç§åŠªåŠ›ã€‚\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OnpOybOhUCTf",
   "metadata": {
    "id": "OnpOybOhUCTf"
   },
   "source": [
    "### **ç¬¬ 4 æ­¥ï¼š** è‡ªåŠ¨å¯¹è¯å­˜å‚¨\n",
    "\n",
    "ç°åœ¨å‘é‡å­˜å‚¨å·²ç»å¯ä»¥å·¥ä½œäº†ï¼Œæˆ‘ä»¬æœ€åå†åšä¸€ä¸ªé›†æˆï¼šåŠ ä¸€ä¸ªè°ƒç”¨ `add_texts` æ›´æ–°å­˜å‚¨çŠ¶æ€çš„è¿è¡Œæ—¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FsK6-AtRVdcZ",
   "metadata": {
    "id": "FsK6-AtRVdcZ"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Reset knowledge base and define what it means to add more messages.\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
    "    return d.get('output')\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    "    \"\\nAnswer the user conversationally. Make sure the conversation flows naturally.\\n\"\n",
    "    \"[Agent]\"\n",
    ")\n",
    "\n",
    "\n",
    "conv_chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'input': (lambda x:x)\n",
    "    }\n",
    "    | RunnableAssign({'output' : chat_prompt | instruct_llm | StrOutputParser()})\n",
    "    | partial(save_memory_and_get_output, vstore=convstore)\n",
    ")\n",
    "\n",
    "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Actually, my favorite is honey! Not sure where you got that idea?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KRMW6G7NVSWF",
   "metadata": {
    "id": "KRMW6G7NVSWF"
   },
   "source": [
    "ä¸åŒäºå°†ä¸Šä¸‹æ–‡æ³¨å…¥ LLM çš„æ›´è‡ªåŠ¨åŒ–çš„å…¨æ–‡æœ¬ï¼ˆfull-textï¼‰æˆ–åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œè¿™æ ·å¯é¿å…ä¸Šä¸‹æ–‡é•¿åº¦å¤±æ§ã€‚è¿™ç§ç­–ç•¥è™½ç„¶ç§°ä¸ä¸Šå®Œå…¨å¯é ï¼Œä½†å¯¹äºéç»“æ„åŒ–çš„å¯¹è¯æ¥è¯´å·²ç»æ˜¯ä¸€ä¸ªå·¨å¤§çš„æ”¹è¿›äº†ï¼ˆç”šè‡³ä¸éœ€è¦å€ŸåŠ©ä¸€ä¸ªå¼ºå¤§çš„æŒ‡ä»¤å¾®è°ƒæ¨¡å‹åšæ§½ä½å¡«å……ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9TPkh3SaLbqh",
   "metadata": {
    "id": "9TPkh3SaLbqh"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **ç¬¬ 3 éƒ¨åˆ† [ç»ƒä¹ ]ï¼š** ç”¨ RAG è¿›è¡Œæ–‡æ¡£å—æ£€ç´¢\n",
    "\n",
    "é‰´äºæˆ‘ä»¬ä¹‹å‰å¯¹æ–‡æ¡£åŠ è½½çš„æ¢ç´¢ï¼Œæ‚¨åº”è¯¥å·²ç»ç†Ÿæ‚‰å¯¹æ•°æ®å—åµŒå…¥å’Œæ£€ç´¢äº†ã€‚ç°åœ¨å€¼å¾—èŠ±ç‚¹æ—¶é—´ç»§ç»­è¿‡ä¸€éï¼Œå› ä¸ºæŠŠ RAG ç”¨åœ¨æ–‡æ¡£ä¸Šæ˜¯ä¸€æŠŠåŒåˆƒå‰‘ï¼šå®ƒçœ‹èµ·æ¥ä¼¼ä¹å¼€ç®±å³ç”¨ï¼Œä½†æƒ³è®©å®ƒåœ¨å®é™…åº”ç”¨ä¸­ä¿æŒå¯é çš„æ€§èƒ½éœ€è¦éå¸¸è°¨æ…åœ°ä¼˜åŒ–ã€‚æˆ‘ä»¬ä¹Ÿå€Ÿæ­¤æœºä¼šå›é¡¾ä¸€ä¸‹åŸºæœ¬çš„ LCEL æŠ€èƒ½ï¼\n",
    "\n",
    "<br> \n",
    "\n",
    "### **ç»ƒä¹ ï¼š**\n",
    "\n",
    "æ‚¨å¯èƒ½è¿˜è®°å¾—ä¹‹å‰æˆ‘ä»¬ç”¨ [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv) åŠ è½½äº†ä¸€äº›æ¯”è¾ƒçŸ­çš„æ–‡ç« ï¼š\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "docs = [\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct\n",
    "]\n",
    "```\n",
    "\n",
    "æ ¹æ®æ‰€å­¦ï¼Œé€‰æ‹©å‡ ä¸ªè®ºæ–‡ï¼Œå¹¶å¼€å‘ä¸€ä¸ªèƒ½è®¨è®ºè¿™äº›è®ºæ–‡çš„èŠå¤©æœºå™¨äººï¼\n",
    "\n",
    "<br>  \n",
    "\n",
    "è™½ç„¶è¿™æ˜¯ä¸€é¡¹ç›¸å½“è‰°å·¨çš„ä»»åŠ¡ï¼Œä½†ä¸‹é¢å°†æä¾›**å¤§éƒ¨åˆ†**å®ç°è¿‡ç¨‹ã€‚æ¼”ç¤ºè¿‡åï¼Œè®¸å¤šå¿…é¡»çš„ç¯èŠ‚å°±å·²ç»å®ç°å¥½äº†ï¼Œæ‚¨çœŸæ­£çš„ä»»åŠ¡æ˜¯å°†å®ƒä»¬é›†æˆåˆ°æœ€ç»ˆçš„ `retrieval_chain`ã€‚æ‚¨ä¼šåœ¨æœ€åä¸€ä¸ª notebook æŠŠå®ƒä»¬é›†æˆåˆ°é“¾ä¸­æ¥å®Œæˆè¯„ä¼°æµ‹è¯•ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSjfCtiQnj9e",
   "metadata": {
    "id": "jSjfCtiQnj9e"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **ä»»åŠ¡ 1ï¼š** è½½å…¥å¹¶åˆ†å—æ‚¨çš„æ–‡æ¡£\n",
    "\n",
    "ä»¥ä¸‹ä»£ç æä¾›äº†ä¸€äº›å¯ä»¥è½½å…¥åˆ° RAG é“¾çš„é»˜è®¤è®ºæ–‡ã€‚æ‚¨å¯ä»¥æ ¹æ®éœ€è¦é€‰æ›´å¤šçš„è®ºæ–‡ï¼Œä½†è¦æ³¨æ„é•¿æ–‡æ¡£çš„å¤„ç†æ—¶é—´ä¹Ÿæ›´é•¿ã€‚å…¶ä¸­è¿˜æœ‰ä¸€äº›åˆ©äºæé«˜ RAG æ€§èƒ½çš„ç®€åŒ–å‡è®¾åŠå¤„ç†æ­¥éª¤ï¼š\n",
    "\n",
    "* æ–‡æ¡£ä»…æˆªå–â€œå‚è€ƒâ€œâ€ï¼ˆReferencesï¼‰éƒ¨åˆ†ä¹‹å‰çš„å†…å®¹ã€‚é˜²æ­¢ç³»ç»Ÿè€ƒè™‘å†—é•¿å’Œä¸é‡è¦çš„å¼•ç”¨å’Œé™„å½•ã€‚\n",
    "* æœ‰ä¸€ä¸ªèƒ½æä¾›å…¨å±€è§†è§’çš„åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ–‡æ¡£çš„æ•°æ®å—ã€‚å¦‚æœæ‚¨çš„å·¥ä½œæµå¹¶ä¸æ˜¯æ¯æ¬¡æ£€ç´¢éƒ½æä¾›å…ƒæ•°æ®ï¼Œé‚£ä¹ˆè¿™ä¸ªæ•°æ®å—å°±ä¼šå¾ˆæœ‰ç”¨ï¼Œç”šè‡³å¯ä»¥åœ¨åˆé€‚çš„æ—¶å€™ä½œä¸ºæ›´é«˜ä¼˜å…ˆçº§ä¿¡æ¯çš„ä¸€éƒ¨åˆ†ã€‚\n",
    "* æ­¤å¤–ï¼Œè¿˜ä¼šæ’å…¥å…ƒæ•°æ®æ¡ç›®ä»¥æä¾›å¸¸è§„ä¿¡æ¯ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œä¼šæœ‰ä¸€äº›èåˆè¿›äº†å…ƒæ•°æ®çš„è·¨æ–‡æ¡£æ•°æ®å—ã€‚\n",
    "\n",
    "**æ³¨æ„ï¼š** ***ä¸ºæ‰§è¡Œè¯„ä¼°ï¼Œè¯·è‡³å°‘æ”¾è¿›ä¸€ç¯‡å‘è¡¨æ—¶é—´ä¸è¶…è¿‡ä¸€ä¸ªæœˆçš„è®ºæ–‡ï¼***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S-3FBdT_lhVT",
   "metadata": {
    "id": "S-3FBdT_lhVT"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    ")\n",
    "\n",
    "## TODO: Please pick some papers and add them to the list as you'd like\n",
    "## NOTE: To re-use for the final assessment, make sure at least one paper is < 1 month old\n",
    "print(\"Loading Documents\")\n",
    "docs = [\n",
    "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
    "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
    "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
    "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
    "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
    "    ## Some longer papers\n",
    "    # ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
    "    # ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
    "    # ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
    "    ## TODO: Feel free to add more\n",
    "]\n",
    "\n",
    "## Cut the paper short if references is included.\n",
    "## This is a standard string in papers.\n",
    "for doc in docs:\n",
    "    content = json.dumps(doc[0].page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]\n",
    "\n",
    "## Split the documents and also filter out stubs (overly short chunks)\n",
    "print(\"Chunking Documents\")\n",
    "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
    "\n",
    "## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"Available Documents:\"\n",
    "doc_metadata = []\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata.get('Title')\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata\n",
    "\n",
    "## Printing out some summary information for reference\n",
    "pprint(doc_string, '\\n')\n",
    "for i, chunks in enumerate(docs_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print(f\" - Metadata: \")\n",
    "    pprint(chunks[0].metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4pWU_OOnnrsT",
   "metadata": {
    "id": "4pWU_OOnnrsT"
   },
   "source": [
    "### **ä»»åŠ¡ 2ï¼š** æ„å»ºæ–‡æ¡£å‘é‡å­˜å‚¨\n",
    "\n",
    "æˆ‘ä»¬ç°åœ¨å·²ç»æœ‰äº†æ‰€æœ‰ç»„ä»¶ï¼Œå¯ä»¥ç»§ç»­å›´ç»•å®ƒä»¬åˆ›å»ºç´¢å¼•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lwwmr3aptwCg",
   "metadata": {
    "id": "lwwmr3aptwCg"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Constructing Vector Stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j39JwCKubto0",
   "metadata": {
    "id": "j39JwCKubto0"
   },
   "source": [
    "<br>\n",
    "\n",
    "æ¥ç€åƒä¸‹é¢è¿™æ ·æŠŠç´¢å¼•åˆå¹¶ä¸ºä¸€ä¸ªï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q7us66iPVc70",
   "metadata": {
    "id": "Q7us66iPVc70"
   },
   "outputs": [],
   "source": [
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VU_VEx2mqJUK",
   "metadata": {
    "id": "VU_VEx2mqJUK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **ä»»åŠ¡ 3ï¼š[ç»ƒä¹ ]** å®ç° RAG é“¾\n",
    "\n",
    "ç»ˆäºï¼Œä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼Œæ¥å®ç° RAG å·¥ä½œæµå§ï¼å›é¡¾ä¸€ä¸‹ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ï¼š\n",
    "\n",
    "* ä¸€ç§ç”¨å‘é‡å­˜å‚¨ä»é›¶åˆ›å»ºå¯¹è¯è®°å¿†çš„æ–¹æ³•ï¼ˆç”¨ `default_FAISS()` åˆå§‹åŒ–ï¼‰\n",
    "* é€šè¿‡ `ArxivLoader` é¢„åŠ è½½äº†åŒ…æ‹¬æ–‡æ¡£ä¿¡æ¯çš„å‘é‡å­˜å‚¨ï¼ˆå­˜åœ¨ `docstore` é‡Œï¼‰ã€‚\n",
    "\n",
    "å†å€ŸåŠ©å‡ ä¸ªå·¥å…·ï¼Œå°±èƒ½é›†æˆæ‚¨çš„é“¾äº†ï¼æˆ‘ä»¬è¿˜æä¾›äº†å‡ ä¸ªé¢å¤–çš„ä¾¿æ·å·¥å…·ï¼ˆ`doc2str` åŠ `RPrint`ï¼‰ï¼Œæ‚¨å¯ä»¥é…Œæƒ…ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œä¸€äº›å¯åŠ¨æç¤ºè¯å’Œç»“æ„å·²ç»å®šä¹‰å¥½äº†ã€‚\n",
    "\n",
    "> **åŸºäºä¸Šè¿°è¿™äº›ï¼š** å®ç° `retrieval_chain` å§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-RXSrb1GcNff",
   "metadata": {
    "id": "-RXSrb1GcNff"
   },
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
    "# instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()\n",
    "\n",
    "################################################################################################\n",
    "## BEGIN TODO: Implement the retrieval chain to make your system work!\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    ## TODO: Make sure to retrieve history & context from convstore & docstore, respectively.\n",
    "    ## HINT: Our solution uses RunnableAssign, itemgetter, long_reorder, and docs2str\n",
    "    | RunnableAssign({'history' : lambda d: None})\n",
    "    | RunnableAssign({'context' : lambda d: None})\n",
    ")\n",
    "\n",
    "## END TODO\n",
    "################################################################################################\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n",
    "\n",
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9W7sC5Z6BfqM",
   "metadata": {
    "id": "9W7sC5Z6BfqM"
   },
   "source": [
    "### **ä»»åŠ¡ 4ï¼š** ä¸ Gradio èŠå¤©æœºå™¨äººäº¤äº’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fMP3l7QL2JWT",
   "metadata": {
    "id": "fMP3l7QL2JWT"
   },
   "outputs": [],
   "source": [
    "# chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "# demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "# try:\n",
    "#     demo.launch(debug=True, share=True, show_api=False)\n",
    "#     demo.close()\n",
    "# except Exception as e:\n",
    "#     demo.close()\n",
    "#     print(e)\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCb3RVVfbmQ0",
   "metadata": {
    "id": "yCb3RVVfbmQ0"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **ç¬¬ 4 éƒ¨åˆ†ï¼š** ä¿å­˜ç´¢å¼•ä»¥ç”¨äºè¯„ä¼°\n",
    "\n",
    "å®ç° RAG é“¾åï¼Œè¯·å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading)ä¿å­˜æ‚¨ç§¯ç´¯å‡ºæ¥çš„å‘é‡å­˜å‚¨ã€‚æœ€åçš„è¯„ä¼°ä¼šç”¨åˆ°ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y4se5wQ4Afda",
   "metadata": {
    "id": "Y4se5wQ4Afda"
   },
   "outputs": [],
   "source": [
    "## Save and compress your index\n",
    "docstore.save_local(\"docstore_index\")\n",
    "!tar czvf docstore_index.tgz docstore_index\n",
    "\n",
    "!rm -rf docstore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LsI7NivbIgFw",
   "metadata": {
    "id": "LsI7NivbIgFw"
   },
   "source": [
    "å¦‚æœæ‰€æœ‰å†…å®¹éƒ½å·²æ­£ç¡®ä¿å­˜ï¼Œå°±å¯ä»¥æ‰§è¡Œä»¥ä¸‹ä»£ç ä» `tgz` å‹ç¼©æ–‡ä»¶æ‹¿åˆ°ç´¢å¼•äº†ï¼ˆåªè¦å®‰è£…å¥½äº† pip ç¯å¢ƒï¼‰ã€‚å½“æ‚¨ç¡®è®¤è¿™ä¸ªä»£ç å•å…ƒèƒ½æ‹¿åˆ°æ‚¨çš„ç´¢å¼•ä¹‹åï¼ŒæŠŠ `docstore_index.tgz` ä¸‹è½½ä¸‹æ¥ï¼Œä¸‹ä¸ª notebook ä¼šç”¨åˆ°ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qs8820ucIu1t",
   "metadata": {
    "id": "Qs8820ucIu1t"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "!tar xzvf docstore_index.tgz\n",
    "new_db = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(\"Testing the index\")\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "as_3vWJGKB2F",
   "metadata": {
    "id": "as_3vWJGKB2F"
   },
   "source": [
    "----\n",
    "\n",
    "## **ç¬¬ 5 éƒ¨åˆ†ï¼š** æ€»ç»“\n",
    "\n",
    "æ­å–œï¼å¦‚æœæ‚¨çš„ RAG é“¾èƒ½æ­£å¸¸è¿è¡Œï¼Œå°±ç»§ç»­è¿›å…¥ 08_evaluation.ipynb è¿›è¡Œ **RAG è¯„ä¼°**å§ï¼\n",
    "\n",
    "### <font color=\"#76b900\">**éå¸¸å¥½ï¼**</font>\n",
    "\n",
    "### **æ¥ä¸‹æ¥**ï¼š\n",
    "**[å¯é€‰]** å›é¡¾ notebook é¡¶éƒ¨çš„â€œæ€è€ƒé—®é¢˜â€ã€‚"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
