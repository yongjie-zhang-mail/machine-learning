# vLLM Qwen3 æµå¼è°ƒç”¨å·¥å…·

è¿™ä¸ªå·¥å…·é›†æä¾›äº†å¯¹ vLLM éƒ¨ç½²çš„ Qwen3 æ¨¡å‹è¿›è¡Œæµå¼è°ƒç”¨çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚

## åŠŸèƒ½ç‰¹æ€§

- âœ… æµå¼å¯¹è¯æ”¯æŒ
- âœ… **æ¨¡å‹æ€è€ƒè¿‡ç¨‹è¾“å‡º** - ğŸ†• æ˜¾ç¤ºAIçš„æ¨ç†è¿‡ç¨‹
- âœ… å¯¹è¯å†å²ç®¡ç†
- âœ… å¤šç§ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
- âœ… çµæ´»çš„å‚æ•°é…ç½®
- âœ… é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- âœ… æ€§èƒ½æµ‹è¯•å’Œç›‘æ§
- âœ… äº¤äº’å¼èŠå¤©ç•Œé¢

## å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨ vLLM æœåŠ¡

é¦–å…ˆç¡®ä¿ vLLM æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œé»˜è®¤åœ°å€ä¸º `http://127.0.0.1:8002`ï¼š

```bash
# ç¤ºä¾‹å¯åŠ¨å‘½ä»¤ï¼ˆæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
vllm serve Qwen3-0.6B --host 127.0.0.1 --port 8002
```

### 2. æµ‹è¯•è¿æ¥

```bash
python stream_chat.py test
```

### 3. å¼€å§‹å¯¹è¯

```bash
# äº¤äº’å¼èŠå¤©
python stream_chat.py chat

# æˆ–è€…ç›´æ¥è¿è¡Œ
python stream_chat.py
```

### 4. è¿è¡Œæ¼”ç¤º

```bash
# æµå¼è°ƒç”¨æ¼”ç¤º
python stream_chat.py demo

# æ€è€ƒæ¨¡å¼æ¼”ç¤º
python stream_chat.py thinking

# å„ç§ä½¿ç”¨ç¤ºä¾‹
python examples.py
```

## ä½¿ç”¨æ–¹æ³•

### åŸºç¡€æµå¼è°ƒç”¨

```python
from stream_chat import VLLMStreamChat

# åˆ›å»ºå®¢æˆ·ç«¯
client = VLLMStreamChat(
    base_url="http://127.0.0.1:8002/v1",
    model="Qwen3-0.6B"
)

# æµå¼å¯¹è¯
for chunk in client.stream_chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"):
    print(chunk, end="", flush=True)
```

### éæµå¼è°ƒç”¨

```python
# ä¸€æ¬¡æ€§è·å–å®Œæ•´å›å¤
response = client.chat("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
print(response)
```

### è‡ªå®šä¹‰å‚æ•°

```python
for chunk in client.stream_chat(
    message="å†™ä¸€ä¸ª Python æ’åºç®—æ³•",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹",
    temperature=0.3,
    max_tokens=1024,
    top_p=0.9,
    enable_thinking=True,
    show_thinking=True
):
    print(chunk, end="", flush=True)
```

### ğŸ§  æ€è€ƒæ¨¡å¼

```python
# æ˜¾ç¤ºAIçš„æ€è€ƒè¿‡ç¨‹
for chunk in client.stream_chat(
    message="9.9å’Œ9.11è°å¤§ï¼Ÿè¯·è¯¦ç»†åˆ†æ",
    enable_thinking=True,
    show_thinking=True
):
    print(chunk, end="", flush=True)

# åˆ†åˆ«å¤„ç†æ€è€ƒå’Œç­”æ¡ˆå†…å®¹
for chunk_data in client.thinking_chat(
    message="è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†"
):
    if chunk_data["type"] == "thinking":
        print(f"æ€è€ƒ: {chunk_data['content']}", end="")
    elif chunk_data["type"] == "answer":
        print(f"ç­”æ¡ˆ: {chunk_data['content']}", end="")
```

### å¯¹è¯å†å²ç®¡ç†

```python
# æ¸…ç©ºå†å²
client.clear_history()

# è·å–å†å²
history = client.get_history()

# è®¾ç½®å†å²
client.set_history(history)
```

## é…ç½®é€‰é¡¹

### è¿æ¥é…ç½®

åœ¨ `config.py` ä¸­ä¿®æ”¹è¿æ¥è®¾ç½®ï¼š

```python
VLLM_CONFIG = {
    "base_url": "http://127.0.0.1:8002/v1",  # vLLM æœåŠ¡åœ°å€
    "api_key": "EMPTY",                      # API å¯†é’¥
    "model": "Qwen3-0.6B",    # æ¨¡å‹åç§°
    "timeout": 60.0                          # è¶…æ—¶æ—¶é—´
}
```

### ç”Ÿæˆå‚æ•°

```python
DEFAULT_PARAMS = {
    "temperature": 0.7,        # æ¸©åº¦ï¼ˆ0-2ï¼Œè¶Šé«˜è¶Šéšæœºï¼‰
    "max_tokens": 2048,        # æœ€å¤§ç”Ÿæˆ token æ•°
    "top_p": 0.9,             # Top-p é‡‡æ ·
    "frequency_penalty": 0.0,  # é¢‘ç‡æƒ©ç½š
    "presence_penalty": 0.0    # å­˜åœ¨æƒ©ç½š
}
```

## å‘½ä»¤è¡Œä½¿ç”¨

### ä¸»ç¨‹åº

```bash
python stream_chat.py [command]

# å¯ç”¨å‘½ä»¤ï¼š
# test  - æµ‹è¯•è¿æ¥
# demo  - æ¼”ç¤ºæµå¼è°ƒç”¨
# chat  - äº¤äº’å¼èŠå¤©
```

### ç¤ºä¾‹ç¨‹åº

```bash
python examples.py [example]

# å¯ç”¨ç¤ºä¾‹ï¼š
# basic        - åŸºç¡€ä½¿ç”¨ç¤ºä¾‹
# conversation - è¿ç»­å¯¹è¯ç¤ºä¾‹
# code         - ä»£ç ç”Ÿæˆç¤ºä¾‹
# performance  - æ€§èƒ½æµ‹è¯•
# parameters   - è‡ªå®šä¹‰å‚æ•°ç¤ºä¾‹
# error        - é”™è¯¯å¤„ç†ç¤ºä¾‹
```

## é«˜çº§åŠŸèƒ½

### 1. ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿

é¢„å®šä¹‰äº†å¤šç§ç³»ç»Ÿæç¤ºè¯ï¼š

- `default`: é€šç”¨åŠ©æ‰‹
- `coding`: ç¼–ç¨‹åŠ©æ‰‹
- `academic`: å­¦æœ¯åŠ©æ‰‹
- `creative`: åˆ›æ„åŠ©æ‰‹

### 2. æ€§èƒ½ç›‘æ§

å†…ç½®æ€§èƒ½æµ‹è¯•åŠŸèƒ½ï¼Œå¯ä»¥ç›‘æ§ï¼š

- å“åº”æ—¶é—´
- ç”Ÿæˆé€Ÿåº¦
- Token ä½¿ç”¨é‡

### 3. é”™è¯¯å¤„ç†

å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼š

- è¿æ¥é”™è¯¯é‡è¯•
- è¶…æ—¶å¤„ç†
- å¼‚å¸¸æ•è·å’Œæ—¥å¿—

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **è¿æ¥å¤±è´¥**
   - æ£€æŸ¥ vLLM æœåŠ¡æ˜¯å¦è¿è¡Œ
   - ç¡®è®¤æœåŠ¡åœ°å€å’Œç«¯å£
   - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®

2. **æ¨¡å‹åç§°é”™è¯¯**
   - ç¡®è®¤æ¨¡å‹åœ¨ vLLM ä¸­æ­£ç¡®åŠ è½½
   - æ£€æŸ¥æ¨¡å‹åç§°æ‹¼å†™

3. **ç”Ÿæˆé€Ÿåº¦æ…¢**
   - æ£€æŸ¥æœåŠ¡å™¨èµ„æº
   - è°ƒæ•´ max_tokens å‚æ•°
   - ä¼˜åŒ– vLLM å¯åŠ¨å‚æ•°

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æµ‹è¯•è¿æ¥
client.chat("æµ‹è¯•", max_tokens=10, keep_history=False)
```

## æ³¨æ„äº‹é¡¹

1. ç¡®ä¿ vLLM æœåŠ¡æ­£åœ¨è¿è¡Œ
2. æ ¹æ®å®é™…éƒ¨ç½²ä¿®æ”¹ `base_url` å’Œ `model` å‚æ•°
3. åˆç†è®¾ç½® `max_tokens` ä»¥é¿å…è¿‡é•¿ç­‰å¾…
4. åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è€ƒè™‘æ·»åŠ é‡è¯•å’Œé”™è¯¯æ¢å¤æœºåˆ¶
5. æ³¨æ„ API è°ƒç”¨é¢‘ç‡é™åˆ¶ï¼ˆå¦‚æœæœ‰ï¼‰

## æ‰©å±•å¼€å‘

å¯ä»¥åŸºäºæ­¤å·¥å…·è¿›è¡Œæ‰©å±•ï¼š

- æ·»åŠ æ›´å¤šçš„ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
- å®ç°å¯¹è¯æ•°æ®çš„æŒä¹…åŒ–å­˜å‚¨
- é›†æˆåˆ° Web åº”ç”¨æˆ– API æœåŠ¡
- æ·»åŠ å¤šæ¨¡æ€æ”¯æŒï¼ˆå¦‚å›¾åƒè¾“å…¥ï¼‰
- å®ç°æ‰¹é‡å¤„ç†åŠŸèƒ½
