#!/usr/bin/env python3
"""
æ€è€ƒæ¨¡å¼æµ‹è¯•è„šæœ¬
"""

from stream_chat import VLLMStreamChat


def test_thinking_mode():
    """æµ‹è¯•æ€è€ƒæ¨¡å¼åŠŸèƒ½"""
    print("=== æ€è€ƒæ¨¡å¼æµ‹è¯• ===")
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = VLLMStreamChat(
        base_url="http://127.0.0.1:8002/v1",
        model="Qwen3-0.6B"
    )
    
    # æµ‹è¯•é—®é¢˜
    test_question = "9.9å’Œ9.11è°å¤§ï¼Ÿè¯·è¯¦ç»†åˆ†æ"
    
    print(f"é—®é¢˜: {test_question}")
    print("\nğŸ¤” æ€è€ƒè¿‡ç¨‹:")
    print("-" * 50)
    
    # æµ‹è¯•æ€è€ƒæ¨¡å¼
    try:
        for chunk_data in client.thinking_chat(
            message=test_question,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªé€»è¾‘æ€ç»´ä¸¥è°¨çš„AIåŠ©æ‰‹ï¼Œè¯·ä»”ç»†æ€è€ƒåç»™å‡ºå‡†ç¡®çš„ç­”æ¡ˆã€‚",
            temperature=0.3,
            max_tokens=1024,
            keep_history=False
        ):
            if chunk_data["type"] == "thinking":
                print(chunk_data["content"], end="", flush=True)
            elif chunk_data["type"] == "answer":
                print("\n" + "-" * 50)
                print("ğŸ’¡ æœ€ç»ˆç­”æ¡ˆ:")
                print(chunk_data["content"], end="", flush=True)
            elif chunk_data["type"] == "error":
                print(f"\nâŒ {chunk_data['content']}")
        
        print("\n\nâœ… æ€è€ƒæ¨¡å¼æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")


def test_normal_mode():
    """æµ‹è¯•æ™®é€šæ¨¡å¼ï¼ˆå¸¦æ€è€ƒè¾“å‡ºï¼‰"""
    print("\n=== æ™®é€šæ¨¡å¼æµ‹è¯•ï¼ˆæ˜¾ç¤ºæ€è€ƒï¼‰ ===")
    
    client = VLLMStreamChat(
        base_url="http://127.0.0.1:8002/v1",
        model="Qwen3-0.6B"
    )
    
    test_question = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    
    print(f"é—®é¢˜: {test_question}")
    print("\nå›ç­”: ", end="", flush=True)
    
    try:
        for chunk in client.stream_chat(
            message=test_question,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚",
            temperature=0.5,
            max_tokens=800,
            enable_thinking=True,
            show_thinking=True,
            keep_history=False
        ):
            print(chunk, end="", flush=True)
        
        print("\n\nâœ… æ™®é€šæ¨¡å¼æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")


def test_no_thinking_mode():
    """æµ‹è¯•å…³é—­æ€è€ƒæ¨¡å¼"""
    print("\n=== å…³é—­æ€è€ƒæ¨¡å¼æµ‹è¯• ===")
    
    client = VLLMStreamChat(
        base_url="http://127.0.0.1:8002/v1",
        model="Qwen3-0.6B"
    )
    
    test_question = "9.9å’Œ9.11è°å¤§ï¼Ÿ"
    
    print(f"é—®é¢˜: {test_question}")
    print("\nå›ç­”: ", end="", flush=True)
    
    try:
        for chunk in client.stream_chat(
            message=test_question,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚",
            temperature=0.5,
            max_tokens=500,
            enable_thinking=False,
            show_thinking=False,
            keep_history=False
        ):
            print(chunk, end="", flush=True)
        
        print("\n\nâœ… å…³é—­æ€è€ƒæ¨¡å¼æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "thinking":
            test_thinking_mode()
        elif sys.argv[1] == "normal":
            test_normal_mode()
        elif sys.argv[1] == "no-thinking":
            test_no_thinking_mode()
        else:
            print("ä½¿ç”¨æ–¹æ³•:")
            print("python test_thinking.py thinking     # æµ‹è¯•æ€è€ƒæ¨¡å¼")
            print("python test_thinking.py normal       # æµ‹è¯•æ™®é€šæ¨¡å¼ï¼ˆæ˜¾ç¤ºæ€è€ƒï¼‰")
            print("python test_thinking.py no-thinking  # æµ‹è¯•å…³é—­æ€è€ƒæ¨¡å¼")
    else:
        print("=== æ€è€ƒåŠŸèƒ½å…¨é¢æµ‹è¯• ===")
        test_thinking_mode()
        test_normal_mode()
        test_no_thinking_mode()
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
