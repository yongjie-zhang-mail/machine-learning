#!/usr/bin/env python3
"""
vLLM éƒ¨ç½²çš„ Qwen3 æ¨¡å‹æµå¼è°ƒç”¨ç¤ºä¾‹
ä½¿ç”¨ OpenAI Python åŒ…è¿›è¡Œæµå¼å¯¹è¯
"""

import os
import sys
import json
import time
from typing import Iterator, Dict, Any, Optional, List
from openai import OpenAI


class VLLMStreamChat:
    """vLLM æµå¼èŠå¤©å®¢æˆ·ç«¯"""
    
    def __init__(
        self, 
        base_url: str = "http://127.0.0.1:8002/v1",
        api_key: str = "EMPTY",
        model: str = "Qwen3-0.6B",
        timeout: float = 60.0
    ):
        """
        åˆå§‹åŒ– vLLM æµå¼èŠå¤©å®¢æˆ·ç«¯
        
        Args:
            base_url: vLLM æœåŠ¡çš„åŸºç¡€ URL
            api_key: API å¯†é’¥ï¼ˆvLLM æœ¬åœ°éƒ¨ç½²é€šå¸¸ä½¿ç”¨ "EMPTY"ï¼‰
            model: æ¨¡å‹åç§°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
        """
        self.client = OpenAI(
            base_url=base_url,
            api_key=api_key,
            timeout=timeout
        )
        self.model = model
        self.conversation_history: List[Dict[str, str]] = []
    
    def stream_chat(
        self,
        message: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.2,
        max_tokens: int = 4096,
        top_p: float = 0.9,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0,
        keep_history: bool = True,
        enable_thinking: bool = True,
        show_thinking: bool = True
    ) -> Iterator[str]:
        """
        è¿›è¡Œæµå¼å¯¹è¯
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            top_p: top_p å‚æ•°
            frequency_penalty: é¢‘ç‡æƒ©ç½š
            presence_penalty: å­˜åœ¨æƒ©ç½š
            keep_history: æ˜¯å¦ä¿æŒå¯¹è¯å†å²
            enable_thinking: æ˜¯å¦å¯ç”¨æ¨¡å‹æ€è€ƒåŠŸèƒ½
            show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
            
        Yields:
            str: æµå¼è¿”å›çš„æ–‡æœ¬ç‰‡æ®µï¼ˆåŒ…æ‹¬æ€è€ƒå†…å®¹å’Œæœ€ç»ˆç­”æ¡ˆï¼‰
        """
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            
            # æ·»åŠ å†å²å¯¹è¯
            if keep_history:
                messages.extend(self.conversation_history)
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": message})
            
            # å‡†å¤‡extra_bodyå‚æ•°
            extra_body = {}
            if enable_thinking:
                extra_body["enable_thinking"] = True
            
            # åˆ›å»ºæµå¼èŠå¤©å®Œæˆ
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                stream=True,
                extra_body=extra_body if extra_body else None
            )
            
            # æ”¶é›†å®Œæ•´å›å¤ç”¨äºå†å²è®°å½•
            full_response = ""
            done_thinking = False
            
            # æµå¼è¾“å‡º
            for chunk in stream:
                # å¤„ç†æ€è€ƒå†…å®¹
                thinking_chunk = getattr(chunk.choices[0].delta, 'reasoning_content', None) or ''
                # å¤„ç†æœ€ç»ˆç­”æ¡ˆå†…å®¹
                answer_chunk = chunk.choices[0].delta.content or ''
                
                if thinking_chunk and show_thinking:
                    yield thinking_chunk
                elif answer_chunk:
                    if not done_thinking and show_thinking and enable_thinking:
                        yield '\n\n=== æœ€ç»ˆç­”æ¡ˆ ===\n'
                        done_thinking = True
                    full_response += answer_chunk
                    yield answer_chunk
            
            # æ›´æ–°å¯¹è¯å†å²
            if keep_history:
                self.conversation_history.append({"role": "user", "content": message})
                self.conversation_history.append({"role": "assistant", "content": full_response})
                
        except Exception as e:
            yield f"é”™è¯¯: {str(e)}"
    
    def chat(
        self,
        message: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
        top_p: float = 0.9,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0,
        keep_history: bool = True,
        enable_thinking: bool = True,
        show_thinking: bool = True
    ) -> str:
        """
        éæµå¼å¯¹è¯ï¼ˆä¸€æ¬¡æ€§è¿”å›å®Œæ•´å›å¤ï¼‰
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            top_p: top_p å‚æ•°
            frequency_penalty: é¢‘ç‡æƒ©ç½š
            presence_penalty: å­˜åœ¨æƒ©ç½š
            keep_history: æ˜¯å¦ä¿æŒå¯¹è¯å†å²
            enable_thinking: æ˜¯å¦å¯ç”¨æ¨¡å‹æ€è€ƒåŠŸèƒ½
            show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
            
        Returns:
            str: å®Œæ•´çš„å›å¤ï¼ˆåŒ…æ‹¬æ€è€ƒå†…å®¹å’Œæœ€ç»ˆç­”æ¡ˆï¼‰
        """
        response = ""
        for chunk in self.stream_chat(
            message=message,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty,
            keep_history=keep_history,
            enable_thinking=enable_thinking,
            show_thinking=show_thinking
        ):
            response += chunk
        return response
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history.clear()
    
    def get_history(self) -> List[Dict[str, str]]:
        """è·å–å¯¹è¯å†å²"""
        return self.conversation_history.copy()
    
    def set_history(self, history: List[Dict[str, str]]):
        """è®¾ç½®å¯¹è¯å†å²"""
        self.conversation_history = history
    
    def thinking_chat(
        self,
        message: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.2,
        max_tokens: int = 4096,
        top_p: float = 0.9,
        keep_history: bool = True
    ) -> Iterator[Dict[str, str]]:
        """
        æ€è€ƒæ¨¡å¼çš„æµå¼å¯¹è¯ï¼Œåˆ†åˆ«è¿”å›æ€è€ƒå†…å®¹å’Œæœ€ç»ˆç­”æ¡ˆ
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            top_p: top_p å‚æ•°
            keep_history: æ˜¯å¦ä¿æŒå¯¹è¯å†å²
            
        Yields:
            Dict[str, str]: {"type": "thinking"|"answer", "content": "å†…å®¹"}
        """
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            
            # æ·»åŠ å†å²å¯¹è¯
            if keep_history:
                messages.extend(self.conversation_history)
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": message})
            
            # å‡†å¤‡extra_bodyå‚æ•°
            extra_body = {"enable_thinking": True}
            
            # åˆ›å»ºæµå¼èŠå¤©å®Œæˆ
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                stream=True,
                extra_body=extra_body
            )
            
            # æ”¶é›†å®Œæ•´å›å¤ç”¨äºå†å²è®°å½•
            full_response = ""
            
            # æµå¼è¾“å‡º
            for chunk in stream:
                # å¤„ç†æ€è€ƒå†…å®¹
                thinking_chunk = getattr(chunk.choices[0].delta, 'reasoning_content', None) or ''
                # å¤„ç†æœ€ç»ˆç­”æ¡ˆå†…å®¹
                answer_chunk = chunk.choices[0].delta.content or ''
                
                if thinking_chunk:
                    yield {"type": "thinking", "content": thinking_chunk}
                elif answer_chunk:
                    full_response += answer_chunk
                    yield {"type": "answer", "content": answer_chunk}
            
            # æ›´æ–°å¯¹è¯å†å²
            if keep_history:
                self.conversation_history.append({"role": "user", "content": message})
                self.conversation_history.append({"role": "assistant", "content": full_response})
                
        except Exception as e:
            yield {"type": "error", "content": f"é”™è¯¯: {str(e)}"}


def interactive_chat():
    """äº¤äº’å¼èŠå¤©æ¼”ç¤º"""
    print("=== vLLM Qwen3 æµå¼èŠå¤©æ¼”ç¤º ===")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    print("è¾“å…¥ 'history' æŸ¥çœ‹å¯¹è¯å†å²")
    print("è¾“å…¥ 'thinking on' å¼€å¯æ€è€ƒæ¨¡å¼")
    print("è¾“å…¥ 'thinking off' å…³é—­æ€è€ƒæ¨¡å¼")
    print("-" * 50)
    
    # åˆ›å»ºèŠå¤©å®¢æˆ·ç«¯
    chat_client = VLLMStreamChat(
        base_url="http://127.0.0.1:8002/v1",  # æ ¹æ®å®é™…éƒ¨ç½²ä¿®æ”¹
        model="Qwen3-0.6B"     # æ ¹æ®å®é™…æ¨¡å‹ä¿®æ”¹
    )
    
    # ç³»ç»Ÿæç¤ºè¯
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ï¼Œä¿æŒå‹å¥½å’Œä¸“ä¸šã€‚"
    
    # æ€è€ƒæ¨¡å¼å¼€å…³
    thinking_mode = True
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nç”¨æˆ·: ").strip()
            
            # å¤„ç†ç‰¹æ®Šå‘½ä»¤
            if user_input.lower() in ['quit', 'exit']:
                print("å†è§ï¼")
                break
            elif user_input.lower() == 'clear':
                chat_client.clear_history()
                print("å¯¹è¯å†å²å·²æ¸…ç©º")
                continue
            elif user_input.lower() == 'history':
                history = chat_client.get_history()
                print(f"å¯¹è¯å†å² ({len(history)} æ¡æ¶ˆæ¯):")
                for i, msg in enumerate(history):
                    print(f"{i+1}. {msg['role']}: {msg['content'][:100]}...")
                continue
            elif user_input.lower() == 'thinking on':
                thinking_mode = True
                print("æ€è€ƒæ¨¡å¼å·²å¼€å¯")
                continue
            elif user_input.lower() == 'thinking off':
                thinking_mode = False
                print("æ€è€ƒæ¨¡å¼å·²å…³é—­")
                continue
            elif not user_input:
                continue
            
            # æµå¼è¾“å‡ºå›å¤
            print("åŠ©æ‰‹: ", end="", flush=True)
            for chunk in chat_client.stream_chat(
                message=user_input,
                system_prompt=system_prompt if len(chat_client.get_history()) == 0 else None,
                temperature=0.7,
                max_tokens=2048,
                enable_thinking=thinking_mode,
                show_thinking=thinking_mode
            ):
                print(chunk, end="", flush=True)
            print()  # æ¢è¡Œ
            
        except KeyboardInterrupt:
            print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"\né”™è¯¯: {e}")


def demo_streaming():
    """æµå¼è°ƒç”¨æ¼”ç¤º"""
    print("=== æµå¼è°ƒç”¨æ¼”ç¤º ===")
    
    # åˆ›å»ºèŠå¤©å®¢æˆ·ç«¯
    chat_client = VLLMStreamChat(
        base_url="http://127.0.0.1:8002/v1",
        model="Qwen3-0.6B"
    )
    
    # ç¤ºä¾‹é—®é¢˜
    questions = [
        "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
        "Python æœ‰å“ªäº›ä¸»è¦çš„æœºå™¨å­¦ä¹ åº“ï¼Ÿ",
        "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯ Transformer æ¶æ„"
    ]
    
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯ä¸“å®¶ï¼Œè¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„æŠ€æœ¯è§£ç­”ã€‚"
    
    for i, question in enumerate(questions, 1):
        print(f"\né—®é¢˜ {i}: {question}")
        print("å›ç­”: ", end="", flush=True)
        
        start_time = time.time()
        response_length = 0
        
        for chunk in chat_client.stream_chat(
            message=question,
            system_prompt=system_prompt if i == 1 else None,
            temperature=0.7,
            max_tokens=1024,
            enable_thinking=True,
            show_thinking=True
        ):
            print(chunk, end="", flush=True)
            response_length += len(chunk)
        
        end_time = time.time()
        print(f"\n[è€—æ—¶: {end_time - start_time:.2f}s, é•¿åº¦: {response_length} å­—ç¬¦]")
        print("-" * 80)


def demo_thinking():
    """æ€è€ƒæ¨¡å¼æ¼”ç¤º"""
    print("=== æ€è€ƒæ¨¡å¼æ¼”ç¤º ===")
    
    # åˆ›å»ºèŠå¤©å®¢æˆ·ç«¯
    chat_client = VLLMStreamChat(
        base_url="http://127.0.0.1:8002/v1",
        model="Qwen3-0.6B"
    )
    
    # éœ€è¦æ¨ç†çš„é—®é¢˜
    questions = [
        "9.9å’Œ9.11è°å¤§ï¼Ÿè¯·è¯¦ç»†åˆ†æ",
        "å¦‚æœä¸€ä¸ªçƒä»10ç±³é«˜åº¦è‡ªç”±è½ä¸‹ï¼Œéœ€è¦å¤šé•¿æ—¶é—´æ‰èƒ½è½åœ°ï¼Ÿ",
        "è¯·è§£é‡Šä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Œæ¶‰åŠåˆ°å“ªäº›ç‰©ç†åŸç†ï¼Ÿ"
    ]
    
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªé€»è¾‘æ€ç»´ä¸¥è°¨çš„AIåŠ©æ‰‹ï¼Œè¯·ä»”ç»†æ€è€ƒåç»™å‡ºå‡†ç¡®çš„ç­”æ¡ˆã€‚"
    
    for i, question in enumerate(questions, 1):
        print(f"\n=== é—®é¢˜ {i} ===")
        print(f"é—®é¢˜: {question}")
        print("\nğŸ¤” æ€è€ƒè¿‡ç¨‹:")
        print("-" * 40)
        
        thinking_content = ""
        answer_content = ""
        
        for chunk_data in chat_client.thinking_chat(
            message=question,
            system_prompt=system_prompt if i == 1 else None,
            temperature=0.3,
            max_tokens=2048
        ):
            if chunk_data["type"] == "thinking":
                thinking_content += chunk_data["content"]
                print(chunk_data["content"], end="", flush=True)
            elif chunk_data["type"] == "answer":
                if not answer_content:  # ç¬¬ä¸€æ¬¡æ¥æ”¶åˆ°ç­”æ¡ˆå†…å®¹
                    print("\n" + "-" * 40)
                    print("ğŸ’¡ æœ€ç»ˆç­”æ¡ˆ:")
                answer_content += chunk_data["content"]
                print(chunk_data["content"], end="", flush=True)
            elif chunk_data["type"] == "error":
                print(f"\nâŒ {chunk_data['content']}")
        
        print("\n" + "=" * 80)


def test_connection():
    """æµ‹è¯•è¿æ¥"""
    print("=== æµ‹è¯• vLLM è¿æ¥ ===")
    
    try:
        chat_client = VLLMStreamChat(
            base_url="http://127.0.0.1:8002/v1",
            model="Qwen3-0.6B"
        )
        
        print("æ­£åœ¨æµ‹è¯•è¿æ¥...")
        response = chat_client.chat(
            message="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±",
            temperature=0.7,
            max_tokens=200,
            keep_history=False
        )
        
        print("è¿æ¥æˆåŠŸï¼")
        print(f"æ¨¡å‹å›å¤: {response}")
        
    except Exception as e:
        print(f"è¿æ¥å¤±è´¥: {e}")
        print("\nè¯·æ£€æŸ¥:")
        print("1. vLLM æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ")
        print("2. æœåŠ¡åœ°å€æ˜¯å¦æ­£ç¡® (é»˜è®¤: http://127.0.0.1:8002)")
        print("3. æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")


if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] == "test":
            test_connection()
        elif sys.argv[1] == "demo":
            demo_streaming()
        elif sys.argv[1] == "thinking":
            demo_thinking()
        elif sys.argv[1] == "chat":
            interactive_chat()
        else:
            print("ä½¿ç”¨æ–¹æ³•:")
            print("python stream_chat.py test      # æµ‹è¯•è¿æ¥")
            print("python stream_chat.py demo      # æ¼”ç¤ºæµå¼è°ƒç”¨")
            print("python stream_chat.py thinking  # æ¼”ç¤ºæ€è€ƒæ¨¡å¼")
            print("python stream_chat.py chat      # äº¤äº’å¼èŠå¤©")
    else:
        print("=== vLLM Qwen3 æµå¼è°ƒç”¨å·¥å…· ===")
        print("ä½¿ç”¨æ–¹æ³•:")
        print("python stream_chat.py test      # æµ‹è¯•è¿æ¥")
        print("python stream_chat.py demo      # æ¼”ç¤ºæµå¼è°ƒç”¨")
        print("python stream_chat.py thinking  # æ¼”ç¤ºæ€è€ƒæ¨¡å¼")
        print("python stream_chat.py chat      # äº¤äº’å¼èŠå¤©")
        print("\nå¼€å§‹äº¤äº’å¼èŠå¤©...")
        interactive_chat()
