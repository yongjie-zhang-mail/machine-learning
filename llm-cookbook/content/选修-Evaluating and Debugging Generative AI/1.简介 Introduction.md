# 评估和调试生成式 AI

## 简介

欢迎来到课程《评估和调试生成式 AI》👏🏻👏🏻

本课程由 Carey Phelps （Weights & Biases 首席产品经理） 与 Deeplearning.ai 合作开发，本课程旨在提供一套系统化的方法和工具，帮助您有效地跟踪和调试生成式 AI 模型。

在构建机器学习系统的过程中，管理和跟踪所有的数据、模型和超参数可能会变得复杂。随着团队规模的增大，这种复杂性可能会进一步加剧。生成式 AI 模型相比于监督学习模型增加了一层复杂性，因为它们的输出是复杂的，所以它们可能更难评估。

本课程将使用 Weights & Biases 的工具，这是一套易用且灵活的工具集，已经成为机器学习实验跟踪的行业标准。本课程涵盖用于文本生成的大型语言模型和用于图像生成的扩散模型。


## 课程内容

在本课程中，我们将专注于以下内容：

1. 如何跟踪和可视化实验
2. 如何监控扩散模型
3. 如何评估和微调大型语言模型（LLMs）

您将学习到一系列的调试和评估工具，包括：

- Experiments：用于跟踪机器学习实验
- Artifacts：用于版本控制和存储数据集和模型
- Tables：用于可视化和检查模型做出的预测
- Reports：用于协作和分享实验结果
- Model Registry：用于管理模型的生命周期
- Prompts：用于评估大型语言模型生成

这些工具可以与 Python、TensorFlow 或 PyTorch 等主流的框架和计算平台一起工作。

## 致谢

我们要特别感谢 Weights & Biases 的 Darek Kleczek 和 Thomas Capelle，以及 Deeplearning.ai 的 Geoff Ludwig 和 Tommy Nelson 对本课程的贡献。

在完成本课程后，您将理解评估和调试生成式 AI 的最佳实践，并且将掌握一套用于系统评估和调试生成式 AI 项目的工具。我们期待您的参与，希望您能从本课程中获益，从而推动您的机器学习项目取得更大的成功。